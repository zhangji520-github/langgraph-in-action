{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "chapter-intro",
   "metadata": {},
   "source": [
    "# ç¬¬ 4 ç« ï¼šAI æ™ºèƒ½ä½“çš„äº¤äº’ä½“éªŒ\n",
    "\n",
    "> æœ¬ç¬”è®°æ–‡ä»¶éœ€è¦ä¸ã€ŠLangGraphå®æˆ˜ã€‹çš„ç¬¬ 4 ç« çš„å†…å®¹é…å¥—ä½¿ç”¨ã€‚\n",
    "\n",
    "åœ¨ AI åº”ç”¨å¿«é€Ÿå‘å±•çš„ä»Šå¤©ï¼Œç”¨æˆ·ä½“éªŒå·²æˆä¸ºåº”ç”¨æˆè´¥çš„å…³é”®ã€‚ç”¨æˆ·ä¸ä»…æœŸæœ› AI æ™ºèƒ½ä½“å…·å¤‡é«˜åº¦æ™ºèƒ½ï¼Œæ›´è¿½æ±‚æµç•…ã€å¿«é€Ÿã€è‡ªç„¶çš„äº¤äº’ä½“éªŒï¼Œä»¥åŠæ™ºèƒ½ä½“åœ¨é•¿æœŸäº’åŠ¨ä¸­å±•ç°å‡ºçš„è®°å¿†åŠ›å’Œç†è§£åŠ›ã€‚\n",
    "\n",
    "æœ¬ç« å°†æ·±å…¥æ¢è®¨ LangGraph æ¡†æ¶å¦‚ä½•é€šè¿‡æµå¼å¤„ç†ã€æŒä¹…åŒ–å’Œäººæœºç¯è·¯åä½œä¸‰å¤§æ ¸å¿ƒæœºåˆ¶ï¼Œå…¨é¢æå‡ AI æ™ºèƒ½ä½“çš„ç”¨æˆ·ä½“éªŒï¼Œæ‰“é€ æ›´å€¼å¾—ä¿¡èµ–ã€æ›´å…·å¸å¼•åŠ›ã€æ›´é«˜æ•ˆå®ç”¨çš„ AI ç³»ç»Ÿã€‚\n",
    "\n",
    "æˆ‘ä»¬å°†æ­ç¤º LangGraph å¦‚ä½•åˆ©ç”¨æµå¼å¤„ç†æŠ€æœ¯ï¼Œå…‹æœ LLM å›ºæœ‰çš„å»¶è¿Ÿï¼Œå®ç°æ™ºèƒ½ä½“è¾“å‡ºçš„å®æ—¶åé¦ˆï¼Œæå‡äº¤äº’çš„å³æ—¶æ€§å’Œæµç•…æ„Ÿã€‚æˆ‘ä»¬å°†æ·±å…¥æ¢ç´¢ LangGraph çš„æŒä¹…åŒ–æœºåˆ¶ï¼Œé˜é‡Šå…¶å¦‚ä½•èµ‹äºˆæ™ºèƒ½ä½“\"è®°å¿†åŠ›\"ï¼Œå®ç°è·¨ä¼šè¯çŠ¶æ€ä¿æŒã€æ–­ç‚¹ç»­ä¼ ã€æ—¶é—´æ—…è¡Œè°ƒè¯•ç­‰é«˜çº§åŠŸèƒ½ã€‚æˆ‘ä»¬è¿˜å°†é‡ç‚¹ä»‹ç»å¦‚ä½•é€šè¿‡äººæœºç¯è·¯åä½œï¼Œå®ç°äººå·¥ç”¨æˆ·å¯¹æ™ºèƒ½ä½“è¡Œä¸ºçš„æœ‰æ•ˆæŒ‡å¯¼å’Œç›‘ç£ï¼Œæ„å»ºæ›´ç¬¦åˆäººç±»ä»·å€¼è§‚ã€æ›´å€¼å¾—ä¿¡èµ–çš„ AI ç³»ç»Ÿã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0omlhyyoq2c",
   "metadata": {},
   "source": [
    "### ğŸš€ ç¯å¢ƒå‡†å¤‡\n",
    "\n",
    "é¦–å…ˆåŠ è½½å¿…è¦çš„ç¯å¢ƒå˜é‡é…ç½®ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d72b951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z3qgqe21voc",
   "metadata": {},
   "source": [
    "## 4.1 å®æ—¶äº’åŠ¨ï¼šæŒæ¡ LangGraph ä¸­çš„æµå¼å¤„ç†\n",
    "\n",
    "åœ¨äººå·¥æ™ºèƒ½æ™ºèƒ½ä½“é¢†åŸŸï¼Œç”¨æˆ·ä½“éªŒ (User Experienceï¼ŒUX) è‡³å…³é‡è¦ã€‚æ— è®ºä¸€ä¸ªæ™ºèƒ½æ™ºèƒ½ä½“çš„æ¨ç†èƒ½åŠ›å¤šä¹ˆå¤æ‚ï¼ŒåŠŸèƒ½å¤šä¹ˆå¼ºå¤§ï¼Œå¦‚æœç”¨æˆ·æ„Ÿè§‰å®ƒé€Ÿåº¦ç¼“æ…¢æˆ–ååº”è¿Ÿé’ï¼Œéƒ½å°†æ— æ³•è¾¾åˆ°ç”¨æˆ·çš„æœŸæœ›ã€‚\n",
    "\n",
    "æµå¼å¤„ç†ä½œä¸ºä¸€ç§å¼ºå¤§çš„æŠ€æœ¯åº”è¿è€Œç”Ÿï¼Œå¯ä»¥å…‹æœå»¶è¿ŸæŒ‘æˆ˜ï¼Œå¹¶æ˜¾è‘—æé«˜ä½¿ç”¨ LangGraph æ„å»ºçš„ AI æ™ºèƒ½ä½“çš„æ„ŸçŸ¥å“åº”é€Ÿåº¦ã€‚æµå¼å¤„ç†ä¸æ˜¯ç­‰å¾…æ•´ä¸ªè¾“å‡ºç”Ÿæˆå®Œæ¯•åå†æ˜¾ç¤ºç»™ç”¨æˆ·ï¼Œè€Œæ˜¯å…è®¸ä»¥åˆ†å—çš„æ–¹å¼é€æ­¥äº¤ä»˜ä¿¡æ¯ã€‚\n",
    "\n",
    "LangGraph åœ¨æ¶æ„è®¾è®¡ä¹‹åˆå°±ä¼˜å…ˆè€ƒè™‘äº†å¯¹æµå¼å¤„ç†çš„ä¸€æµæ”¯æŒã€‚å®ƒæä¾›äº†ä¸€å¥—çµæ´»çš„æµå¼å¤„ç†æ¨¡å¼ï¼Œæ¯ç§æ¨¡å¼éƒ½é’ˆå¯¹ä¸åŒçš„ç”¨ä¾‹ä»¥åŠç›‘æ§å’Œç”¨æˆ·åé¦ˆæ‰€éœ€çš„è¯¦ç»†ç¨‹åº¦é‡èº«å®šåˆ¶ã€‚\n",
    "\n",
    "### 4.1.1 æ¢ç´¢æµå¼å¤„ç†æ¨¡å¼ï¼šæ ¹æ®ç”¨æˆ·éœ€æ±‚å®šåˆ¶è¾“å‡º\n",
    "\n",
    "LangGraph é€šè¿‡ `.stream()` å’Œ `.astream()` æ–¹æ³•æä¾›å¤šæ ·åŒ–çš„æµå¼å¤„ç†æ¨¡å¼ï¼Œä½¿å¼€å‘äººå‘˜èƒ½å¤Ÿé€‰æ‹©åœ¨å›¾æ‰§è¡ŒæœŸé—´æµå¼ä¼ è¾“çš„æ•°æ®ç±»å‹å’Œç²’åº¦ã€‚`stream_mode` å‚æ•°æ˜¯è§£é”è¿™äº›åŠŸèƒ½çš„å…³é”®ã€‚\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7xi8ro7xnbd",
   "metadata": {},
   "source": [
    "##### ç¤ºä¾‹ 4-1ï¼šä¸€ä¸ªç®€å•çš„ç¬‘è¯ç”Ÿæˆå›¾\n",
    "\n",
    "è®©æˆ‘ä»¬é¦–å…ˆåˆ›å»ºä¸€ä¸ªç®€å•çš„ LangGraphï¼Œå®ƒæ—¨åœ¨ä¼˜åŒ–ç”¨æˆ·æä¾›çš„ä¸»é¢˜ï¼Œç„¶åå›´ç»•è¯¥ä¸»é¢˜ç”Ÿæˆä¸€ä¸ªç¬‘è¯ã€‚è¿™å°†å¸®åŠ©æˆ‘ä»¬ç†è§£ä¸åŒçš„æµå¼å¤„ç†æ¨¡å¼ã€‚\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "lay7wu1msd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, START\n",
    "\n",
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    joke: str\n",
    "\n",
    "def refine_topic(state: State):\n",
    "    return {\"topic\": state[\"topic\"] + \" and cats\"}\n",
    "\n",
    "def generate_joke(state: State):\n",
    "    return {\"joke\": f\"This is a joke about {state['topic']}\"}\n",
    "\n",
    "graph = (\n",
    "    StateGraph(State)\n",
    "    .add_node(refine_topic)\n",
    "    .add_node(generate_joke)\n",
    "    .add_edge(START, \"refine_topic\")\n",
    "    .add_edge(\"refine_topic\", \"generate_joke\")\n",
    "    .compile()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9abgx0zao5",
   "metadata": {},
   "source": [
    "#### 4.1.1.1 æµå¼å¤„ç†æ¨¡å¼ï¼š`\"values\"`\n",
    "\n",
    "`\"values\"` æµå¼å¤„ç†æ¨¡å¼é€šè¿‡åœ¨æ¯ä¸ªæ­¥éª¤åæµå¼ä¼ è¾“å›¾çš„å®Œæ•´çŠ¶æ€ï¼Œæä¾›å¯¹å›¾æ‰§è¡Œçš„é«˜çº§è§†å›¾ã€‚æœ¬è´¨ä¸Šï¼Œåœ¨ LangGraph ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹æˆ–è¶…çº§æ­¥éª¤æ‰§è¡Œå®Œæ¯•åï¼Œæ•´ä¸ªçŠ¶æ€å¯¹è±¡ï¼ˆåŒ…å«æ‰€æœ‰å·²å®šä¹‰çš„çŠ¶æ€å˜é‡åŠå…¶å½“å‰å€¼ï¼‰éƒ½ä¼šä½œä¸ºå•ä¸ªå—åœ¨æµä¸­å‘å‡ºã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p72l0l15o39",
   "metadata": {},
   "source": [
    "##### ç¤ºä¾‹ 4-2ï¼šä½¿ç”¨ `stream_mode=\"values\"` æ‰§è¡Œå›¾\n",
    "\n",
    "å½“æˆ‘ä»¬ä½¿ç”¨ `stream_mode=\"values\"` æ‰§è¡Œæ­¤å›¾æ—¶ï¼Œæˆ‘ä»¬ä¼šè§‚å¯Ÿåˆ°åœ¨æ¯ä¸ªèŠ‚ç‚¹æ‰§è¡ŒåçŠ¶æ€éƒ½è¢«æµå¼ä¼ è¾“ï¼š\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5amzi1td7mx",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'topic': 'ice cream'}\n",
      "{'topic': 'ice cream and cats'}\n",
      "{'topic': 'ice cream and cats', 'joke': 'This is a joke about ice cream and cats'}\n"
     ]
    }
   ],
   "source": [
    "for chunk in graph.stream(\n",
    "    {\"topic\": \"ice cream\"},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xssnyt9znjb",
   "metadata": {},
   "source": [
    "**ğŸ’¡ æ ¸å¿ƒæ¦‚å¿µè§£æ**ï¼š\n",
    "\n",
    "æ­£å¦‚æ‚¨ä»è¾“å‡ºä¸­çœ‹åˆ°çš„ï¼Œç¬¬ä¸€ä¸ªå—æ˜¾ç¤ºäº†ä»…åŒ…å« \"topic\" çš„åˆå§‹çŠ¶æ€ã€‚ç¬¬äºŒä¸ªå—åæ˜ äº† `refine_topic` èŠ‚ç‚¹æ‰§è¡Œåçš„çŠ¶æ€ï¼Œæ›´æ–°äº† \"topic\"ã€‚æœ€åï¼Œæœ€åä¸€ä¸ªå—æ˜¾ç¤ºäº† `generate_joke` èŠ‚ç‚¹æ‰§è¡Œåçš„å®Œæ•´çŠ¶æ€ï¼Œç°åœ¨åŒæ—¶åŒ…å« \"topic\" å’Œ \"joke\"ã€‚è¿™ç§é€æ­¥æŸ¥çœ‹å®Œæ•´çŠ¶æ€çš„æ–¹å¼æ˜¯ `\"values\"` æµå¼å¤„ç†çš„æœ¬è´¨ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rxeqhi9jlsf",
   "metadata": {},
   "source": [
    "#### 4.1.1.2 æµå¼å¤„ç†æ¨¡å¼ï¼š`\"updates\"`\n",
    "\n",
    "ä¸ `\"values\"` ç›¸æ¯”ï¼Œ`\"updates\"` æµå¼å¤„ç†æ¨¡å¼æä¾›äº†æ›´é›†ä¸­å’Œç®€æ´çš„ä¿¡æ¯æµã€‚`\"updates\"` ä¸æ˜¯æµå¼ä¼ è¾“æ•´ä¸ªçŠ¶æ€ï¼Œè€Œæ˜¯ä»…æµå¼ä¼ è¾“æ¯ä¸ªèŠ‚ç‚¹å¯¹çŠ¶æ€è¿›è¡Œçš„ç‰¹å®šæ›´æ–°ã€‚è¿™æ„å‘³ç€æ‚¨ä¼šæ”¶åˆ°é”®å€¼å¯¹æµï¼Œå…¶ä¸­é”®æ˜¯ç”Ÿæˆæ›´æ–°çš„èŠ‚ç‚¹çš„åç§°ï¼Œå€¼æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­åŒ…å«è¯¥èŠ‚ç‚¹ä¿®æ”¹çš„çŠ¶æ€å˜é‡ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "riywzvs34kt",
   "metadata": {},
   "source": [
    "##### ç¤ºä¾‹ 4-3ï¼šä½¿ç”¨ `stream_mode=\"updates\"` æ‰§è¡Œå›¾\n",
    "\n",
    "ä½¿ç”¨ä¸ä¹‹å‰ç›¸åŒçš„ç¬‘è¯ç”Ÿæˆç¨‹åºï¼Œè®©æˆ‘ä»¬è§‚å¯Ÿä½¿ç”¨ `stream_mode=\"updates\"` çš„è¾“å‡ºï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "vqnohbbo8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'refine_topic': {'topic': 'ice cream and cats'}}\n",
      "{'generate_joke': {'joke': 'This is a joke about ice cream and cats'}}\n"
     ]
    }
   ],
   "source": [
    "for chunk in graph.stream(\n",
    "    {\"topic\": \"ice cream\"},\n",
    "    stream_mode=\"updates\",\n",
    "):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vw3ehxc73m",
   "metadata": {},
   "source": [
    "**ğŸ’¡ å…³é”®ç†è§£**ï¼š\n",
    "\n",
    "åœ¨è¿™é‡Œï¼Œè¾“å‡ºæ›´åŠ ç²¾ç®€ã€‚æˆ‘ä»¬çœ‹åˆ°ä¸æ¯ä¸ªèŠ‚ç‚¹åç§°å…³è”çš„æ›´æ–°ã€‚`'refine_topic'` æ›´æ–°æ˜¾ç¤ºäº† `topic` çš„æ›´æ”¹ï¼Œ`'generate_joke'` æ›´æ–°æ˜¾ç¤ºäº†æ–°ç”Ÿæˆçš„ `joke`ã€‚æ­¤æ¨¡å¼æœ‰æ•ˆåœ°éš”ç¦»å¹¶ä»…å‘ˆç°æ¯ä¸ªæ­¥éª¤ä¸­å‘ç”Ÿçš„æ›´æ”¹ï¼Œä½¿å…¶æˆä¸ºæœ‰é’ˆå¯¹æ€§çš„ã€é€æ­¥ UI æ›´æ–°çš„ç†æƒ³é€‰æ‹©ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rnwoinhkbwq",
   "metadata": {},
   "source": [
    "#### 4.1.1.3 æµå¼å¤„ç†æ¨¡å¼ï¼š`\"custom\"`\n",
    "\n",
    "`\"custom\"` æµå¼å¤„ç†æ¨¡å¼æä¾›äº†æè‡´çš„çµæ´»æ€§ï¼Œå…è®¸å¼€å‘äººå‘˜ä»å…¶ LangGraph èŠ‚ç‚¹å†…éƒ¨æµå¼ä¼ è¾“ä»»æ„æ•°æ®ã€‚è¿™æ˜¯é€šè¿‡ `StreamWriter` å¯¹è±¡å®ç°çš„ï¼Œè¯¥å¯¹è±¡å¯ä»¥åœ¨ä»»ä½•èŠ‚ç‚¹å‡½æ•°ä¸­è®¿é—®ã€‚èŠ‚ç‚¹å¯ä»¥ä½¿ç”¨ `StreamWriter` åœ¨å…¶æ‰§è¡ŒæœŸé—´çš„ä»»ä½•æ—¶é—´ç‚¹å°†ä»»ä½•ç±»å‹çš„æ•°æ®å‘é€åˆ°æµä¸­ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gv9b7hm2si",
   "metadata": {},
   "source": [
    "##### ç¤ºä¾‹ 4-4ï¼šä¿®æ”¹ `generate_joke` èŠ‚ç‚¹ä»¥ä½¿ç”¨ `StreamWriter`\n",
    "\n",
    "è®©æˆ‘ä»¬ä¿®æ”¹æˆ‘ä»¬çš„ `generate_joke` èŠ‚ç‚¹ï¼Œä½¿å…¶ä½¿ç”¨ `StreamWriter` åœ¨è¿”å›ç¬‘è¯ä¹‹å‰å‘é€ä¸€æ¡è‡ªå®šä¹‰æ¶ˆæ¯ï¼š\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "y05l5d4uyyr",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.types import StreamWriter\n",
    "\n",
    "def generate_joke_with_custom(state: State, writer: StreamWriter):\n",
    "    writer({\"custom_key\": \"Writing custom data while generating a joke\"})\n",
    "    return {\"joke\": f\"This is a joke about {state['topic']}\"}\n",
    "\n",
    "graph_custom = (\n",
    "    StateGraph(State)\n",
    "    .add_node(refine_topic)\n",
    "    .add_node(\"generate_joke\", generate_joke_with_custom)\n",
    "    .add_edge(START, \"refine_topic\")\n",
    "    .add_edge(\"refine_topic\", \"generate_joke\")\n",
    "    .compile()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n5708j667ah",
   "metadata": {},
   "source": [
    "##### ç¤ºä¾‹ 4-5ï¼šä½¿ç”¨ `stream_mode=\"custom\"` æ‰§è¡Œå›¾\n",
    "\n",
    "ç°åœ¨ï¼Œä½¿ç”¨ `stream_mode=\"custom\"` è¿è¡Œä¼šäº§ç”Ÿä»¥ä¸‹ç»“æœï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "v24rba5v9y",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'custom_key': 'Writing custom data while generating a joke'}\n"
     ]
    }
   ],
   "source": [
    "for chunk in graph_custom.stream(\n",
    "    {\"topic\": \"ice cream\"},\n",
    "    stream_mode=\"custom\",\n",
    "):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0o5neb985mxm",
   "metadata": {},
   "source": [
    "**ğŸ’¡ è‡ªå®šä¹‰æµå¼å¤„ç†çš„å¨åŠ›**ï¼š\n",
    "\n",
    "å¦‚ç¤ºä¾‹æ‰€ç¤ºï¼Œæˆ‘ä»¬æ”¶åˆ°äº†é€šè¿‡ `StreamWriter` å‘é€çš„è‡ªå®šä¹‰å­—å…¸ã€‚æ­¤æ¨¡å¼å…è®¸å°†ä»»ä½•ç›¸å…³ä¿¡æ¯åµŒå…¥åˆ°æµä¸­ï¼Œä½¿å…¶é«˜åº¦é€‚åº”ç‰¹å®šçš„åº”ç”¨ç¨‹åºéœ€æ±‚ã€‚ç»“åˆå·¥å…·ä½¿ç”¨æ—¶ï¼Œ`\"custom\"` æµå¼å¤„ç†åœ¨æä¾›å¯¹å¤æ‚æ™ºèƒ½ä½“è¡Œä¸ºçš„å®æ—¶æ´å¯Ÿæ–¹é¢å˜å¾—æ›´åŠ å¼ºå¤§ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s4oxttjhf6",
   "metadata": {},
   "source": [
    "#### 4.1.1.4 æµå¼å¤„ç†æ¨¡å¼ï¼š`\"messages\"`\n",
    "\n",
    "å¯¹äºæ¶‰åŠ LLM äº¤äº’çš„åº”ç”¨ç¨‹åºï¼Œå°¤å…¶æ˜¯å¯¹è¯å¼æ™ºèƒ½ä½“ï¼Œ`\"messages\"` æµå¼å¤„ç†æ¨¡å¼éå¸¸å®è´µã€‚æ­¤æ¨¡å¼æµå¼ä¼ è¾“ LangGraph ä¸­ LLM ç”Ÿæˆçš„å•ä¸ªä»¤ç‰Œï¼Œä»¥åŠç›¸å…³çš„å…ƒæ•°æ®ã€‚è¿™ç§ä»¤ç‰Œçº§æµå¼å¤„ç†å…è®¸å®ç°æœ€ç»†ç²’åº¦çš„å®æ—¶åé¦ˆï¼Œä»è€Œåœ¨èŠå¤©æœºå™¨äººå’Œå…¶ä»–æ–‡æœ¬ç”Ÿæˆåº”ç”¨ç¨‹åºä¸­å®ç°\"æ‰“å­—æ•ˆæœ\"ç•Œé¢ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wtq85u84ijm",
   "metadata": {},
   "source": [
    "##### ç¤ºä¾‹ 4-6ï¼šä½¿ç”¨ LLM ç”Ÿæˆç¬‘è¯å¹¶å¯ç”¨ `stream_mode=\"messages\"`\n",
    "\n",
    "è®©æˆ‘ä»¬å¢å¼ºæˆ‘ä»¬çš„ç¬‘è¯ç”Ÿæˆå›¾ï¼Œä½¿ç”¨ LLM ç”Ÿæˆç¬‘è¯ï¼Œå¹¶è§‚å¯Ÿ `\"messages\"` æµï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0qex4xeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"Qwen/Qwen2.5-7B-Instruct\")\n",
    "\n",
    "def generate_joke_with_llm(state: State):\n",
    "    llm_response = llm.invoke(\n",
    "        [{\"role\": \"user\", \"content\": f\"Generate a joke about {state['topic']}\"}]\n",
    "    )\n",
    "    return {\"joke\": llm_response.content}\n",
    "\n",
    "graph_with_llm = (\n",
    "    StateGraph(State)\n",
    "    .add_node(refine_topic)\n",
    "    .add_node(\"generate_joke\", generate_joke_with_llm)\n",
    "    .add_edge(START, \"refine_topic\")\n",
    "    .add_edge(\"refine_topic\", \"generate_joke\")\n",
    "    .compile()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "w1f7h1k3mb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why| did| the| cat| sit| on| the| ice| cream| cone|?\n",
      "\n",
      "|Because| he| wanted| to| see| if| it| would| turn| into| a| laser| pointer|-fl|avored| dessert|!|"
     ]
    }
   ],
   "source": [
    "for message_chunk, metadata in graph_with_llm.stream(\n",
    "    {\"topic\": \"ice cream\"},\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    if message_chunk.content:\n",
    "        print(message_chunk.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccw8ddvmx4s",
   "metadata": {},
   "source": [
    "**ğŸ’¡ è¯å…ƒçº§æµå¼å¤„ç†çš„é­…åŠ›**ï¼š\n",
    "\n",
    "æ­£å¦‚æ‚¨æ‰€çœ‹åˆ°çš„ï¼Œæˆ‘ä»¬ç°åœ¨æ”¶åˆ°äº†ä¸€ç³»åˆ—å•ä¸ªè¯å…ƒï¼Œä»è€Œå¯ä»¥é€è¯å…ƒç”šè‡³é€å­—ç¬¦åœ°æ˜¾ç¤ºæ­£åœ¨ç”Ÿæˆçš„ç¬‘è¯ã€‚éšé™„çš„ `metadata` å­—å…¸æä¾›äº†å…³äºæ¯ä¸ªè¯å…ƒçš„å®è´µä¸Šä¸‹æ–‡ã€‚è¿™ç§ç»†ç²’åº¦çš„æµå¼å¤„ç†æ˜¯æ„å»ºçœŸæ­£äº¤äº’å¼èŠå¤©åº”ç”¨ç¨‹åºçš„å…³é”®æŠ€æœ¯ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed8fe43",
   "metadata": {},
   "source": [
    "#### 4.1.1.5 æµå¼å¤„ç†æ¨¡å¼ï¼š`\"debug\"`\n",
    "\n",
    "è°ƒè¯•æµæ¨¡å¼ä¸“ä¸ºéœ€è¦å…¨é¢ç›‘æ§ LangGraph æ‰§è¡Œè¿‡ç¨‹çš„å¼€å‘è€…è®¾è®¡ã€‚è¯¥æ¨¡å¼ä¼šæµ\n",
    "å¼ä¼ è¾“åŒ…å«ä¸°å¯Œè°ƒè¯•ä¿¡æ¯çš„äº‹ä»¶æµï¼Œåœ¨å›¾æ‰§è¡Œçš„æ¯ä¸ªæ­¥éª¤æä¾›è¯¦ç»†æ•°æ®ï¼Œæ¶µç›–ä»»åŠ¡è°ƒ\n",
    "åº¦ã€æ‰§è¡Œç»“æœã€é”™è¯¯ä¿¡æ¯åŠçŠ¶æ€è½¬æ¢ç­‰å…³é”®äº‹ä»¶ã€‚è°ƒè¯•æµæ¨¡å¼è™½ç„¶å› å…¶æŠ€æœ¯æ€§å’Œä¿¡æ¯\n",
    "å¯†åº¦è¾ƒé«˜è€Œä¸é€‚åˆç›´æ¥é¢å‘ç”¨æˆ·å±•ç¤ºï¼Œä½†åœ¨å¼€å‘è°ƒè¯•é˜¶æ®µå’Œéœ€è¦æ·±åº¦ç›‘æ§çš„åœºæ™¯ä¸­ä¸º\n",
    "å¼€å‘è€…æä¾›äº†ä¸å¯æˆ–ç¼ºçš„åˆ†æå·¥å…·ï¼Œç‰¹åˆ«æœ‰åŠ©äºè¿›è¡Œæ€§èƒ½ä¼˜åŒ–å’Œå¤æ‚è¡Œä¸ºåˆ†æã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e3455b",
   "metadata": {},
   "source": [
    "##### ç¤ºä¾‹ 4-7ï¼šä½¿ç”¨ stream_mode=\"debug\" æ‰§è¡Œå›¾çš„è¾“å‡º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "113a5b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'step': 1, 'timestamp': '2025-08-20T13:50:34.979202+00:00', 'type': 'task', 'payload': {'id': 'bf38cfd7-04b6-a0c8-38d5-797dfc4b0771', 'name': 'refine_topic', 'input': {'topic': 'ice cream'}, 'triggers': ('branch:to:refine_topic',)}}\n",
      "{'step': 1, 'timestamp': '2025-08-20T13:50:34.980498+00:00', 'type': 'task_result', 'payload': {'id': 'bf38cfd7-04b6-a0c8-38d5-797dfc4b0771', 'name': 'refine_topic', 'error': None, 'result': [('topic', 'ice cream and cats')], 'interrupts': []}}\n",
      "{'step': 2, 'timestamp': '2025-08-20T13:50:34.980693+00:00', 'type': 'task', 'payload': {'id': 'af02992f-8f49-42e6-2689-9d685d0be885', 'name': 'generate_joke', 'input': {'topic': 'ice cream and cats'}, 'triggers': ('branch:to:generate_joke',)}}\n",
      "{'step': 2, 'timestamp': '2025-08-20T13:50:34.981762+00:00', 'type': 'task_result', 'payload': {'id': 'af02992f-8f49-42e6-2689-9d685d0be885', 'name': 'generate_joke', 'error': None, 'result': [('joke', 'This is a joke about ice cream and cats')], 'interrupts': []}}\n"
     ]
    }
   ],
   "source": [
    "for chunk in graph_custom.stream(\n",
    "    {\"topic\": \"ice cream\"},\n",
    "    stream_mode=\"debug\",\n",
    "):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8c932d",
   "metadata": {},
   "source": [
    "#### 4.1.1.7 ç»„åˆæµå¼å¤„ç†æ¨¡å¼\n",
    "\n",
    "LangGraph æ”¯æŒåŒæ—¶ç»„åˆå¤šç§æµå¼å¤„ç†æ¨¡å¼ï¼Œé€šè¿‡å°†æµæ¨¡å¼å­—ç¬¦ä¸²åˆ—è¡¨ä¼ é€’ç»™\n",
    " `stream_mode` å‚æ•°ï¼Œå¼€å‘è€…å¯ä»¥æ¥æ”¶åŒ…å«å¤šç§æ¨¡å¼æ•°æ®çš„äº¤é”™æµã€‚ç»„åˆæ¨¡å¼æ—¶ï¼Œæµå¼å¤„ç†è¾“å‡ºå°†å˜ä¸º `(stream_mode, data)` å…ƒç»„ï¼Œå…¶ä¸­ `stream_mode` æŒ‡ç¤ºæ•°æ®ç±»å‹ï¼Œdata æ˜¯è¯¥æ¨¡å¼çš„å®é™…æµå¼å†…å®¹ã€‚æ­¤åŠŸèƒ½æ”¯æŒæ›´ä¸°å¯Œçš„ç›‘æ§å’Œç”¨æˆ·åé¦ˆåœºæ™¯ï¼Œä¾‹å¦‚åŒæ—¶æŸ¥çœ‹é«˜çº§çŠ¶æ€æ›´æ–°å’Œç»†ç²’åº¦çš„ LLM è¯å…ƒæµã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a469643",
   "metadata": {},
   "source": [
    "##### ç¤ºä¾‹ 4-8ï¼šç»„åˆæµå¼å¤„ç†æ¨¡å¼çš„è¾“å‡º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03ed76bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream mode: updates\n",
      "{'refine_topic': {'topic': 'ice cream and cats'}}\n",
      "\n",
      "\n",
      "Stream mode: custom\n",
      "{'custom_key': 'Writing custom data wHITLe generating a joke'}\n",
      "\n",
      "\n",
      "Stream mode: updates\n",
      "{'generate_joke': {'joke': 'This is a joke about ice cream and cats'}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langgraph.types import StreamWriter\n",
    "\n",
    "def generate_joke(state: State, writer: StreamWriter):\n",
    "    writer({\"custom_key\": \"Writing custom data wHITLe generating a joke\"})\n",
    "    return {\"joke\": f\"This is a joke about {state['topic']}\"}\n",
    "\n",
    "graph = (\n",
    "    StateGraph(State)\n",
    "    .add_node(refine_topic)\n",
    "    .add_node(generate_joke)\n",
    "    .add_edge(START, \"refine_topic\")\n",
    "    .add_edge(\"refine_topic\", \"generate_joke\")\n",
    "    .compile()\n",
    ")\n",
    "\n",
    "for stream_mode, chunk in graph.stream(\n",
    "    {\"topic\": \"ice cream\"},\n",
    "    stream_mode=[\"updates\", \"custom\"],\n",
    "):\n",
    "    print(f\"Stream mode: {stream_mode}\")\n",
    "    print(chunk)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aeef285",
   "metadata": {},
   "source": [
    "### 4.1.2 äº‹ä»¶æµå¼å¤„ç†\n",
    "\n",
    "é™¤äº† `stream()` å’Œ `astream()` æä¾›çš„åŸºäºæ¨¡å¼çš„æµå¼å¤„ç†ï¼ŒLangGraph è¿˜æä¾›äº† `astream_events()` æ–¹æ³•ï¼Œç”¨äºè®¿é—®å›¾ä¸­æ‰§è¡ŒæœŸé—´å‘ç”Ÿçš„è¾ƒä½çº§åˆ«äº‹ä»¶æµã€‚æ­¤æ–¹æ³•é€‚ç”¨äºæ•è·èŠ‚ç‚¹å†…éƒ¨äº‹ä»¶ï¼Œä¾¿äºå¼€å‘è€…æ›´ç²¾ç»†åœ°äº†è§£å›¾çš„å†…éƒ¨å·¥ä½œåŸç†ã€‚\n",
    "\n",
    "`astream_events()` ä¸ LangChain å¯¹è±¡ä¸­æ ‡å‡†çš„äº‹ä»¶æµå¼å¤„ç†æ¥å£ä¸€è‡´ï¼Œä½¿å…¶ç¬¦åˆ LangChain ç”Ÿæ€ç³»ç»Ÿå¼€å‘è€…çš„ä½¿ç”¨ä¹ æƒ¯ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b79cb41",
   "metadata": {},
   "source": [
    "##### ç¤ºä¾‹ 4-9ï¼šä½¿ç”¨ astream_events æ–¹æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28f63639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on_chain_start: LangGraph\n",
      "on_chain_start: call_model\n",
      "on_chat_model_start: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_stream: ChatOpenAI\n",
      "on_chat_model_end: ChatOpenAI\n",
      "on_chain_stream: call_model\n",
      "on_chain_end: call_model\n",
      "on_chain_stream: LangGraph\n",
      "on_chain_end: LangGraph\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "\n",
    "model = ChatOpenAI(model=\"Qwen/Qwen3-8B\")\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state['messages'])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "workflow = StateGraph(MessagesState)\n",
    "workflow.add_node(call_model)\n",
    "workflow.add_edge(START, \"call_model\")\n",
    "workflow.add_edge(\"call_model\", END)\n",
    "app = workflow.compile()\n",
    "\n",
    "inputs = [{\"role\": \"user\", \"content\": \"hi!\"}]\n",
    "async for event in app.astream_events({\"messages\": inputs}, version=\"v1\"):\n",
    "    kind = event[\"event\"]\n",
    "    print(f\"{kind}: {event['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mmpfx3bjxrk",
   "metadata": {},
   "source": [
    "## 4.2 ç¡®ä¿è¿ç»­æ€§ï¼šLangGraph ä¸­çš„æŒä¹…åŒ–\n",
    "\n",
    "åœ¨ä¸Šä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†æµå¼å¤„ç†ä½œä¸ºä¸€ç§å¢å¼º AI æ™ºèƒ½ä½“å®æ—¶å“åº”æ€§çš„æœºåˆ¶ã€‚ç„¶è€Œï¼Œä¸€ä¸ªçœŸæ­£å¼ºå¤§ä¸”ä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒçš„äººå·¥æ™ºèƒ½æ™ºèƒ½ä½“éœ€è¦çš„ä¸ä»…ä»…æ˜¯é€Ÿåº¦ï¼›è¿˜éœ€è¦è¿ç»­æ€§ã€‚ç”¨æˆ·æœŸæœ›æ™ºèƒ½ä½“èƒ½å¤Ÿè®°ä½è¿‡å»çš„äº¤äº’ï¼Œè·¨å¤šä¸ªä¼šè¯ä¿æŒä¸Šä¸‹æ–‡ï¼Œå¹¶èƒ½ä¼˜é›…åœ°ä»ä¸­æ–­æˆ–é”™è¯¯ä¸­æ¢å¤ã€‚\n",
    "\n",
    "LangGraph ä¸­çš„æŒä¹…åŒ–ï¼Œé€šè¿‡å…¶å†…ç½®çš„å­˜æ¡£ç‚¹ç³»ç»Ÿå®ç°ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨äº¤äº’è¿‡ç¨‹ä¸­ä¿æŒå…¶çŠ¶æ€ï¼Œä»è€Œå®ç°ä¸€ç³»åˆ—é«˜çº§åŠŸèƒ½ï¼š\n",
    "- **å¯¹è¯è®°å¿†**ï¼šè·¨ä¼šè¯ä¿æŒå¯¹è¯å†å²\n",
    "- **æ–­ç‚¹ç»­ä¼ **ï¼šä»ä¸­æ–­ç‚¹æ¢å¤æ‰§è¡Œ\n",
    "- **æ—¶é—´æ—…è¡Œè°ƒè¯•**ï¼šå›æº¯åˆ°å†å²çŠ¶æ€è¿›è¡Œè°ƒè¯•\n",
    "- **çŠ¶æ€åˆ†æ”¯**ï¼šä»å†å²çŠ¶æ€åˆ›å»ºæ–°çš„æ‰§è¡Œè·¯å¾„\n",
    "\n",
    "### 4.2.1 æŒä¹…åŒ–çš„æ ¸å¿ƒæ¦‚å¿µï¼šçº¿ç¨‹å’Œå­˜æ¡£ç‚¹\n",
    "\n",
    "åœ¨ LangGraph ä¸­ï¼Œ**çº¿ç¨‹**ä»£è¡¨å›¾çš„ç‹¬ç‰¹æ‰§è¡Œä¸Šä¸‹æ–‡ã€‚æ¯ä¸ªçº¿ç¨‹éƒ½ç”±å”¯ä¸€çš„ `thread_id` æ ‡è¯†ã€‚**å­˜æ¡£ç‚¹**æ˜¯å›¾åœ¨ç‰¹å®šæ—¶é—´ç‚¹çš„çŠ¶æ€å¿«ç…§ã€‚å½“å¯ç”¨æŒä¹…åŒ–æ—¶ï¼ŒLangGraph ä¼šåœ¨å›¾æ‰§è¡Œçš„æ¯ä¸ªè¶…æ­¥åè‡ªåŠ¨åˆ›å»ºè¿™äº›å¿«ç…§ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dhobctgmi6g",
   "metadata": {},
   "source": [
    "### 4.2.2 å­˜æ¡£ç‚¹å™¨å®ç°ï¼šé€‰æ‹©æ‚¨çš„æŒä¹…æ€§åç«¯\n",
    "\n",
    "LangGraph æä¾›äº†å‡ ç§å†…ç½®çš„å­˜æ¡£ç‚¹å™¨å®ç°ï¼Œæ¯ç§å®ç°éƒ½åˆ©ç”¨ä¸åŒçš„å­˜å‚¨åç«¯ã€‚\n",
    "\n",
    "#### MemorySaverï¼šå†…å­˜å­˜æ¡£ç‚¹\n",
    "\n",
    "- **ä¼˜ç‚¹**ï¼šå¿«é€Ÿã€æ˜“äºè®¾ç½®\n",
    "- **ç¼ºç‚¹**ï¼šæ•°æ®ä¸æŒä¹…ï¼Œåº”ç”¨é‡å¯åä¸¢å¤±\n",
    "- **ç”¨é€”**ï¼šå¼€å‘ã€æµ‹è¯•ã€åŸå‹è®¾è®¡\n",
    "\n",
    "#### SqliteSaverï¼šåŸºäºæ–‡ä»¶çš„æŒä¹…åŒ–\n",
    "\n",
    "- **ä¼˜ç‚¹**ï¼šè½»é‡çº§ã€è·¨åº”ç”¨é‡å¯æŒä¹…\n",
    "- **ç¼ºç‚¹**ï¼šä¸é€‚åˆé«˜å¹¶å‘åœºæ™¯\n",
    "- **ç”¨é€”**ï¼šæœ¬åœ°åº”ç”¨ã€å°è§„æ¨¡éƒ¨ç½²\n",
    "\n",
    "#### PostgresSaverï¼šç”Ÿäº§çº§æŒä¹…åŒ–\n",
    "\n",
    "- **ä¼˜ç‚¹**ï¼šå¯é ã€å¯æ‰©å±•ã€æ”¯æŒå¹¶å‘\n",
    "- **ç¼ºç‚¹**ï¼šéœ€è¦æ•°æ®åº“æœåŠ¡å™¨\n",
    "- **ç”¨é€”**ï¼šç”Ÿäº§ç¯å¢ƒã€å¤šç”¨æˆ·åº”ç”¨\n",
    "\n",
    "è®©æˆ‘ä»¬é€šè¿‡å®é™…ä¾‹å­æ¥æ¼”ç¤ºæŒä¹…åŒ–çš„ä½¿ç”¨ï¼š\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6h4p0hrc4ft",
   "metadata": {},
   "source": [
    "##### ç¤ºä¾‹ 4-10 & 4-11ï¼šä½¿ç”¨ MemorySaver å®ç°æŒä¹…åŒ–çš„ç¬‘è¯ç”Ÿæˆå™¨\n",
    "\n",
    "è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå¸¦æœ‰æŒä¹…åŒ–åŠŸèƒ½çš„ç¬‘è¯ç”Ÿæˆå™¨ï¼Œæ¼”ç¤ºå¦‚ä½•ä¿å­˜å’Œæ¢å¤çŠ¶æ€ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6dv850lkmkn",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç¬¬ä¸€æ¬¡è¿è¡Œï¼š\n",
      "{'refine_topic': {'topic': 'ice cream and cats'}}\n",
      "{'generate_joke': {'joke': 'Why did the cat wear a scarf around its ice cream cone?\\n\\nBecause it was a chilly day and the cat wanted to keep its \"paws\" cool!'}}\n",
      "\n",
      "è·å–æœ€ç»ˆçŠ¶æ€ï¼š\n",
      "{'topic': 'ice cream and cats', 'joke': 'Why did the cat wear a scarf around its ice cream cone?\\n\\nBecause it was a chilly day and the cat wanted to keep its \"paws\" cool!'}\n"
     ]
    }
   ],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# ä½¿ç”¨ LLM ç‰ˆæœ¬çš„ç¬‘è¯ç”Ÿæˆå™¨ï¼Œä½†æ·»åŠ æŒä¹…åŒ–\n",
    "graph_persistent = (\n",
    "    StateGraph(State)\n",
    "    .add_node(refine_topic)\n",
    "    .add_node(\"generate_joke\", generate_joke_with_llm)\n",
    "    .add_edge(START, \"refine_topic\")\n",
    "    .add_edge(\"refine_topic\", \"generate_joke\")\n",
    "    .compile(checkpointer=MemorySaver())  # å¯ç”¨æŒä¹…åŒ–\n",
    ")\n",
    "\n",
    "# å®šä¹‰çº¿ç¨‹é…ç½®\n",
    "config = {\"configurable\": {\"thread_id\": \"my_thread_1\"}}\n",
    "\n",
    "print(\"ç¬¬ä¸€æ¬¡è¿è¡Œï¼š\")\n",
    "for chunk in graph_persistent.stream(\n",
    "    {\"topic\": \"ice cream\"},\n",
    "    config=config,\n",
    "    stream_mode=\"updates\",\n",
    "):\n",
    "    print(chunk)\n",
    "\n",
    "print(\"\\nè·å–æœ€ç»ˆçŠ¶æ€ï¼š\")\n",
    "print(graph_persistent.get_state(config).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525a0f61",
   "metadata": {},
   "source": [
    "##### ç¤ºä¾‹ 4-12ï¼šä½¿ç”¨ SqliteSaver ç¼–è¯‘å…·æœ‰æŒä¹…åŒ–çš„ LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "215608aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç¬¬ä¸€æ¬¡è¿è¡Œï¼š\n",
      "{'refine_topic': {'topic': 'ice cream and cats'}}\n",
      "{'generate_joke': {'joke': 'Why did the cat refuse to eat the ice creamï¼Ÿ\\n\\nBecause it was purr-fectly chilled!'}}\n",
      "\n",
      "è·å–æœ€ç»ˆçŠ¶æ€ï¼š\n",
      "{'topic': 'ice cream and cats', 'joke': 'Why did the cat refuse to eat the ice creamï¼Ÿ\\n\\nBecause it was purr-fectly chilled!'}\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "conn = sqlite3.connect(\"checkpoints.sqlite\", check_same_thread=False)\n",
    "checkpointer = SqliteSaver(conn)\n",
    "\n",
    "# ä½¿ç”¨ LLM ç‰ˆæœ¬çš„ç¬‘è¯ç”Ÿæˆå™¨ï¼Œä½†æ·»åŠ æŒä¹…åŒ–\n",
    "graph_persistent = (\n",
    "    StateGraph(State)\n",
    "    .add_node(refine_topic)\n",
    "    .add_node(\"generate_joke\", generate_joke_with_llm)\n",
    "    .add_edge(START, \"refine_topic\")\n",
    "    .add_edge(\"refine_topic\", \"generate_joke\")\n",
    "    .compile(checkpointer=checkpointer)  # å¯ç”¨æŒä¹…åŒ–\n",
    ")\n",
    "\n",
    "# å®šä¹‰çº¿ç¨‹é…ç½®\n",
    "config = {\"configurable\": {\"thread_id\": \"my_thread_1\"}}\n",
    "\n",
    "print(\"ç¬¬ä¸€æ¬¡è¿è¡Œï¼š\")\n",
    "for chunk in graph_persistent.stream(\n",
    "    {\"topic\": \"ice cream\"},\n",
    "    config=config,\n",
    "    stream_mode=\"updates\",\n",
    "):\n",
    "    print(chunk)\n",
    "\n",
    "print(\"\\nè·å–æœ€ç»ˆçŠ¶æ€ï¼š\")\n",
    "print(graph_persistent.get_state(config).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xujpn9l6hk8",
   "metadata": {},
   "source": [
    "##### ç¤ºä¾‹ 4-15ï¼šä½¿ç”¨ graph.get_state_history æµè§ˆæ‰§è¡Œå†å²è®°å½•\n",
    "\n",
    "æŒä¹…åŒ–çš„ä¸€ä¸ªå¼ºå¤§åŠŸèƒ½æ˜¯èƒ½å¤ŸæŸ¥çœ‹å®Œæ•´çš„çŠ¶æ€å†å²ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ev4wggaxa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "çŠ¶æ€å†å²å›é¡¾ï¼š\n",
      "\n",
      "å­˜æ¡£ç‚¹ 1:\n",
      "  ID: 1f07dcca...\n",
      "  æ­¥éª¤: {'source': 'loop', 'step': 2, 'parents': {}}\n",
      "  çŠ¶æ€: {'topic': 'ice cream and cats', 'joke': 'Why did the cat refuse to eat the ice creamï¼Ÿ\\n\\nBecause it was purr-fectly chilled!'}\n",
      "  ä¸‹ä¸€ä¸ªèŠ‚ç‚¹: ()\n",
      "--------------------------------------------------\n",
      "\n",
      "å­˜æ¡£ç‚¹ 2:\n",
      "  ID: 1f07dcca...\n",
      "  æ­¥éª¤: {'source': 'loop', 'step': 1, 'parents': {}}\n",
      "  çŠ¶æ€: {'topic': 'ice cream and cats'}\n",
      "  ä¸‹ä¸€ä¸ªèŠ‚ç‚¹: ('generate_joke',)\n",
      "--------------------------------------------------\n",
      "\n",
      "å­˜æ¡£ç‚¹ 3:\n",
      "  ID: 1f07dcca...\n",
      "  æ­¥éª¤: {'source': 'loop', 'step': 0, 'parents': {}}\n",
      "  çŠ¶æ€: {'topic': 'ice cream'}\n",
      "  ä¸‹ä¸€ä¸ªèŠ‚ç‚¹: ('refine_topic',)\n",
      "--------------------------------------------------\n",
      "\n",
      "å­˜æ¡£ç‚¹ 4:\n",
      "  ID: 1f07dcca...\n",
      "  æ­¥éª¤: {'source': 'input', 'step': -1, 'parents': {}}\n",
      "  çŠ¶æ€: {}\n",
      "  ä¸‹ä¸€ä¸ªèŠ‚ç‚¹: ('__start__',)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"çŠ¶æ€å†å²å›é¡¾ï¼š\")\n",
    "state_history = list(graph_persistent.get_state_history(config))\n",
    "for i, snapshot in enumerate(state_history):\n",
    "    print(f\"\\nå­˜æ¡£ç‚¹ {i + 1}:\")\n",
    "    print(f\"  ID: {snapshot.config['configurable']['checkpoint_id'][:8]}...\")\n",
    "    print(f\"  æ­¥éª¤: {snapshot.metadata}\")\n",
    "    print(f\"  çŠ¶æ€: {snapshot.values}\")\n",
    "    print(f\"  ä¸‹ä¸€ä¸ªèŠ‚ç‚¹: {snapshot.next}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4jc25pz882o",
   "metadata": {},
   "source": [
    "**ğŸ’¡ çŠ¶æ€å†å²çš„ä»·å€¼**ï¼š\n",
    "\n",
    "é€šè¿‡æŸ¥çœ‹çŠ¶æ€å†å²ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼š\n",
    "- æ¯ä¸ªå­˜æ¡£ç‚¹çš„å”¯ä¸€ ID\n",
    "- æ‰§è¡Œçš„æ­¥éª¤å’Œå…ƒæ•°æ®ä¿¡æ¯\n",
    "- æ¯ä¸ªé˜¶æ®µçš„å®Œæ•´çŠ¶æ€\n",
    "- è®¡åˆ’æ‰§è¡Œçš„ä¸‹ä¸€ä¸ªèŠ‚ç‚¹\n",
    "\n",
    "è¿™å¯¹äºè°ƒè¯•ã€ç›‘æ§å’Œç†è§£æ™ºèƒ½ä½“çš„è¡Œä¸ºéå¸¸æœ‰ä»·å€¼ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e508b6",
   "metadata": {},
   "source": [
    "##### ç¤ºä¾‹ 4-16ï¼šä½¿ç”¨ graph.stream(..., checkpoint_id=...) é‡æ”¾æ‰§è¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04bba284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é‡æ”¾æ£€æŸ¥ç‚¹ï¼š{'topic': 'ice cream and cats'}\n",
      "ä¸‹ä¸€ä¸ªèŠ‚ç‚¹ï¼š('generate_joke',)\n",
      "\n",
      "ä»æ£€æŸ¥ç‚¹ç»§ç»­æ‰§è¡Œï¼š\n",
      "{'topic': 'ice cream and cats'}\n",
      "{'topic': 'ice cream and cats', 'joke': 'Why did the cat join the ice cream shop\\'s loyalty program?\\n\\nBecause she heard they had a \"paw-some\" reward for paws-singly good reviewers!'}\n"
     ]
    }
   ],
   "source": [
    "# æ­£ç¡®çš„é‡æ”¾æ–¹å¼ï¼šä»æ£€æŸ¥ç‚¹ç»§ç»­æ‰§è¡Œ\n",
    "# é€‰æ‹©ä¸€ä¸ªä¸­é—´å­˜æ¡£ç‚¹ï¼ˆæœ‰ä¸‹ä¸€ä¸ªæ­¥éª¤çš„ï¼‰\n",
    "checkpoint_to_replay = state_history[1]  # ç¬¬2ä¸ªå­˜æ¡£ç‚¹ï¼ŒçŠ¶æ€ä¸ºåˆšå®Œæˆrefine_topic\n",
    "print(f\"é‡æ”¾æ£€æŸ¥ç‚¹ï¼š{checkpoint_to_replay.values}\")\n",
    "print(f\"ä¸‹ä¸€ä¸ªèŠ‚ç‚¹ï¼š{checkpoint_to_replay.next}\")\n",
    "\n",
    "# ä»è¯¥æ£€æŸ¥ç‚¹ç»§ç»­æ‰§è¡Œ\n",
    "replay_config = checkpoint_to_replay.config\n",
    "print(\"\\nä»æ£€æŸ¥ç‚¹ç»§ç»­æ‰§è¡Œï¼š\")\n",
    "for event in graph_persistent.stream(None, replay_config, stream_mode=\"values\"):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24381069",
   "metadata": {},
   "source": [
    "##### ç¤ºä¾‹ 4-17ï¼šä½¿ç”¨ graph.update_state(..., values) ä»å­˜æ¡£ç‚¹åˆ†æ”¯æ‰§è¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c3c6d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'topic': 'ice cream and dogs'}\n",
      "{'topic': 'ice cream and dogs', 'joke': 'Why did the dog refuse the ice cream?\\n\\nBecause he heard it was \"paw-sicly\" good!'}\n"
     ]
    }
   ],
   "source": [
    "checkpoint_to_branch = state_history[1] # è®©æˆ‘ä»¬ä»å†å²è®°å½•ä¸­çš„ç¬¬ 2 ä¸ªå­˜æ¡£ç‚¹åˆ†æ”¯\n",
    "branch_config = checkpoint_to_branch.config\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ªæ–°å­˜æ¡£ç‚¹ï¼Œä½¿ç”¨ç›¸åŒçš„ checkpoint_id ä½†æ›´æ–°çŠ¶æ€\n",
    "new_branch = graph_persistent.update_state(\n",
    "    branch_config,\n",
    "    {\"topic\": \"ice cream and dogs\"}\n",
    ")\n",
    "\n",
    "# æ¢å¤å›¾çš„æ‰§è¡Œ - å®ƒç°åœ¨å°†ä½¿ç”¨åˆ†æ”¯çš„å­˜æ¡£ç‚¹\n",
    "for event in graph_persistent.stream(None, new_branch, stream_mode=\"values\"):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ca9ba0",
   "metadata": {},
   "source": [
    "##### ç¤ºä¾‹ 4-18ï¼šè·å–å­å›¾çŠ¶æ€å¿«ç…§\n",
    "\n",
    "è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªåŒ…å«å­å›¾çš„å¤æ‚æ™ºèƒ½ä½“ç³»ç»Ÿï¼Œæ¼”ç¤ºå±‚æ¬¡åŒ–çŠ¶æ€ç®¡ç†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d2fe22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# å®šä¹‰å­å›¾çŠ¶æ€\n",
    "class SubtaskState(TypedDict):\n",
    "    subtask_input: str\n",
    "    subtask_result: str\n",
    "\n",
    "# å®šä¹‰ä¸»å›¾çŠ¶æ€  \n",
    "class MainState(TypedDict):\n",
    "    main_topic: str\n",
    "    processed_data: str\n",
    "    final_output: str\n",
    "\n",
    "# å­å›¾èŠ‚ç‚¹å‡½æ•°\n",
    "def subtask_processor(state: SubtaskState):\n",
    "    \"\"\"å­å›¾ä¸­çš„å¤„ç†èŠ‚ç‚¹\"\"\"\n",
    "    return {\n",
    "        \"subtask_result\": f\"å¤„ç†å®Œæˆ: {state['subtask_input']} -> å­ä»»åŠ¡ç»“æœ\"\n",
    "    }\n",
    "\n",
    "def subtask_formatter(state: SubtaskState):\n",
    "    \"\"\"å­å›¾ä¸­çš„æ ¼å¼åŒ–èŠ‚ç‚¹\"\"\"\n",
    "    return {\n",
    "        \"subtask_result\": f\"[æ ¼å¼åŒ–] {state['subtask_result']}\"\n",
    "    }\n",
    "\n",
    "# æ„å»ºç‹¬ç«‹çš„å­å›¾ï¼ˆå¸¦æŒä¹…åŒ–ï¼‰\n",
    "subgraph = StateGraph(SubtaskState)\n",
    "subgraph.add_node(\"process\", subtask_processor)\n",
    "subgraph.add_node(\"format\", subtask_formatter)\n",
    "subgraph.add_edge(START, \"process\")\n",
    "subgraph.add_edge(\"process\", \"format\")\n",
    "subgraph.add_edge(\"format\", END)\n",
    "compiled_subgraph = subgraph.compile(checkpointer=MemorySaver())\n",
    "\n",
    "# ä¸»å›¾èŠ‚ç‚¹å‡½æ•°\n",
    "def prepare_data(state: MainState):\n",
    "    \"\"\"å‡†å¤‡æ•°æ®\"\"\"\n",
    "    return {\n",
    "        \"processed_data\": f\"é¢„å¤„ç†: {state['main_topic']}\"\n",
    "    }\n",
    "\n",
    "def call_subgraph_node(state: MainState):\n",
    "    \"\"\"è°ƒç”¨å­å›¾çš„èŠ‚ç‚¹\"\"\"\n",
    "    # ä½¿ç”¨ç‹¬ç«‹çš„å­å›¾é…ç½®\n",
    "    subgraph_config = {\"configurable\": {\"thread_id\": f\"sub_{state['main_topic']}\"}}\n",
    "    \n",
    "    # è°ƒç”¨å­å›¾å¤„ç†\n",
    "    subgraph_input = {\"subtask_input\": state[\"processed_data\"]}\n",
    "    subgraph_result = compiled_subgraph.invoke(subgraph_input, subgraph_config)\n",
    "    \n",
    "    return {\n",
    "        \"processed_data\": subgraph_result[\"subtask_result\"]\n",
    "    }\n",
    "\n",
    "def finalize_output(state: MainState):\n",
    "    \"\"\"ç”Ÿæˆæœ€ç»ˆè¾“å‡º\"\"\"\n",
    "    return {\n",
    "        \"final_output\": f\"æœ€ç»ˆç»“æœ: {state['processed_data']}\"\n",
    "    }\n",
    "\n",
    "# æ„å»ºä¸»å›¾\n",
    "main_graph = StateGraph(MainState)\n",
    "main_graph.add_node(\"prepare\", prepare_data)\n",
    "main_graph.add_node(\"subgraph_call\", call_subgraph_node)\n",
    "main_graph.add_node(\"finalize\", finalize_output)\n",
    "\n",
    "main_graph.add_edge(START, \"prepare\")\n",
    "main_graph.add_edge(\"prepare\", \"subgraph_call\")\n",
    "main_graph.add_edge(\"subgraph_call\", \"finalize\")\n",
    "main_graph.add_edge(\"finalize\", END)\n",
    "\n",
    "# ç¼–è¯‘ä¸»å›¾ï¼ˆå¸¦æŒä¹…åŒ–ï¼‰\n",
    "hierarchical_graph = main_graph.compile(checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8c4051",
   "metadata": {},
   "source": [
    "ç°åœ¨è®©æˆ‘ä»¬è¿è¡Œå±‚æ¬¡åŒ–æ™ºèƒ½ä½“å¹¶æ¼”ç¤ºå¦‚ä½•è·å–åŒ…å«å­å›¾çš„å®Œæ•´çŠ¶æ€å¿«ç…§ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57db982c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== è¿è¡Œå±‚æ¬¡åŒ–æ™ºèƒ½ä½“ ===\n",
      "æ‰§è¡Œç»“æœï¼š {'main_topic': 'æ•°æ®åˆ†æä»»åŠ¡', 'processed_data': '[æ ¼å¼åŒ–] å¤„ç†å®Œæˆ: é¢„å¤„ç†: æ•°æ®åˆ†æä»»åŠ¡ -> å­ä»»åŠ¡ç»“æœ', 'final_output': 'æœ€ç»ˆç»“æœ: [æ ¼å¼åŒ–] å¤„ç†å®Œæˆ: é¢„å¤„ç†: æ•°æ®åˆ†æä»»åŠ¡ -> å­ä»»åŠ¡ç»“æœ'}\n",
      "\n",
      "==================================================\n",
      "=== è·å–å­å›¾çŠ¶æ€å¿«ç…§ ===\n",
      "Grandparent State (ä¸»å›¾çŠ¶æ€):\n",
      "{'main_topic': 'æ•°æ®åˆ†æä»»åŠ¡', 'processed_data': '[æ ¼å¼åŒ–] å¤„ç†å®Œæˆ: é¢„å¤„ç†: æ•°æ®åˆ†æä»»åŠ¡ -> å­ä»»åŠ¡ç»“æœ', 'final_output': 'æœ€ç»ˆç»“æœ: [æ ¼å¼åŒ–] å¤„ç†å®Œæˆ: é¢„å¤„ç†: æ•°æ®åˆ†æä»»åŠ¡ -> å­ä»»åŠ¡ç»“æœ'}\n",
      "---------------\n",
      "Parent Graph State (å­å›¾çŠ¶æ€):\n",
      "{'subtask_input': 'é¢„å¤„ç†: æ•°æ®åˆ†æä»»åŠ¡', 'subtask_result': '[æ ¼å¼åŒ–] å¤„ç†å®Œæˆ: é¢„å¤„ç†: æ•°æ®åˆ†æä»»åŠ¡ -> å­ä»»åŠ¡ç»“æœ'}\n",
      "---------------\n",
      "Subgraph State History (å­å›¾å†å²çŠ¶æ€):\n",
      "  å­å›¾å¿«ç…§ 1: {'subtask_input': 'é¢„å¤„ç†: æ•°æ®åˆ†æä»»åŠ¡', 'subtask_result': '[æ ¼å¼åŒ–] å¤„ç†å®Œæˆ: é¢„å¤„ç†: æ•°æ®åˆ†æä»»åŠ¡ -> å­ä»»åŠ¡ç»“æœ'}\n",
      "  å­å›¾å¿«ç…§ 2: {'subtask_input': 'é¢„å¤„ç†: æ•°æ®åˆ†æä»»åŠ¡', 'subtask_result': 'å¤„ç†å®Œæˆ: é¢„å¤„ç†: æ•°æ®åˆ†æä»»åŠ¡ -> å­ä»»åŠ¡ç»“æœ'}\n",
      "  å­å›¾å¿«ç…§ 3: {'subtask_input': 'é¢„å¤„ç†: æ•°æ®åˆ†æä»»åŠ¡'}\n",
      "\n",
      "=== å½“å‰ç¤ºä¾‹çš„çŠ¶æ€è®¿é—® ===\n",
      "ä¸»å›¾æœ€ç»ˆçŠ¶æ€ï¼š {'main_topic': 'æ•°æ®åˆ†æä»»åŠ¡', 'processed_data': '[æ ¼å¼åŒ–] å¤„ç†å®Œæˆ: é¢„å¤„ç†: æ•°æ®åˆ†æä»»åŠ¡ -> å­ä»»åŠ¡ç»“æœ', 'final_output': 'æœ€ç»ˆç»“æœ: [æ ¼å¼åŒ–] å¤„ç†å®Œæˆ: é¢„å¤„ç†: æ•°æ®åˆ†æä»»åŠ¡ -> å­ä»»åŠ¡ç»“æœ'}\n",
      "å­å›¾æœ€ç»ˆçŠ¶æ€ï¼š {'subtask_input': 'é¢„å¤„ç†: æ•°æ®åˆ†æä»»åŠ¡', 'subtask_result': '[æ ¼å¼åŒ–] å¤„ç†å®Œæˆ: é¢„å¤„ç†: æ•°æ®åˆ†æä»»åŠ¡ -> å­ä»»åŠ¡ç»“æœ'}\n",
      "âœ… é€šè¿‡ç‹¬ç«‹çš„çŠ¶æ€ç®¡ç†ï¼Œæˆ‘ä»¬å¯ä»¥åˆ†åˆ«è®¿é—®ä¸»å›¾å’Œå­å›¾çš„å®Œæ•´æ‰§è¡Œå†å²\n"
     ]
    }
   ],
   "source": [
    "# è¿è¡Œå±‚æ¬¡åŒ–æ™ºèƒ½ä½“\n",
    "config = {\"configurable\": {\"thread_id\": \"hierarchical_thread\"}}\n",
    "\n",
    "print(\"=== è¿è¡Œå±‚æ¬¡åŒ–æ™ºèƒ½ä½“ ===\")\n",
    "result = hierarchical_graph.invoke(\n",
    "    {\"main_topic\": \"æ•°æ®åˆ†æä»»åŠ¡\"}, \n",
    "    config=config\n",
    ")\n",
    "print(\"æ‰§è¡Œç»“æœï¼š\", result)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"=== è·å–å­å›¾çŠ¶æ€å¿«ç…§ ===\")\n",
    "\n",
    "# è·å–ä¸»å›¾çŠ¶æ€\n",
    "main_state = hierarchical_graph.get_state(config, subgraphs=True)\n",
    "print(\"Grandparent State (ä¸»å›¾çŠ¶æ€):\")\n",
    "print(main_state.values)  # æ‰“å°ä¸»å›¾çš„çŠ¶æ€å€¼\n",
    "print(\"---------------\")\n",
    "\n",
    "# è·å–å­å›¾çŠ¶æ€ï¼ˆå­å›¾æœ‰ç‹¬ç«‹çš„thread_idï¼‰\n",
    "subgraph_config = {\"configurable\": {\"thread_id\": \"sub_æ•°æ®åˆ†æä»»åŠ¡\"}}\n",
    "subgraph_state = compiled_subgraph.get_state(subgraph_config, subgraphs=True)\n",
    "print(\"Parent Graph State (å­å›¾çŠ¶æ€):\")\n",
    "print(subgraph_state.values)  # æ‰“å°å­å›¾çš„çŠ¶æ€å€¼\n",
    "print(\"---------------\")\n",
    "\n",
    "# æ£€æŸ¥å­å›¾çš„å†å²çŠ¶æ€\n",
    "print(\"Subgraph State History (å­å›¾å†å²çŠ¶æ€):\")\n",
    "sub_history = list(compiled_subgraph.get_state_history(subgraph_config))\n",
    "for i, snapshot in enumerate(sub_history[:3]):\n",
    "    print(f\"  å­å›¾å¿«ç…§ {i+1}: {snapshot.values}\")\n",
    "\n",
    "print(\"\\n=== å½“å‰ç¤ºä¾‹çš„çŠ¶æ€è®¿é—® ===\")\n",
    "print(\"ä¸»å›¾æœ€ç»ˆçŠ¶æ€ï¼š\", main_state.values)\n",
    "print(\"å­å›¾æœ€ç»ˆçŠ¶æ€ï¼š\", subgraph_state.values)\n",
    "print(\"âœ… é€šè¿‡ç‹¬ç«‹çš„çŠ¶æ€ç®¡ç†ï¼Œæˆ‘ä»¬å¯ä»¥åˆ†åˆ«è®¿é—®ä¸»å›¾å’Œå­å›¾çš„å®Œæ•´æ‰§è¡Œå†å²\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pq60sga94ze",
   "metadata": {},
   "source": [
    "##### ç¤ºä¾‹ 4-19ï¼šæ›´æ–°å­å›¾çŠ¶æ€\n",
    "\n",
    "è®©æˆ‘ä»¬æ¼”ç¤ºå¦‚ä½•æ›´æ–°ä¸»å›¾å’Œå­å›¾çš„çŠ¶æ€ï¼Œå¹¶è§‚å¯ŸçŠ¶æ€å˜åŒ–ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "jxdht2prsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# é¦–å…ˆï¼Œè®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªåŒ…å«æ›´å¤šçŠ¶æ€å­—æ®µçš„å¢å¼ºç‰ˆå­å›¾\n",
    "from typing import TypedDict\n",
    "\n",
    "class EnhancedSubtaskState(TypedDict):\n",
    "    subtask_input: str\n",
    "    subtask_result: str\n",
    "    city: str  # æ–°å¢åŸå¸‚å­—æ®µ\n",
    "    temperature: int  # æ–°å¢æ¸©åº¦å­—æ®µ\n",
    "\n",
    "class EnhancedMainState(TypedDict):\n",
    "    main_topic: str\n",
    "    processed_data: str\n",
    "    final_output: str\n",
    "    user_location: str  # æ–°å¢ç”¨æˆ·ä½ç½®å­—æ®µ\n",
    "\n",
    "# å¢å¼ºçš„å­å›¾èŠ‚ç‚¹å‡½æ•°\n",
    "def enhanced_subtask_processor(state: EnhancedSubtaskState):\n",
    "    \"\"\"å¢å¼ºçš„å­å›¾å¤„ç†èŠ‚ç‚¹\"\"\"\n",
    "    city = state.get(\"city\", \"æœªçŸ¥åŸå¸‚\")\n",
    "    temp = state.get(\"temperature\", 20)\n",
    "    return {\n",
    "        \"subtask_result\": f\"å¤„ç†å®Œæˆ: {state['subtask_input']} (ä½ç½®: {city}, æ¸©åº¦: {temp}Â°C) -> å­ä»»åŠ¡ç»“æœ\"\n",
    "    }\n",
    "\n",
    "def enhanced_subtask_formatter(state: EnhancedSubtaskState):\n",
    "    \"\"\"å¢å¼ºçš„å­å›¾æ ¼å¼åŒ–èŠ‚ç‚¹\"\"\"\n",
    "    return {\n",
    "        \"subtask_result\": f\"[æ ¼å¼åŒ–] {state['subtask_result']}\"\n",
    "    }\n",
    "\n",
    "# æ„å»ºå¢å¼ºç‰ˆå­å›¾\n",
    "enhanced_subgraph = StateGraph(EnhancedSubtaskState)\n",
    "enhanced_subgraph.add_node(\"process\", enhanced_subtask_processor)\n",
    "enhanced_subgraph.add_node(\"format\", enhanced_subtask_formatter)\n",
    "enhanced_subgraph.add_edge(START, \"process\")\n",
    "enhanced_subgraph.add_edge(\"process\", \"format\")\n",
    "enhanced_subgraph.add_edge(\"format\", END)\n",
    "compiled_enhanced_subgraph = enhanced_subgraph.compile(checkpointer=MemorySaver())\n",
    "\n",
    "# å¢å¼ºç‰ˆä¸»å›¾èŠ‚ç‚¹å‡½æ•°\n",
    "def enhanced_prepare_data(state: EnhancedMainState):\n",
    "    \"\"\"å¢å¼ºçš„æ•°æ®å‡†å¤‡\"\"\"\n",
    "    return {\n",
    "        \"processed_data\": f\"é¢„å¤„ç†: {state['main_topic']}\",\n",
    "        \"user_location\": state.get(\"user_location\", \"é»˜è®¤ä½ç½®\")\n",
    "    }\n",
    "\n",
    "def enhanced_call_subgraph_node(state: EnhancedMainState):\n",
    "    \"\"\"è°ƒç”¨å¢å¼ºç‰ˆå­å›¾çš„èŠ‚ç‚¹\"\"\"\n",
    "    subgraph_config = {\"configurable\": {\"thread_id\": f\"enhanced_sub_{state['main_topic']}\"}}\n",
    "    \n",
    "    # è°ƒç”¨å­å›¾å¤„ç†ï¼Œä¼ é€’æ›´å¤šå‚æ•°\n",
    "    subgraph_input = {\n",
    "        \"subtask_input\": state[\"processed_data\"],\n",
    "        \"city\": \"beijing\",  # é»˜è®¤åŸå¸‚\n",
    "        \"temperature\": 25   # é»˜è®¤æ¸©åº¦\n",
    "    }\n",
    "    subgraph_result = compiled_enhanced_subgraph.invoke(subgraph_input, subgraph_config)\n",
    "    \n",
    "    return {\n",
    "        \"processed_data\": subgraph_result[\"subtask_result\"]\n",
    "    }\n",
    "\n",
    "def enhanced_finalize_output(state: EnhancedMainState):\n",
    "    \"\"\"ç”Ÿæˆå¢å¼ºçš„æœ€ç»ˆè¾“å‡º\"\"\"\n",
    "    return {\n",
    "        \"final_output\": f\"æœ€ç»ˆç»“æœ: {state['processed_data']} (ç”¨æˆ·ä½ç½®: {state.get('user_location', 'æœªçŸ¥')})\"\n",
    "    }\n",
    "\n",
    "# æ„å»ºå¢å¼ºç‰ˆä¸»å›¾\n",
    "enhanced_main_graph = StateGraph(EnhancedMainState)\n",
    "enhanced_main_graph.add_node(\"prepare\", enhanced_prepare_data)\n",
    "enhanced_main_graph.add_node(\"subgraph_call\", enhanced_call_subgraph_node)\n",
    "enhanced_main_graph.add_node(\"finalize\", enhanced_finalize_output)\n",
    "\n",
    "enhanced_main_graph.add_edge(START, \"prepare\")\n",
    "enhanced_main_graph.add_edge(\"prepare\", \"subgraph_call\")\n",
    "enhanced_main_graph.add_edge(\"subgraph_call\", \"finalize\")\n",
    "enhanced_main_graph.add_edge(\"finalize\", END)\n",
    "\n",
    "# ç¼–è¯‘å¢å¼ºç‰ˆä¸»å›¾\n",
    "enhanced_hierarchical_graph = enhanced_main_graph.compile(checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "suptevtvcwr",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== æ­¥éª¤ 1: è¿è¡Œå¢å¼ºç‰ˆå±‚æ¬¡åŒ–æ™ºèƒ½ä½“ ===\n",
      "åˆå§‹æ‰§è¡Œç»“æœï¼š {'main_topic': 'å¤©æ°”åˆ†æä»»åŠ¡', 'processed_data': '[æ ¼å¼åŒ–] å¤„ç†å®Œæˆ: é¢„å¤„ç†: å¤©æ°”åˆ†æä»»åŠ¡ (ä½ç½®: beijing, æ¸©åº¦: 25Â°C) -> å­ä»»åŠ¡ç»“æœ', 'final_output': 'æœ€ç»ˆç»“æœ: [æ ¼å¼åŒ–] å¤„ç†å®Œæˆ: é¢„å¤„ç†: å¤©æ°”åˆ†æä»»åŠ¡ (ä½ç½®: beijing, æ¸©åº¦: 25Â°C) -> å­ä»»åŠ¡ç»“æœ (ç”¨æˆ·ä½ç½®: shanghai)', 'user_location': 'shanghai'}\n",
      "\n",
      "============================================================\n",
      "=== æ­¥éª¤ 2: è·å–å½“å‰çŠ¶æ€ ===\n",
      "ä¸»å›¾å½“å‰çŠ¶æ€ï¼š {'main_topic': 'å¤©æ°”åˆ†æä»»åŠ¡', 'processed_data': '[æ ¼å¼åŒ–] å¤„ç†å®Œæˆ: é¢„å¤„ç†: å¤©æ°”åˆ†æä»»åŠ¡ (ä½ç½®: beijing, æ¸©åº¦: 25Â°C) -> å­ä»»åŠ¡ç»“æœ', 'final_output': 'æœ€ç»ˆç»“æœ: [æ ¼å¼åŒ–] å¤„ç†å®Œæˆ: é¢„å¤„ç†: å¤©æ°”åˆ†æä»»åŠ¡ (ä½ç½®: beijing, æ¸©åº¦: 25Â°C) -> å­ä»»åŠ¡ç»“æœ (ç”¨æˆ·ä½ç½®: shanghai)', 'user_location': 'shanghai'}\n",
      "å­å›¾å½“å‰çŠ¶æ€ï¼š {'subtask_input': 'é¢„å¤„ç†: å¤©æ°”åˆ†æä»»åŠ¡', 'subtask_result': '[æ ¼å¼åŒ–] å¤„ç†å®Œæˆ: é¢„å¤„ç†: å¤©æ°”åˆ†æä»»åŠ¡ (ä½ç½®: beijing, æ¸©åº¦: 25Â°C) -> å­ä»»åŠ¡ç»“æœ', 'city': 'beijing', 'temperature': 25}\n",
      "\n",
      "============================================================\n",
      "=== æ­¥éª¤ 3: æ›´æ–°ä¸»å›¾çŠ¶æ€ ===\n",
      "æ›´æ–°ä¸»å›¾çŠ¶æ€: user_location ä» 'shanghai' æ”¹ä¸º 'guangzhou'\n",
      "ä¸»å›¾çŠ¶æ€æ›´æ–°å®Œæˆï¼Œæ–°é…ç½®ï¼š {'configurable': {'thread_id': 'enhanced_hierarchical_thread', 'checkpoint_ns': '', 'checkpoint_id': '1f07dccb-1910-6134-8004-0cb0146968a7'}}\n",
      "æ›´æ–°åçš„ä¸»å›¾çŠ¶æ€ï¼š {'main_topic': 'å¤©æ°”åˆ†æä»»åŠ¡', 'processed_data': '[æ ¼å¼åŒ–] å¤„ç†å®Œæˆ: é¢„å¤„ç†: å¤©æ°”åˆ†æä»»åŠ¡ (ä½ç½®: beijing, æ¸©åº¦: 25Â°C) -> å­ä»»åŠ¡ç»“æœ', 'final_output': 'æœ€ç»ˆç»“æœ: [æ ¼å¼åŒ–] å¤„ç†å®Œæˆ: é¢„å¤„ç†: å¤©æ°”åˆ†æä»»åŠ¡ (ä½ç½®: beijing, æ¸©åº¦: 25Â°C) -> å­ä»»åŠ¡ç»“æœ (ç”¨æˆ·ä½ç½®: shanghai)', 'user_location': 'guangzhou'}\n",
      "\n",
      "============================================================\n",
      "=== æ­¥éª¤ 4: æ›´æ–°å­å›¾çŠ¶æ€ ===\n",
      "æ›´æ–°å­å›¾çŠ¶æ€: city ä» 'beijing' æ”¹ä¸º 'la', temperature ä» 25 æ”¹ä¸º 18\n",
      "å­å›¾çŠ¶æ€æ›´æ–°å®Œæˆï¼Œæ–°é…ç½®ï¼š {'configurable': {'thread_id': 'enhanced_sub_å¤©æ°”åˆ†æä»»åŠ¡', 'checkpoint_ns': '', 'checkpoint_id': '1f07dccb-1910-690e-8003-72bbf49d8012'}}\n",
      "æ›´æ–°åçš„å­å›¾çŠ¶æ€ï¼š {'subtask_input': 'é¢„å¤„ç†: å¤©æ°”åˆ†æä»»åŠ¡', 'subtask_result': '[æ ¼å¼åŒ–] å¤„ç†å®Œæˆ: é¢„å¤„ç†: å¤©æ°”åˆ†æä»»åŠ¡ (ä½ç½®: beijing, æ¸©åº¦: 25Â°C) -> å­ä»»åŠ¡ç»“æœ', 'city': 'la', 'temperature': 18}\n",
      "\n",
      "============================================================\n",
      "=== æ­¥éª¤ 5: ä»æ›´æ–°çš„çŠ¶æ€ç»§ç»­æ‰§è¡Œ ===\n",
      "ä»æ›´æ–°çš„å­å›¾çŠ¶æ€ç»§ç»­æ‰§è¡Œ:\n",
      "å­å›¾æ‰§è¡Œç»“æœï¼š {'subtask_input': 'é¢„å¤„ç†: å¤©æ°”åˆ†æä»»åŠ¡', 'subtask_result': '[æ ¼å¼åŒ–] å¤„ç†å®Œæˆ: é¢„å¤„ç†: å¤©æ°”åˆ†æä»»åŠ¡ (ä½ç½®: beijing, æ¸©åº¦: 25Â°C) -> å­ä»»åŠ¡ç»“æœ', 'city': 'la', 'temperature': 18}\n"
     ]
    }
   ],
   "source": [
    "# è¿è¡Œå¢å¼ºç‰ˆæ™ºèƒ½ä½“å¹¶æ¼”ç¤ºçŠ¶æ€æ›´æ–°\n",
    "print(\"=== æ­¥éª¤ 1: è¿è¡Œå¢å¼ºç‰ˆå±‚æ¬¡åŒ–æ™ºèƒ½ä½“ ===\")\n",
    "enhanced_config = {\"configurable\": {\"thread_id\": \"enhanced_hierarchical_thread\"}}\n",
    "\n",
    "# åˆå§‹è¿è¡Œ\n",
    "initial_result = enhanced_hierarchical_graph.invoke(\n",
    "    {\"main_topic\": \"å¤©æ°”åˆ†æä»»åŠ¡\", \"user_location\": \"shanghai\"}, \n",
    "    enhanced_config\n",
    ")\n",
    "print(\"åˆå§‹æ‰§è¡Œç»“æœï¼š\", initial_result)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== æ­¥éª¤ 2: è·å–å½“å‰çŠ¶æ€ ===\")\n",
    "\n",
    "# è·å–ä¸»å›¾çŠ¶æ€\n",
    "main_state = enhanced_hierarchical_graph.get_state(enhanced_config)\n",
    "print(\"ä¸»å›¾å½“å‰çŠ¶æ€ï¼š\", main_state.values)\n",
    "\n",
    "# è·å–å­å›¾çŠ¶æ€\n",
    "subgraph_config = {\"configurable\": {\"thread_id\": \"enhanced_sub_å¤©æ°”åˆ†æä»»åŠ¡\"}}\n",
    "subgraph_state = compiled_enhanced_subgraph.get_state(subgraph_config)\n",
    "print(\"å­å›¾å½“å‰çŠ¶æ€ï¼š\", subgraph_state.values)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== æ­¥éª¤ 3: æ›´æ–°ä¸»å›¾çŠ¶æ€ ===\")\n",
    "\n",
    "# æ›´æ–°ä¸»å›¾çŠ¶æ€ä¸­çš„ user_location å­—æ®µ\n",
    "print(\"æ›´æ–°ä¸»å›¾çŠ¶æ€: user_location ä» 'shanghai' æ”¹ä¸º 'guangzhou'\")\n",
    "updated_main_config = enhanced_hierarchical_graph.update_state(\n",
    "    enhanced_config,\n",
    "    {\"user_location\": \"guangzhou\"}\n",
    ")\n",
    "print(\"ä¸»å›¾çŠ¶æ€æ›´æ–°å®Œæˆï¼Œæ–°é…ç½®ï¼š\", updated_main_config)\n",
    "\n",
    "# æŸ¥çœ‹æ›´æ–°åçš„ä¸»å›¾çŠ¶æ€\n",
    "updated_main_state = enhanced_hierarchical_graph.get_state(updated_main_config)\n",
    "print(\"æ›´æ–°åçš„ä¸»å›¾çŠ¶æ€ï¼š\", updated_main_state.values)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== æ­¥éª¤ 4: æ›´æ–°å­å›¾çŠ¶æ€ ===\")\n",
    "\n",
    "# æ›´æ–°å­å›¾çŠ¶æ€ä¸­çš„ city å­—æ®µï¼ˆæŒ‰ç…§ä½ çš„è¦æ±‚ï¼‰\n",
    "print(\"æ›´æ–°å­å›¾çŠ¶æ€: city ä» 'beijing' æ”¹ä¸º 'la', temperature ä» 25 æ”¹ä¸º 18\")\n",
    "updated_subgraph_config = compiled_enhanced_subgraph.update_state(\n",
    "    subgraph_config,  # å°†å­å›¾çŠ¶æ€çš„ config ä½œä¸ºç¬¬ä¸€ä¸ªå‚æ•°ä¼ å…¥\n",
    "    {\"city\": \"la\", \"temperature\": 18}  # updates å‚æ•°æŒ‡å®šè¦æ›´æ–°çš„çŠ¶æ€é”®å€¼å¯¹\n",
    ")\n",
    "print(\"å­å›¾çŠ¶æ€æ›´æ–°å®Œæˆï¼Œæ–°é…ç½®ï¼š\", updated_subgraph_config)\n",
    "\n",
    "# æŸ¥çœ‹æ›´æ–°åçš„å­å›¾çŠ¶æ€\n",
    "updated_subgraph_state = compiled_enhanced_subgraph.get_state(updated_subgraph_config)\n",
    "print(\"æ›´æ–°åçš„å­å›¾çŠ¶æ€ï¼š\", updated_subgraph_state.values)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== æ­¥éª¤ 5: ä»æ›´æ–°çš„çŠ¶æ€ç»§ç»­æ‰§è¡Œ ===\")\n",
    "\n",
    "# ä»æ›´æ–°çš„å­å›¾çŠ¶æ€ç»§ç»­æ‰§è¡Œï¼ˆé‡æ–°å¤„ç†ï¼‰\n",
    "print(\"ä»æ›´æ–°çš„å­å›¾çŠ¶æ€ç»§ç»­æ‰§è¡Œ:\")\n",
    "for chunk in compiled_enhanced_subgraph.stream(None, updated_subgraph_config, stream_mode=\"values\"):\n",
    "    print(\"å­å›¾æ‰§è¡Œç»“æœï¼š\", chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "k98hg03un",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== æ­¥éª¤ 6: ä»¥å­å›¾èŠ‚ç‚¹èº«ä»½æ›´æ–°çŠ¶æ€ ===\n",
      "è¿è¡Œæ–°çš„å®ä¾‹ä»¥è·å–å­å›¾æ‰§è¡ŒçŠ¶æ€...\n",
      "åˆå§‹æµ‹è¯•ç»“æœï¼š {'main_topic': 'æµ‹è¯•ä»»åŠ¡', 'processed_data': '[æ ¼å¼åŒ–] å¤„ç†å®Œæˆ: é¢„å¤„ç†: æµ‹è¯•ä»»åŠ¡ (ä½ç½®: beijing, æ¸©åº¦: 25Â°C) -> å­ä»»åŠ¡ç»“æœ', 'final_output': 'æœ€ç»ˆç»“æœ: [æ ¼å¼åŒ–] å¤„ç†å®Œæˆ: é¢„å¤„ç†: æµ‹è¯•ä»»åŠ¡ (ä½ç½®: beijing, æ¸©åº¦: 25Â°C) -> å­ä»»åŠ¡ç»“æœ (ç”¨æˆ·ä½ç½®: beijing)', 'user_location': 'beijing'}\n",
      "\n",
      "è·å–å­å›¾çŠ¶æ€ä»¥è¿›è¡ŒèŠ‚ç‚¹èº«ä»½æ›´æ–°...\n",
      "å½“å‰å­å›¾çŠ¶æ€ï¼š {'subtask_input': 'é¢„å¤„ç†: æµ‹è¯•ä»»åŠ¡', 'subtask_result': '[æ ¼å¼åŒ–] å¤„ç†å®Œæˆ: é¢„å¤„ç†: æµ‹è¯•ä»»åŠ¡ (ä½ç½®: beijing, æ¸©åº¦: 25Â°C) -> å­ä»»åŠ¡ç»“æœ', 'city': 'beijing', 'temperature': 25}\n",
      "\n",
      "=== ä»¥ 'process' èŠ‚ç‚¹èº«ä»½æ›´æ–°å­å›¾çŠ¶æ€ ===\n",
      "ä»¥ 'process' èŠ‚ç‚¹èº«ä»½æ›´æ–°çŠ¶æ€å®Œæˆï¼\n",
      "æ›´æ–°åçš„é…ç½®ï¼š {'configurable': {'thread_id': 'enhanced_sub_æµ‹è¯•ä»»åŠ¡', 'checkpoint_ns': '', 'checkpoint_id': '1f07dccb-1924-63fa-8003-718cc2c95b4d'}}\n",
      "ä»¥èŠ‚ç‚¹èº«ä»½æ›´æ–°åçš„å­å›¾çŠ¶æ€ï¼š {'subtask_input': 'é¢„å¤„ç†: æµ‹è¯•ä»»åŠ¡', 'subtask_result': 'æ¨¡æ‹ŸprocessèŠ‚ç‚¹æ‰§è¡Œç»“æœ', 'city': 'la', 'temperature': 18}\n",
      "ä¸‹ä¸€ä¸ªè¦æ‰§è¡Œçš„èŠ‚ç‚¹ï¼š ('format',)\n",
      "\n",
      "=== ä»èŠ‚ç‚¹èº«ä»½æ›´æ–°çš„çŠ¶æ€ç»§ç»­æ‰§è¡Œ ===\n",
      "ä» 'process' èŠ‚ç‚¹æ›´æ–°çš„çŠ¶æ€ç»§ç»­æ‰§è¡Œåç»­èŠ‚ç‚¹...\n",
      "ç»§ç»­æ‰§è¡Œç»“æœï¼š {'subtask_input': 'é¢„å¤„ç†: æµ‹è¯•ä»»åŠ¡', 'subtask_result': 'æ¨¡æ‹ŸprocessèŠ‚ç‚¹æ‰§è¡Œç»“æœ', 'city': 'la', 'temperature': 18}\n",
      "ç»§ç»­æ‰§è¡Œç»“æœï¼š {'subtask_input': 'é¢„å¤„ç†: æµ‹è¯•ä»»åŠ¡', 'subtask_result': '[æ ¼å¼åŒ–] æ¨¡æ‹ŸprocessèŠ‚ç‚¹æ‰§è¡Œç»“æœ', 'city': 'la', 'temperature': 18}\n",
      "\n",
      "ğŸ’¡ ä»¥èŠ‚ç‚¹èº«ä»½æ›´æ–°çŠ¶æ€çš„å…³é”®æ¦‚å¿µï¼š\n",
      "1. as_node='process' å‚æ•°æ¨¡æ‹Ÿäº† process èŠ‚ç‚¹æ‰§è¡Œåçš„çŠ¶æ€æ›´æ–°æ•ˆæœ\n",
      "2. è¿™ç§æ–¹å¼å®ç°äº†æ›´ç²¾ç»†çš„å­å›¾çŠ¶æ€æ§åˆ¶ï¼Œå…è®¸ç²¾ç¡®æ¨¡æ‹Ÿå­å›¾å†…éƒ¨èŠ‚ç‚¹è¡Œä¸º\n",
      "3. æ›´æ–°åå¯ä»¥ç»§ç»­æ‰§è¡Œåç»­èŠ‚ç‚¹ï¼Œæ”¯æŒé«˜çº§æµç¨‹å¹²é¢„å’Œå®šåˆ¶éœ€æ±‚\n",
      "4. ç‰¹åˆ«é€‚ç”¨äºæµ‹è¯•ã€è°ƒè¯•å’Œæ¨¡æ‹Ÿç‰¹å®šèŠ‚ç‚¹æ‰§è¡Œç»“æœçš„åœºæ™¯\n"
     ]
    }
   ],
   "source": [
    "# æ¼”ç¤ºä»¥å­å›¾èŠ‚ç‚¹èº«ä»½æ›´æ–°çŠ¶æ€\n",
    "print(\"=== æ­¥éª¤ 6: ä»¥å­å›¾èŠ‚ç‚¹èº«ä»½æ›´æ–°çŠ¶æ€ ===\")\n",
    "\n",
    "# é¦–å…ˆè¿è¡Œä¸€ä¸ªæ–°çš„å®ä¾‹æ¥è·å–æ´»è·ƒçš„å­å›¾çŠ¶æ€\n",
    "print(\"è¿è¡Œæ–°çš„å®ä¾‹ä»¥è·å–å­å›¾æ‰§è¡ŒçŠ¶æ€...\")\n",
    "test_config = {\"configurable\": {\"thread_id\": \"test_node_update\"}}\n",
    "test_subgraph_config = {\"configurable\": {\"thread_id\": \"enhanced_sub_æµ‹è¯•ä»»åŠ¡\"}}\n",
    "\n",
    "# å…ˆè¿è¡Œä¸»å›¾\n",
    "test_result = enhanced_hierarchical_graph.invoke(\n",
    "    {\"main_topic\": \"æµ‹è¯•ä»»åŠ¡\", \"user_location\": \"beijing\"}, \n",
    "    test_config\n",
    ")\n",
    "\n",
    "print(\"åˆå§‹æµ‹è¯•ç»“æœï¼š\", test_result)\n",
    "\n",
    "print(\"\\nè·å–å­å›¾çŠ¶æ€ä»¥è¿›è¡ŒèŠ‚ç‚¹èº«ä»½æ›´æ–°...\")\n",
    "current_subgraph_state = compiled_enhanced_subgraph.get_state(test_subgraph_config)\n",
    "print(\"å½“å‰å­å›¾çŠ¶æ€ï¼š\", current_subgraph_state.values)\n",
    "\n",
    "print(\"\\n=== ä»¥ 'process' èŠ‚ç‚¹èº«ä»½æ›´æ–°å­å›¾çŠ¶æ€ ===\")\n",
    "# ä»¥å­å›¾ enhanced_subgraph çš„ process èŠ‚ç‚¹èº«ä»½æ›´æ–°çŠ¶æ€\n",
    "updated_config_as_node = compiled_enhanced_subgraph.update_state(\n",
    "    test_subgraph_config,  # å°†å­å›¾çŠ¶æ€çš„ config ä½œä¸ºç¬¬ä¸€ä¸ªå‚æ•°ä¼ å…¥\n",
    "    {\"city\": \"la\", \"temperature\": 18, \"subtask_result\": \"æ¨¡æ‹ŸprocessèŠ‚ç‚¹æ‰§è¡Œç»“æœ\"},  # updates å‚æ•°æŒ‡å®š\"è™šå‡çš„\"å¤„ç†ç»“æœ\n",
    "    as_node=\"process\",  # æŒ‡å®šè¦æ¨¡æ‹Ÿçš„å­å›¾èŠ‚ç‚¹åç§°ä¸º process\n",
    ")\n",
    "\n",
    "print(\"ä»¥ 'process' èŠ‚ç‚¹èº«ä»½æ›´æ–°çŠ¶æ€å®Œæˆï¼\")\n",
    "print(\"æ›´æ–°åçš„é…ç½®ï¼š\", updated_config_as_node)\n",
    "\n",
    "# æŸ¥çœ‹æ›´æ–°åçš„çŠ¶æ€\n",
    "updated_state_as_node = compiled_enhanced_subgraph.get_state(updated_config_as_node)\n",
    "print(\"ä»¥èŠ‚ç‚¹èº«ä»½æ›´æ–°åçš„å­å›¾çŠ¶æ€ï¼š\", updated_state_as_node.values)\n",
    "print(\"ä¸‹ä¸€ä¸ªè¦æ‰§è¡Œçš„èŠ‚ç‚¹ï¼š\", updated_state_as_node.next)\n",
    "\n",
    "print(\"\\n=== ä»èŠ‚ç‚¹èº«ä»½æ›´æ–°çš„çŠ¶æ€ç»§ç»­æ‰§è¡Œ ===\")\n",
    "print(\"ä» 'process' èŠ‚ç‚¹æ›´æ–°çš„çŠ¶æ€ç»§ç»­æ‰§è¡Œåç»­èŠ‚ç‚¹...\")\n",
    "\n",
    "# ä»æ›´æ–°çš„çŠ¶æ€ç»§ç»­æ‰§è¡Œï¼ˆä¼šæ‰§è¡ŒformatèŠ‚ç‚¹ï¼‰\n",
    "for chunk in compiled_enhanced_subgraph.stream(None, updated_config_as_node, stream_mode=\"values\"):\n",
    "    print(\"ç»§ç»­æ‰§è¡Œç»“æœï¼š\", chunk)\n",
    "\n",
    "print(\"\\nğŸ’¡ ä»¥èŠ‚ç‚¹èº«ä»½æ›´æ–°çŠ¶æ€çš„å…³é”®æ¦‚å¿µï¼š\")\n",
    "print(\"1. as_node='process' å‚æ•°æ¨¡æ‹Ÿäº† process èŠ‚ç‚¹æ‰§è¡Œåçš„çŠ¶æ€æ›´æ–°æ•ˆæœ\")\n",
    "print(\"2. è¿™ç§æ–¹å¼å®ç°äº†æ›´ç²¾ç»†çš„å­å›¾çŠ¶æ€æ§åˆ¶ï¼Œå…è®¸ç²¾ç¡®æ¨¡æ‹Ÿå­å›¾å†…éƒ¨èŠ‚ç‚¹è¡Œä¸º\")\n",
    "print(\"3. æ›´æ–°åå¯ä»¥ç»§ç»­æ‰§è¡Œåç»­èŠ‚ç‚¹ï¼Œæ”¯æŒé«˜çº§æµç¨‹å¹²é¢„å’Œå®šåˆ¶éœ€æ±‚\")\n",
    "print(\"4. ç‰¹åˆ«é€‚ç”¨äºæµ‹è¯•ã€è°ƒè¯•å’Œæ¨¡æ‹Ÿç‰¹å®šèŠ‚ç‚¹æ‰§è¡Œç»“æœçš„åœºæ™¯\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9t03x2evrid",
   "metadata": {},
   "source": [
    "##### ç¤ºä¾‹ 4-20ï¼šä»¥å­å›¾èŠ‚ç‚¹èº«ä»½æ›´æ–°çŠ¶æ€\n",
    "\n",
    "æ¼”ç¤ºå¦‚ä½•ä»¥ç‰¹å®šå­å›¾èŠ‚ç‚¹çš„èº«ä»½æ›´æ–°çŠ¶æ€ï¼Œå®ç°æ›´ç²¾ç»†çš„çŠ¶æ€æ§åˆ¶ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "vqe8o0ll0y",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºä¸€ä¸ªæ›´çœŸå®çš„å¤©æ°”æŸ¥è¯¢å­å›¾ç¤ºä¾‹\n",
    "from typing import TypedDict, List\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "\n",
    "class WeatherState(TypedDict):\n",
    "    location: str\n",
    "    messages: List[BaseMessage]\n",
    "    weather_result: str\n",
    "\n",
    "class ParentTaskState(TypedDict):\n",
    "    task_name: str\n",
    "    location: str\n",
    "    weather_info: str\n",
    "    final_report: str\n",
    "\n",
    "# å¤©æ°”å­å›¾èŠ‚ç‚¹å‡½æ•°\n",
    "def weather_analyzer(state: WeatherState):\n",
    "    \"\"\"åˆ†æå¤©æ°”è¯·æ±‚\"\"\"\n",
    "    location = state.get(\"location\", \"unknown\")\n",
    "    return {\n",
    "        \"messages\": state[\"messages\"] + [HumanMessage(content=f\"è¯·æŸ¥è¯¢{location}çš„å¤©æ°”\")],\n",
    "        \"location\": location\n",
    "    }\n",
    "\n",
    "def weather_node(state: WeatherState):\n",
    "    \"\"\"å¤©æ°”æŸ¥è¯¢èŠ‚ç‚¹ - è¿™æ˜¯æˆ‘ä»¬è¦æ¨¡æ‹Ÿçš„å…³é”®èŠ‚ç‚¹\"\"\"\n",
    "    location = state.get(\"location\", \"unknown\")\n",
    "    # æ¨¡æ‹Ÿå¤©æ°”APIè°ƒç”¨\n",
    "    weather_result = f\"{location}çš„å¤©æ°”æ˜¯æ™´æœ—çš„\"\n",
    "    return {\n",
    "        \"messages\": state[\"messages\"] + [AIMessage(content=weather_result)],\n",
    "        \"weather_result\": weather_result\n",
    "    }\n",
    "\n",
    "def weather_formatter(state: WeatherState):\n",
    "    \"\"\"æ ¼å¼åŒ–å¤©æ°”ç»“æœ\"\"\"\n",
    "    weather_result = state.get(\"weather_result\", \"æ— å¤©æ°”æ•°æ®\")\n",
    "    return {\n",
    "        \"weather_result\": f\"[æ ¼å¼åŒ–] {weather_result}\"\n",
    "    }\n",
    "\n",
    "# æ„å»ºå¤©æ°”å­å›¾ (weather_graph)\n",
    "weather_graph = StateGraph(WeatherState)\n",
    "weather_graph.add_node(\"weather_analyzer\", weather_analyzer)\n",
    "weather_graph.add_node(\"weather_node\", weather_node)  # å…³é”®çš„weather_node\n",
    "weather_graph.add_node(\"weather_formatter\", weather_formatter)\n",
    "\n",
    "weather_graph.add_edge(START, \"weather_analyzer\")\n",
    "weather_graph.add_edge(\"weather_analyzer\", \"weather_node\")\n",
    "weather_graph.add_edge(\"weather_node\", \"weather_formatter\")\n",
    "weather_graph.add_edge(\"weather_formatter\", END)\n",
    "\n",
    "compiled_weather_graph = weather_graph.compile(checkpointer=MemorySaver())\n",
    "\n",
    "# çˆ¶å›¾èŠ‚ç‚¹å‡½æ•°\n",
    "def prepare_weather_task(state: ParentTaskState):\n",
    "    \"\"\"å‡†å¤‡å¤©æ°”æŸ¥è¯¢ä»»åŠ¡\"\"\"\n",
    "    return {\n",
    "        \"task_name\": f\"å¤©æ°”æŸ¥è¯¢ä»»åŠ¡: {state.get('task_name', 'é»˜è®¤ä»»åŠ¡')}\",\n",
    "        \"location\": state.get(\"location\", \"beijing\")\n",
    "    }\n",
    "\n",
    "def call_weather_subgraph(state: ParentTaskState):\n",
    "    \"\"\"è°ƒç”¨å¤©æ°”å­å›¾\"\"\"\n",
    "    subgraph_config = {\"configurable\": {\"thread_id\": f\"weather_{state['location']}\"}}\n",
    "    \n",
    "    # è°ƒç”¨å¤©æ°”å­å›¾\n",
    "    subgraph_input = {\n",
    "        \"location\": state[\"location\"],\n",
    "        \"messages\": [HumanMessage(content=\"å¼€å§‹å¤©æ°”æŸ¥è¯¢\")],\n",
    "        \"weather_result\": \"\"\n",
    "    }\n",
    "    subgraph_result = compiled_weather_graph.invoke(subgraph_input, subgraph_config)\n",
    "    \n",
    "    return {\n",
    "        \"weather_info\": subgraph_result[\"weather_result\"]\n",
    "    }\n",
    "\n",
    "def generate_final_report(state: ParentTaskState):\n",
    "    \"\"\"ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š\"\"\"\n",
    "    return {\n",
    "        \"final_report\": f\"ä»»åŠ¡å®Œæˆ: {state['task_name']}, å¤©æ°”ä¿¡æ¯: {state.get('weather_info', 'æ— ')}\"\n",
    "    }\n",
    "\n",
    "# æ„å»ºçˆ¶å›¾\n",
    "parent_weather_graph = StateGraph(ParentTaskState)\n",
    "parent_weather_graph.add_node(\"prepare_task\", prepare_weather_task)\n",
    "parent_weather_graph.add_node(\"call_weather\", call_weather_subgraph)\n",
    "parent_weather_graph.add_node(\"generate_report\", generate_final_report)\n",
    "\n",
    "parent_weather_graph.add_edge(START, \"prepare_task\")\n",
    "parent_weather_graph.add_edge(\"prepare_task\", \"call_weather\")\n",
    "parent_weather_graph.add_edge(\"call_weather\", \"generate_report\")\n",
    "parent_weather_graph.add_edge(\"generate_report\", END)\n",
    "\n",
    "compiled_parent_weather_graph = parent_weather_graph.compile(checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a71c4a2",
   "metadata": {},
   "source": [
    "##### ç¤ºä¾‹ 4-20ï¼šä»¥å­å›¾èŠ‚ç‚¹èº«ä»½æ›´æ–°çŠ¶æ€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "726e29a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¿è¡Œæ–°çš„å®ä¾‹ä»¥è·å–å­å›¾æ‰§è¡ŒçŠ¶æ€...\n",
      "åˆå§‹æµ‹è¯•ç»“æœï¼š {'main_topic': 'æµ‹è¯•ä»»åŠ¡', 'processed_data': '[æ ¼å¼åŒ–] å¤„ç†å®Œæˆ: é¢„å¤„ç†: æµ‹è¯•ä»»åŠ¡ (ä½ç½®: beijing, æ¸©åº¦: 25Â°C) -> å­ä»»åŠ¡ç»“æœ', 'final_output': 'æœ€ç»ˆç»“æœ: [æ ¼å¼åŒ–] å¤„ç†å®Œæˆ: é¢„å¤„ç†: æµ‹è¯•ä»»åŠ¡ (ä½ç½®: beijing, æ¸©åº¦: 25Â°C) -> å­ä»»åŠ¡ç»“æœ (ç”¨æˆ·ä½ç½®: beijing)', 'user_location': 'beijing'}\n",
      "\n",
      "è·å–å­å›¾çŠ¶æ€ä»¥è¿›è¡ŒèŠ‚ç‚¹èº«ä»½æ›´æ–°...\n",
      "å½“å‰å­å›¾çŠ¶æ€ï¼š {'subtask_input': 'é¢„å¤„ç†: æµ‹è¯•ä»»åŠ¡', 'subtask_result': '[æ ¼å¼åŒ–] å¤„ç†å®Œæˆ: é¢„å¤„ç†: æµ‹è¯•ä»»åŠ¡ (ä½ç½®: beijing, æ¸©åº¦: 25Â°C) -> å­ä»»åŠ¡ç»“æœ', 'city': 'beijing', 'temperature': 25}\n",
      "\n",
      "=== ä»¥ 'process' èŠ‚ç‚¹èº«ä»½æ›´æ–°å­å›¾çŠ¶æ€ ===\n",
      "ä»¥ 'process' èŠ‚ç‚¹èº«ä»½æ›´æ–°çŠ¶æ€å®Œæˆï¼\n",
      "æ›´æ–°åçš„é…ç½®ï¼š {'configurable': {'thread_id': 'enhanced_sub_æµ‹è¯•ä»»åŠ¡', 'checkpoint_ns': '', 'checkpoint_id': '1f07dccb-1953-6fd8-8009-840394da26e1'}}\n",
      "ä»¥èŠ‚ç‚¹èº«ä»½æ›´æ–°åçš„å­å›¾çŠ¶æ€ï¼š {'subtask_input': 'é¢„å¤„ç†: æµ‹è¯•ä»»åŠ¡', 'subtask_result': 'æ¨¡æ‹ŸprocessèŠ‚ç‚¹æ‰§è¡Œç»“æœ', 'city': 'la', 'temperature': 18}\n",
      "ä¸‹ä¸€ä¸ªè¦æ‰§è¡Œçš„èŠ‚ç‚¹ï¼š ('format',)\n",
      "\n",
      "=== ä»èŠ‚ç‚¹èº«ä»½æ›´æ–°çš„çŠ¶æ€ç»§ç»­æ‰§è¡Œ ===\n",
      "ä» 'process' èŠ‚ç‚¹æ›´æ–°çš„çŠ¶æ€ç»§ç»­æ‰§è¡Œåç»­èŠ‚ç‚¹...\n",
      "ç»§ç»­æ‰§è¡Œç»“æœï¼š {'subtask_input': 'é¢„å¤„ç†: æµ‹è¯•ä»»åŠ¡', 'subtask_result': 'æ¨¡æ‹ŸprocessèŠ‚ç‚¹æ‰§è¡Œç»“æœ', 'city': 'la', 'temperature': 18}\n",
      "ç»§ç»­æ‰§è¡Œç»“æœï¼š {'subtask_input': 'é¢„å¤„ç†: æµ‹è¯•ä»»åŠ¡', 'subtask_result': '[æ ¼å¼åŒ–] æ¨¡æ‹ŸprocessèŠ‚ç‚¹æ‰§è¡Œç»“æœ', 'city': 'la', 'temperature': 18}\n"
     ]
    }
   ],
   "source": [
    "# é¦–å…ˆè¿è¡Œä¸€ä¸ªæ–°çš„å®ä¾‹æ¥è·å–æ´»è·ƒçš„å­å›¾çŠ¶æ€\n",
    "print(\"è¿è¡Œæ–°çš„å®ä¾‹ä»¥è·å–å­å›¾æ‰§è¡ŒçŠ¶æ€...\")\n",
    "test_config = {\"configurable\": {\"thread_id\": \"test_node_update\"}}\n",
    "test_subgraph_config = {\"configurable\": {\"thread_id\":\n",
    "\"enhanced_sub_æµ‹è¯•ä»»åŠ¡\"}}\n",
    "\n",
    "# å…ˆè¿è¡Œä¸»å›¾\n",
    "test_result = enhanced_hierarchical_graph.invoke(\n",
    "    {\"main_topic\": \"æµ‹è¯•ä»»åŠ¡\", \"user_location\": \"beijing\"},\n",
    "    test_config\n",
    ")\n",
    "\n",
    "print(\"åˆå§‹æµ‹è¯•ç»“æœï¼š\", test_result)\n",
    "\n",
    "print(\"\\nè·å–å­å›¾çŠ¶æ€ä»¥è¿›è¡ŒèŠ‚ç‚¹èº«ä»½æ›´æ–°...\")\n",
    "current_subgraph_state = compiled_enhanced_subgraph.get_state(test_subgraph_config)\n",
    "print(\"å½“å‰å­å›¾çŠ¶æ€ï¼š\", current_subgraph_state.values)\n",
    "\n",
    "print(\"\\n=== ä»¥ 'process' èŠ‚ç‚¹èº«ä»½æ›´æ–°å­å›¾çŠ¶æ€ ===\")\n",
    "# ä»¥å­å›¾ enhanced_subgraph çš„ process èŠ‚ç‚¹èº«ä»½æ›´æ–°çŠ¶æ€\n",
    "updated_config_as_node = compiled_enhanced_subgraph.update_state(\n",
    "    test_subgraph_config,  # å°†å­å›¾çŠ¶æ€çš„ config ä½œä¸ºç¬¬ä¸€ä¸ªå‚æ•°ä¼ å…¥\n",
    "    {\"city\": \"la\", \"temperature\": 18, \"subtask_result\":\n",
    "\"æ¨¡æ‹ŸprocessèŠ‚ç‚¹æ‰§è¡Œç»“æœ\"},  # updates å‚æ•°æŒ‡å®š\"è™šå‡çš„\"å¤„ç†ç»“æœ\n",
    "    as_node=\"process\",  # æŒ‡å®šè¦æ¨¡æ‹Ÿçš„å­å›¾èŠ‚ç‚¹åç§°ä¸º process\n",
    ")\n",
    "\n",
    "print(\"ä»¥ 'process' èŠ‚ç‚¹èº«ä»½æ›´æ–°çŠ¶æ€å®Œæˆï¼\")\n",
    "print(\"æ›´æ–°åçš„é…ç½®ï¼š\", updated_config_as_node)\n",
    "\n",
    "# æŸ¥çœ‹æ›´æ–°åçš„çŠ¶æ€\n",
    "updated_state_as_node = compiled_enhanced_subgraph.get_state(updated_config_as_node)\n",
    "print(\"ä»¥èŠ‚ç‚¹èº«ä»½æ›´æ–°åçš„å­å›¾çŠ¶æ€ï¼š\", updated_state_as_node.values)\n",
    "print(\"ä¸‹ä¸€ä¸ªè¦æ‰§è¡Œçš„èŠ‚ç‚¹ï¼š\", updated_state_as_node.next)\n",
    "\n",
    "print(\"\\n=== ä»èŠ‚ç‚¹èº«ä»½æ›´æ–°çš„çŠ¶æ€ç»§ç»­æ‰§è¡Œ ===\")\n",
    "print(\"ä» 'process' èŠ‚ç‚¹æ›´æ–°çš„çŠ¶æ€ç»§ç»­æ‰§è¡Œåç»­èŠ‚ç‚¹...\")\n",
    "\n",
    "# ä»æ›´æ–°çš„çŠ¶æ€ç»§ç»­æ‰§è¡Œï¼ˆä¼šæ‰§è¡ŒformatèŠ‚ç‚¹ï¼‰\n",
    "for chunk in compiled_enhanced_subgraph.stream(None,\n",
    "updated_config_as_node, stream_mode=\"values\"):\n",
    "    print(\"ç»§ç»­æ‰§è¡Œç»“æœï¼š\", chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44jlwpqvowm",
   "metadata": {},
   "source": [
    "## 4.3 äººæœºåä½œï¼šæ„å»ºå¯æ§çš„æ™ºèƒ½ä½“ç³»ç»Ÿ\n",
    "\n",
    "è™½ç„¶æµå¼å¤„ç†å’ŒæŒä¹…åŒ–å¤§å¤§å¢å¼ºäº†æ™ºèƒ½ä½“çš„ç”¨æˆ·ä½“éªŒå’Œå¯é æ€§ï¼Œä½†åœ¨è®¸å¤šå®é™…åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬è¿˜éœ€è¦ä¸€ä¸ªå…³é”®ç»„ä»¶ï¼š**äººæœºç¯è·¯åä½œï¼ˆHuman-in-the-loopï¼‰**ã€‚è¿™ç§æœºåˆ¶å…è®¸äººç±»ç”¨æˆ·åœ¨æ™ºèƒ½ä½“æ‰§è¡Œçš„å…³é”®èŠ‚ç‚¹è¿›è¡Œå¹²é¢„ã€æŒ‡å¯¼æˆ–æ‰¹å‡†ï¼Œç¡®ä¿æ™ºèƒ½ä½“çš„è¡Œä¸ºç¬¦åˆäººç±»çš„æœŸæœ›å’Œä»·å€¼è§‚ã€‚\n",
    "\n",
    "äººæœºç¯è·¯åä½œç‰¹åˆ«é‡è¦çš„åœºæ™¯åŒ…æ‹¬ï¼š\n",
    "- **é«˜é£é™©å†³ç­–**ï¼šé‡‘èäº¤æ˜“ã€åŒ»ç–—è¯Šæ–­ã€æ³•å¾‹å»ºè®®ç­‰\n",
    "- **åˆ›æ„åä½œ**ï¼šå†…å®¹åˆ›ä½œã€è®¾è®¡å†³ç­–ã€ç­–ç•¥è§„åˆ’ç­‰\n",
    "- **è´¨é‡æ§åˆ¶**ï¼šç¡®ä¿è¾“å‡ºç¬¦åˆç‰¹å®šæ ‡å‡†å’Œè¦æ±‚\n",
    "- **å­¦ä¹ æ”¹è¿›**ï¼šé€šè¿‡äººç±»åé¦ˆæ”¹å–„æ™ºèƒ½ä½“è¡Œä¸º\n",
    "\n",
    "### 4.3.1 ä¸­æ–­æœºåˆ¶ï¼šæ™ºèƒ½ä½“çš„\"æš‚åœé”®\"\n",
    "\n",
    "LangGraph æä¾›äº†ä¸¤ç§ä¸»è¦çš„ä¸­æ–­æœºåˆ¶ï¼š\n",
    "\n",
    "1. **é™æ€ä¸­æ–­**ï¼šåœ¨ç¼–è¯‘æ—¶å®šä¹‰çš„å›ºå®šä¸­æ–­ç‚¹\n",
    "   - `interrupt_before=\"node_name\"`ï¼šåœ¨æŒ‡å®šèŠ‚ç‚¹æ‰§è¡Œå‰ä¸­æ–­\n",
    "   - `interrupt_after=\"node_name\"`ï¼šåœ¨æŒ‡å®šèŠ‚ç‚¹æ‰§è¡Œåä¸­æ–­\n",
    "\n",
    "2. **åŠ¨æ€ä¸­æ–­**ï¼šé€šè¿‡ `interrupt()` å‡½æ•°åœ¨èŠ‚ç‚¹å†…éƒ¨è§¦å‘çš„ä¸­æ–­\n",
    "   - æ›´çµæ´»ï¼Œå¯ä»¥åŸºäºè¿è¡Œæ—¶æ¡ä»¶å†³å®šæ˜¯å¦ä¸­æ–­\n",
    "   - å¯ä»¥ä¼ é€’æ•°æ®ç»™äººç±»ç”¨æˆ·è¿›è¡Œå†³ç­–\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4lej3asxot",
   "metadata": {},
   "source": [
    "##### ç¤ºä¾‹ 4-22ï¼šä½¿ç”¨ interrupt() å‡½æ•°çš„â€œäººå·¥å®¡æ‰¹èŠ‚ç‚¹â€ä»¥åŠåŸºäºäººå·¥è¾“å…¥ï¼ˆæ‰¹å‡† / æ‹’ç»ï¼‰çš„è·¯ç”±é€»è¾‘\n",
    "\n",
    "è®©æˆ‘ä»¬æ„å»ºä¸€ä¸ªæ™ºèƒ½ä½“ï¼Œå®ƒä¼šåœ¨æ‰§è¡Œå…³é”®æ“ä½œå‰è¯·æ±‚äººå·¥å®¡æ‰¹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "htghvpdimib",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langgraph.types import interrupt, Command\n",
    "\n",
    "# å®šä¹‰çŠ¶æ€ç±»å‹\n",
    "class ApprovalState(TypedDict):\n",
    "    topic: str\n",
    "    proposed_action_details: str\n",
    "    final_result: str\n",
    "\n",
    "# å®šä¹‰èŠ‚ç‚¹å‡½æ•°\n",
    "def propose_action(state: ApprovalState) -> ApprovalState:\n",
    "    \"\"\"æå‡ºä¸€ä¸ªéœ€è¦äººç±»å®¡æ‰¹çš„æ“ä½œ\"\"\"\n",
    "    return {\n",
    "        **state,\n",
    "        \"proposed_action_details\": f\"åŸºäºä¸»é¢˜ '{state['topic']}' çš„æ“ä½œæè®®ï¼šå‘é€è¥é”€é‚®ä»¶ç»™1000ä¸ªå®¢æˆ·\"\n",
    "    }\n",
    "\n",
    "def human_approval_node(state: ApprovalState) -> Command[Literal[\"execute_action\", \"revise_action\"]]:\n",
    "    \"\"\"è·å–äººç±»å®¡æ‰¹çš„èŠ‚ç‚¹\"\"\"\n",
    "    approval_request = interrupt(\n",
    "        {\n",
    "            \"question\": \"æ˜¯å¦æ‰¹å‡†æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼Ÿ\",\n",
    "            \"action_details\": state[\"proposed_action_details\"],\n",
    "            \"options\": [\"approve\", \"deny\"]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if approval_request.get(\"user_response\") == \"approve\":\n",
    "        return Command(goto=\"execute_action\")\n",
    "    else:\n",
    "        return Command(goto=\"revise_action\")\n",
    "\n",
    "def execute_action(state: ApprovalState) -> ApprovalState:\n",
    "    \"\"\"æ‰§è¡Œå·²æ‰¹å‡†çš„æ“ä½œ\"\"\"\n",
    "    return {\n",
    "        **state,\n",
    "        \"final_result\": f\"âœ… å·²æ‰§è¡Œæ“ä½œ: {state['proposed_action_details']}\"\n",
    "    }\n",
    "\n",
    "def revise_action(state: ApprovalState) -> ApprovalState:\n",
    "    \"\"\"ä¿®æ”¹è¢«æ‹’ç»çš„æ“ä½œ\"\"\"\n",
    "    return {\n",
    "        **state, \n",
    "        \"final_result\": f\"âŒ æ“ä½œè¢«æ‹’ç»ï¼Œå·²ä¿®æ”¹ä¸º: å‘é€è¥é”€é‚®ä»¶ç»™50ä¸ªç›®æ ‡å®¢æˆ·ï¼ˆç¼©å°è§„æ¨¡ï¼‰\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "r967n77llj",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ„å»ºéœ€è¦äººå·¥å®¡æ‰¹çš„å›¾\n",
    "approval_graph_builder = StateGraph(ApprovalState)\n",
    "\n",
    "# æ·»åŠ èŠ‚ç‚¹\n",
    "approval_graph_builder.add_node(\"propose_action\", propose_action)\n",
    "approval_graph_builder.add_node(\"human_approval\", human_approval_node)\n",
    "approval_graph_builder.add_node(\"execute_action\", execute_action)\n",
    "approval_graph_builder.add_node(\"revise_action\", revise_action)\n",
    "\n",
    "# æ·»åŠ è¾¹\n",
    "approval_graph_builder.add_edge(START, \"propose_action\")\n",
    "approval_graph_builder.add_edge(\"propose_action\", \"human_approval\")\n",
    "approval_graph_builder.add_edge(\"revise_action\", \"human_approval\")  # ä¿®æ”¹åå†æ¬¡è¯·æ±‚å®¡æ‰¹\n",
    "\n",
    "# ç¼–è¯‘å›¾ï¼ˆå¿…é¡»åŒ…å«æ£€æŸ¥ç‚¹å™¨ä»¥æ”¯æŒä¸­æ–­ï¼‰\n",
    "approval_graph = approval_graph_builder.compile(checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ru348eqx2c",
   "metadata": {},
   "source": [
    "ç°åœ¨è®©æˆ‘ä»¬æµ‹è¯•è¿™ä¸ªäººå·¥å®¡æ‰¹æ™ºèƒ½ä½“çš„å·¥ä½œæµç¨‹ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7meefxj997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ç¬¬ä¸€æ­¥ï¼šå¯åŠ¨æ™ºèƒ½ä½“ï¼Œç­‰å¾…äººå·¥å®¡æ‰¹ ===\n",
      "æ‰§è¡Œå®Œæˆï¼š {'topic': 'äº§å“æ¨å¹¿æ´»åŠ¨', 'proposed_action_details': \"åŸºäºä¸»é¢˜ 'äº§å“æ¨å¹¿æ´»åŠ¨' çš„æ“ä½œæè®®ï¼šå‘é€è¥é”€é‚®ä»¶ç»™1000ä¸ªå®¢æˆ·\", '__interrupt__': [Interrupt(value={'question': 'æ˜¯å¦æ‰¹å‡†æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼Ÿ', 'action_details': \"åŸºäºä¸»é¢˜ 'äº§å“æ¨å¹¿æ´»åŠ¨' çš„æ“ä½œæè®®ï¼šå‘é€è¥é”€é‚®ä»¶ç»™1000ä¸ªå®¢æˆ·\", 'options': ['approve', 'deny']}, id='bcb607546bc01bba5ec6be4582d8ee95')]}\n",
      "\n",
      "å½“å‰çŠ¶æ€: {'topic': 'äº§å“æ¨å¹¿æ´»åŠ¨', 'proposed_action_details': \"åŸºäºä¸»é¢˜ 'äº§å“æ¨å¹¿æ´»åŠ¨' çš„æ“ä½œæè®®ï¼šå‘é€è¥é”€é‚®ä»¶ç»™1000ä¸ªå®¢æˆ·\"}\n",
      "ä¸‹ä¸€ä¸ªèŠ‚ç‚¹: ('human_approval',)\n",
      "æ˜¯å¦è¢«ä¸­æ–­: PregelTask(id='1cb3f54d-2f12-5706-a34d-88c0f13a1bda', name='human_approval', path=('__pregel_pull', 'human_approval'), error=None, interrupts=(Interrupt(value={'question': 'æ˜¯å¦æ‰¹å‡†æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼Ÿ', 'action_details': \"åŸºäºä¸»é¢˜ 'äº§å“æ¨å¹¿æ´»åŠ¨' çš„æ“ä½œæè®®ï¼šå‘é€è¥é”€é‚®ä»¶ç»™1000ä¸ªå®¢æˆ·\", 'options': ['approve', 'deny']}, id='bcb607546bc01bba5ec6be4582d8ee95'),), state=None, result=None)\n"
     ]
    }
   ],
   "source": [
    "# å¯åŠ¨å®¡æ‰¹æµç¨‹\n",
    "config = {\"configurable\": {\"thread_id\": \"approval_thread\"}}\n",
    "\n",
    "print(\"=== ç¬¬ä¸€æ­¥ï¼šå¯åŠ¨æ™ºèƒ½ä½“ï¼Œç­‰å¾…äººå·¥å®¡æ‰¹ ===\")\n",
    "try:\n",
    "    result = approval_graph.invoke(\n",
    "        {\"topic\": \"äº§å“æ¨å¹¿æ´»åŠ¨\"}, \n",
    "        config=config\n",
    "    )\n",
    "    print(\"æ‰§è¡Œå®Œæˆï¼š\", result)\n",
    "except Exception as e:\n",
    "    print(f\"æ™ºèƒ½ä½“åœ¨ç­‰å¾…äººå·¥å®¡æ‰¹å¤„ä¸­æ–­: {e}\")\n",
    "    \n",
    "# æ£€æŸ¥å½“å‰çŠ¶æ€\n",
    "current_state = approval_graph.get_state(config)\n",
    "print(f\"\\nå½“å‰çŠ¶æ€: {current_state.values}\")\n",
    "print(f\"ä¸‹ä¸€ä¸ªèŠ‚ç‚¹: {current_state.next}\")\n",
    "print(f\"æ˜¯å¦è¢«ä¸­æ–­: {current_state.tasks[0] if current_state.tasks else 'No interrupts'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c8wrzch92vj",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ç¬¬äºŒæ­¥ï¼šäººå·¥æ‹’ç»å®¡æ‰¹ ===\n",
      "æ‹’ç»åçš„æ‰§è¡Œç»“æœï¼š {'topic': 'äº§å“æ¨å¹¿æ´»åŠ¨', 'proposed_action_details': \"åŸºäºä¸»é¢˜ 'äº§å“æ¨å¹¿æ´»åŠ¨' çš„æ“ä½œæè®®ï¼šå‘é€è¥é”€é‚®ä»¶ç»™1000ä¸ªå®¢æˆ·\", 'final_result': 'âŒ æ“ä½œè¢«æ‹’ç»ï¼Œå·²ä¿®æ”¹ä¸º: å‘é€è¥é”€é‚®ä»¶ç»™50ä¸ªç›®æ ‡å®¢æˆ·ï¼ˆç¼©å°è§„æ¨¡ï¼‰', '__interrupt__': [Interrupt(value={'question': 'æ˜¯å¦æ‰¹å‡†æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼Ÿ', 'action_details': \"åŸºäºä¸»é¢˜ 'äº§å“æ¨å¹¿æ´»åŠ¨' çš„æ“ä½œæè®®ï¼šå‘é€è¥é”€é‚®ä»¶ç»™1000ä¸ªå®¢æˆ·\", 'options': ['approve', 'deny']}, id='d190af3ae0a9a6d9e79508ad44ee627b')]}\n",
      "\n",
      "æ›´æ–°åçŠ¶æ€: {'topic': 'äº§å“æ¨å¹¿æ´»åŠ¨', 'proposed_action_details': \"åŸºäºä¸»é¢˜ 'äº§å“æ¨å¹¿æ´»åŠ¨' çš„æ“ä½œæè®®ï¼šå‘é€è¥é”€é‚®ä»¶ç»™1000ä¸ªå®¢æˆ·\", 'final_result': 'âŒ æ“ä½œè¢«æ‹’ç»ï¼Œå·²ä¿®æ”¹ä¸º: å‘é€è¥é”€é‚®ä»¶ç»™50ä¸ªç›®æ ‡å®¢æˆ·ï¼ˆç¼©å°è§„æ¨¡ï¼‰'}\n",
      "ä¸‹ä¸€ä¸ªèŠ‚ç‚¹: ('human_approval',)\n"
     ]
    }
   ],
   "source": [
    "# æ¨¡æ‹Ÿäººå·¥æ‹’ç»å®¡æ‰¹\n",
    "print(\"\\n=== ç¬¬äºŒæ­¥ï¼šäººå·¥æ‹’ç»å®¡æ‰¹ ===\")\n",
    "try:\n",
    "    result = approval_graph.invoke(\n",
    "        Command(resume={\"user_response\": \"deny\"}), \n",
    "        config=config\n",
    "    )\n",
    "    print(\"æ‹’ç»åçš„æ‰§è¡Œç»“æœï¼š\", result)\n",
    "except Exception as e:\n",
    "    print(f\"ç»§ç»­ç­‰å¾…å®¡æ‰¹: {e}\")\n",
    "    \n",
    "# å†æ¬¡æ£€æŸ¥çŠ¶æ€\n",
    "current_state = approval_graph.get_state(config)\n",
    "print(f\"\\næ›´æ–°åçŠ¶æ€: {current_state.values}\")\n",
    "print(f\"ä¸‹ä¸€ä¸ªèŠ‚ç‚¹: {current_state.next}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1144zbce357r",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ç¬¬ä¸‰æ­¥ï¼šæ‰¹å‡†ä¿®æ”¹åçš„æ“ä½œ ===\n",
      "æœ€ç»ˆæ‰§è¡Œç»“æœï¼š âœ… å·²æ‰§è¡Œæ“ä½œ: åŸºäºä¸»é¢˜ 'äº§å“æ¨å¹¿æ´»åŠ¨' çš„æ“ä½œæè®®ï¼šå‘é€è¥é”€é‚®ä»¶ç»™1000ä¸ªå®¢æˆ·\n"
     ]
    }
   ],
   "source": [
    "# æœ€ç»ˆæ‰¹å‡†ä¿®æ”¹åçš„æ“ä½œ\n",
    "print(\"\\n=== ç¬¬ä¸‰æ­¥ï¼šæ‰¹å‡†ä¿®æ”¹åçš„æ“ä½œ ===\")\n",
    "final_result = approval_graph.invoke(\n",
    "    Command(resume={\"user_response\": \"approve\"}), \n",
    "    config=config\n",
    ")\n",
    "print(\"æœ€ç»ˆæ‰§è¡Œç»“æœï¼š\", final_result[\"final_result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f7864d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StateSnapshot(values={'topic': 'äº§å“æ¨å¹¿æ´»åŠ¨', 'proposed_action_details': \"åŸºäºä¸»é¢˜ 'äº§å“æ¨å¹¿æ´»åŠ¨' çš„æ“ä½œæè®®ï¼šå‘é€è¥é”€é‚®ä»¶ç»™1000ä¸ªå®¢æˆ·\", 'final_result': \"âœ… å·²æ‰§è¡Œæ“ä½œ: åŸºäºä¸»é¢˜ 'äº§å“æ¨å¹¿æ´»åŠ¨' çš„æ“ä½œæè®®ï¼šå‘é€è¥é”€é‚®ä»¶ç»™1000ä¸ªå®¢æˆ·\"}, next=(), config={'configurable': {'thread_id': 'approval_thread', 'checkpoint_ns': '', 'checkpoint_id': '1f07dccb-1994-6b0a-8005-9e3b0eb5ea80'}}, metadata={'source': 'loop', 'step': 5, 'parents': {}}, created_at='2025-08-20T13:50:55.489918+00:00', parent_config={'configurable': {'thread_id': 'approval_thread', 'checkpoint_ns': '', 'checkpoint_id': '1f07dccb-1993-6d86-8004-4ac9c814bad6'}}, tasks=(), interrupts=())\n"
     ]
    }
   ],
   "source": [
    "print(approval_graph.get_state(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe6e05f",
   "metadata": {},
   "source": [
    "##### ç¤ºä¾‹ 4-23ï¼šä½¿ç”¨ interrupt() å‡½æ•°çš„â€œäººå·¥ç¼–è¾‘èŠ‚ç‚¹â€ä»¥åŠä½¿ç”¨äººå·¥ç¼–è¾‘çš„å€¼æ›´æ–°å›¾çŠ¶æ€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a68fd92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.types import interrupt, Command\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# å®šä¹‰å®Œæ•´çš„çŠ¶æ€ç±»\n",
    "class HumanInLoopState(TypedDict):\n",
    "    user_question: str\n",
    "    generated_summary: str\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "def summary_generation_node(state: HumanInLoopState):\n",
    "    \"\"\"ç”Ÿæˆåˆå§‹æ‘˜è¦çš„èŠ‚ç‚¹\"\"\"\n",
    "    user_question = state[\"user_question\"]\n",
    "\n",
    "    # æ¨¡æ‹ŸLLMç”Ÿæˆæ‘˜è¦\n",
    "    generated_summary = f\"\"\"Based on the question \"{user_question}\", \n",
    "here's an initial summary:\n",
    "    \n",
    "LangGraph is a framework for building AI agents with the following \n",
    "features:\n",
    "- State management capabilities\n",
    "- Graph-based workflow design\n",
    "- Integration with LangChain ecosystem\n",
    "- Support for complex agent interactions\n",
    "\n",
    "This summary needs human review for accuracy and completeness.\"\"\"\n",
    "\n",
    "    return {\n",
    "        \"generated_summary\": generated_summary,\n",
    "        \"messages\": [AIMessage(content=f\"Generated initial summary for: {user_question}\")]\n",
    "    }\n",
    "\n",
    "def human_review_node(state: HumanInLoopState):\n",
    "    \"\"\"äººå·¥å®¡æ ¸èŠ‚ç‚¹ - ä½¿ç”¨ Interrupt è¯·æ±‚äººå·¥å¹²é¢„\"\"\"\n",
    "    current_summary = state[\"generated_summary\"]\n",
    "\n",
    "    # è§¦å‘ä¸­æ–­ï¼Œè¯·æ±‚äººå·¥å®¡æ ¸\n",
    "    edited_summary = interrupt(\n",
    "        {\n",
    "            \"task\": \"Please review and edit the generated summary\",\n",
    "            \"current_summary\": current_summary,\n",
    "            \"instructions\": \"Improve accuracy, add missing details, and ensure clarity\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"generated_summary\": edited_summary,\n",
    "        \"messages\": [AIMessage(content=\"Summary sent for human review\")]\n",
    "    }\n",
    "\n",
    "# åˆ›å»ºçŠ¶æ€å›¾\n",
    "graph_builder = StateGraph(HumanInLoopState)\n",
    "\n",
    "# æ·»åŠ èŠ‚ç‚¹\n",
    "graph_builder.add_node(\"generate_summary\", summary_generation_node)\n",
    "graph_builder.add_node(\"human_review\", human_review_node)\n",
    "\n",
    "# å®šä¹‰è¾¹\n",
    "graph_builder.add_edge(START, \"generate_summary\")\n",
    "graph_builder.add_edge(\"generate_summary\", \"human_review\")\n",
    "graph_builder.add_edge(\"human_review\", END)\n",
    "\n",
    "# ç¼–è¯‘å›¾ - åœ¨äººå·¥å®¡æ ¸èŠ‚ç‚¹å‰ä¸­æ–­\n",
    "memory_saver = MemorySaver()\n",
    "complete_human_loop_graph = graph_builder.compile(\n",
    "    checkpointer=memory_saver,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "57ab408d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ç¬¬ä¸€æ­¥ï¼šè¿è¡Œå›¾ç›´åˆ°äººå·¥å®¡æ ¸èŠ‚ç‚¹...\n",
      "==================================================\n",
      "ğŸ“Š ç¬¬ä¸€æ­¥ç»“æœ: {'user_question': 'What are the key advantages of LangGraph for enterprise AI applications?', 'generated_summary': 'Based on the question \"What are the key advantages of LangGraph for enterprise AI applications?\", \\nhere\\'s an initial summary:\\n\\nLangGraph is a framework for building AI agents with the following \\nfeatures:\\n- State management capabilities\\n- Graph-based workflow design\\n- Integration with LangChain ecosystem\\n- Support for complex agent interactions\\n\\nThis summary needs human review for accuracy and completeness.', 'messages': [HumanMessage(content='Please generate a comprehensive summary', additional_kwargs={}, response_metadata={}, id='3713a1a2-4871-4403-bf24-ca0faecc6c31'), AIMessage(content='Generated initial summary for: What are the key advantages of LangGraph for enterprise AI applications?', additional_kwargs={}, response_metadata={}, id='a2bde527-d564-46c2-bd0b-2ffadfa4d6d6')], '__interrupt__': [Interrupt(value={'task': 'Please review and edit the generated summary', 'current_summary': 'Based on the question \"What are the key advantages of LangGraph for enterprise AI applications?\", \\nhere\\'s an initial summary:\\n\\nLangGraph is a framework for building AI agents with the following \\nfeatures:\\n- State management capabilities\\n- Graph-based workflow design\\n- Integration with LangChain ecosystem\\n- Support for complex agent interactions\\n\\nThis summary needs human review for accuracy and completeness.', 'instructions': 'Improve accuracy, add missing details, and ensure clarity'}, id='68da2689c4711efc14e68284023749a5')]}\n",
      "\n",
      "ğŸ” å½“å‰çŠ¶æ€å€¼: {'user_question': 'What are the key advantages of LangGraph for enterprise AI applications?', 'generated_summary': 'Based on the question \"What are the key advantages of LangGraph for enterprise AI applications?\", \\nhere\\'s an initial summary:\\n\\nLangGraph is a framework for building AI agents with the following \\nfeatures:\\n- State management capabilities\\n- Graph-based workflow design\\n- Integration with LangChain ecosystem\\n- Support for complex agent interactions\\n\\nThis summary needs human review for accuracy and completeness.', 'messages': [HumanMessage(content='Please generate a comprehensive summary', additional_kwargs={}, response_metadata={}, id='3713a1a2-4871-4403-bf24-ca0faecc6c31'), AIMessage(content='Generated initial summary for: What are the key advantages of LangGraph for enterprise AI applications?', additional_kwargs={}, response_metadata={}, id='a2bde527-d564-46c2-bd0b-2ffadfa4d6d6')]}\n",
      "â­ï¸ ä¸‹ä¸€ä¸ªå¾…æ‰§è¡ŒèŠ‚ç‚¹: ('human_review',)\n",
      "ğŸ“‹ å¾…å¤„ç†ä»»åŠ¡: (PregelTask(id='b405b0d8-b0c7-c6d8-6d20-37aee25317aa', name='human_review', path=('__pregel_pull', 'human_review'), error=None, interrupts=(Interrupt(value={'task': 'Please review and edit the generated summary', 'current_summary': 'Based on the question \"What are the key advantages of LangGraph for enterprise AI applications?\", \\nhere\\'s an initial summary:\\n\\nLangGraph is a framework for building AI agents with the following \\nfeatures:\\n- State management capabilities\\n- Graph-based workflow design\\n- Integration with LangChain ecosystem\\n- Support for complex agent interactions\\n\\nThis summary needs human review for accuracy and completeness.', 'instructions': 'Improve accuracy, add missing details, and ensure clarity'}, id='68da2689c4711efc14e68284023749a5'),), state=None, result=None),)\n",
      "ğŸ¯ ä»»åŠ¡è¯¦æƒ…: PregelTask(id='b405b0d8-b0c7-c6d8-6d20-37aee25317aa', name='human_review', path=('__pregel_pull', 'human_review'), error=None, interrupts=(Interrupt(value={'task': 'Please review and edit the generated summary', 'current_summary': 'Based on the question \"What are the key advantages of LangGraph for enterprise AI applications?\", \\nhere\\'s an initial summary:\\n\\nLangGraph is a framework for building AI agents with the following \\nfeatures:\\n- State management capabilities\\n- Graph-based workflow design\\n- Integration with LangChain ecosystem\\n- Support for complex agent interactions\\n\\nThis summary needs human review for accuracy and completeness.', 'instructions': 'Improve accuracy, add missing details, and ensure clarity'}, id='68da2689c4711efc14e68284023749a5'),), state=None, result=None)\n",
      "âš ï¸ ä¸­æ–­æ•°æ®: Interrupt(value={'task': 'Please review and edit the generated summary', 'current_summary': 'Based on the question \"What are the key advantages of LangGraph for enterprise AI applications?\", \\nhere\\'s an initial summary:\\n\\nLangGraph is a framework for building AI agents with the following \\nfeatures:\\n- State management capabilities\\n- Graph-based workflow design\\n- Integration with LangChain ecosystem\\n- Support for complex agent interactions\\n\\nThis summary needs human review for accuracy and completeness.', 'instructions': 'Improve accuracy, add missing details, and ensure clarity'}, id='68da2689c4711efc14e68284023749a5')\n",
      "ğŸ“ éœ€è¦å®¡æ ¸çš„å†…å®¹: {'task': 'Please review and edit the generated summary', 'current_summary': 'Based on the question \"What are the key advantages of LangGraph for enterprise AI applications?\", \\nhere\\'s an initial summary:\\n\\nLangGraph is a framework for building AI agents with the following \\nfeatures:\\n- State management capabilities\\n- Graph-based workflow design\\n- Integration with LangChain ecosystem\\n- Support for complex agent interactions\\n\\nThis summary needs human review for accuracy and completeness.', 'instructions': 'Improve accuracy, add missing details, and ensure clarity'}\n"
     ]
    }
   ],
   "source": [
    "# ğŸ’¡ ç¬¬ä¸€æ­¥ï¼šè¿è¡Œå›¾ç›´åˆ°ä¸­æ–­ç‚¹\n",
    "thread_config = {\"configurable\": {\"thread_id\": \"complete_human_loop\"}}\n",
    "\n",
    "print(\"ğŸš€ ç¬¬ä¸€æ­¥ï¼šè¿è¡Œå›¾ç›´åˆ°äººå·¥å®¡æ ¸èŠ‚ç‚¹...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# åˆå§‹è¿è¡Œ - ä¼šåœ¨ human_review èŠ‚ç‚¹å‰åœæ­¢\n",
    "first_result = complete_human_loop_graph.invoke(\n",
    "    {\n",
    "        \"user_question\": \"What are the key advantages of LangGraph for enterprise AI applications?\",\n",
    "        \"messages\": [HumanMessage(content=\"Please generate a comprehensive summary\")]\n",
    "    },\n",
    "    config=thread_config\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“Š ç¬¬ä¸€æ­¥ç»“æœ: {first_result}\")\n",
    "print()\n",
    "\n",
    "# æ£€æŸ¥å½“å‰å›¾çŠ¶æ€\n",
    "current_state = complete_human_loop_graph.get_state(thread_config)\n",
    "print(f\"ğŸ” å½“å‰çŠ¶æ€å€¼: {current_state.values}\")\n",
    "print(f\"â­ï¸ ä¸‹ä¸€ä¸ªå¾…æ‰§è¡ŒèŠ‚ç‚¹: {current_state.next}\")\n",
    "print(f\"ğŸ“‹ å¾…å¤„ç†ä»»åŠ¡: {current_state.tasks}\")\n",
    "\n",
    "if current_state.tasks:\n",
    "    task_info = current_state.tasks[0]\n",
    "    print(f\"ğŸ¯ ä»»åŠ¡è¯¦æƒ…: {task_info}\")\n",
    "    if hasattr(task_info, 'interrupts') and task_info.interrupts:\n",
    "        interrupt_data = task_info.interrupts[0]\n",
    "        print(f\"âš ï¸ ä¸­æ–­æ•°æ®: {interrupt_data}\")\n",
    "        if hasattr(interrupt_data, 'value'):\n",
    "            print(f\"ğŸ“ éœ€è¦å®¡æ ¸çš„å†…å®¹: {interrupt_data.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "77f31b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ ç¬¬äºŒæ­¥ï¼šæä¾›äººå·¥å®¡æ ¸åçš„æ”¹è¿›æ‘˜è¦...\n",
      "==================================================\n",
      "âœï¸ äººå·¥æ”¹è¿›çš„æ‘˜è¦:\n",
      "LangGraph provides significant advantages for \n",
      "enterprise AI applications:\n",
      "\n",
      "ğŸ—ï¸ **Enterprise-Grade Architecture**:\n",
      "- Robust state management with built-in persistence\n",
      "- Scalable graph-based workflow orchestration\n",
      "- Production-ready error handling and recovery\n",
      "\n",
      "ğŸ”§ **Advanced Integration Capabilities**:\n",
      "- Seamless integration with LangChain ecosystem\n",
      "- Support for multiple LLM providers and tools\n",
      "- Human-in-the-loop workflows for quality assurance\n",
      "\n",
      "âš¡ **Performance & User Experience**:\n",
      "- Real-time streaming capabilities for responsive interfaces\n",
      "- Efficient checkpoint and resume functionality\n",
      "- Memory management for long-running conversations\n",
      "\n",
      "ğŸ›¡ï¸ **Enterprise Features**:\n",
      "- Comprehensive monitoring and observability\n",
      "- Security controls and access management\n",
      "- Deployment flexibility across cloud and on-premise\n",
      "\n",
      "This makes LangGraph an ideal choice for mission-critical enterprise AI \n",
      "agent deployments.\n",
      "\n",
      "ğŸ“ æ‘˜è¦é•¿åº¦: 907 å­—ç¬¦\n"
     ]
    }
   ],
   "source": [
    "# ğŸ’¡ ç¬¬äºŒæ­¥ï¼šæ¨¡æ‹Ÿäººå·¥å®¡æ ¸å¹¶æä¾›æ”¹è¿›çš„æ‘˜è¦\n",
    "print(\"\\nğŸš€ ç¬¬äºŒæ­¥ï¼šæä¾›äººå·¥å®¡æ ¸åçš„æ”¹è¿›æ‘˜è¦...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# æ¨¡æ‹Ÿäººå·¥å®¡æ ¸åçš„æ”¹è¿›æ‘˜è¦\n",
    "improved_summary = \"\"\"LangGraph provides significant advantages for \n",
    "enterprise AI applications:\n",
    "\n",
    "ğŸ—ï¸ **Enterprise-Grade Architecture**:\n",
    "- Robust state management with built-in persistence\n",
    "- Scalable graph-based workflow orchestration\n",
    "- Production-ready error handling and recovery\n",
    "\n",
    "ğŸ”§ **Advanced Integration Capabilities**:\n",
    "- Seamless integration with LangChain ecosystem\n",
    "- Support for multiple LLM providers and tools\n",
    "- Human-in-the-loop workflows for quality assurance\n",
    "\n",
    "âš¡ **Performance & User Experience**:\n",
    "- Real-time streaming capabilities for responsive interfaces\n",
    "- Efficient checkpoint and resume functionality\n",
    "- Memory management for long-running conversations\n",
    "\n",
    "ğŸ›¡ï¸ **Enterprise Features**:\n",
    "- Comprehensive monitoring and observability\n",
    "- Security controls and access management\n",
    "- Deployment flexibility across cloud and on-premise\n",
    "\n",
    "This makes LangGraph an ideal choice for mission-critical enterprise AI \n",
    "agent deployments.\"\"\"\n",
    "\n",
    "print(f\"âœï¸ äººå·¥æ”¹è¿›çš„æ‘˜è¦:\\n{improved_summary}\")\n",
    "print(f\"\\nğŸ“ æ‘˜è¦é•¿åº¦: {len(improved_summary)} å­—ç¬¦\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c2706d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ ç¬¬ä¸‰æ­¥ï¼šæ¢å¤å›¾æ‰§è¡Œï¼Œä½¿ç”¨äººå·¥å®¡æ ¸çš„æ‘˜è¦...\n",
      "==================================================\n",
      "âœ… æœ€ç»ˆæ‰§è¡Œç»“æœ: {'user_question': 'What are the key advantages of LangGraph for enterprise AI applications?', 'generated_summary': 'LangGraph provides significant advantages for \\nenterprise AI applications:\\n\\nğŸ—ï¸ **Enterprise-Grade Architecture**:\\n- Robust state management with built-in persistence\\n- Scalable graph-based workflow orchestration\\n- Production-ready error handling and recovery\\n\\nğŸ”§ **Advanced Integration Capabilities**:\\n- Seamless integration with LangChain ecosystem\\n- Support for multiple LLM providers and tools\\n- Human-in-the-loop workflows for quality assurance\\n\\nâš¡ **Performance & User Experience**:\\n- Real-time streaming capabilities for responsive interfaces\\n- Efficient checkpoint and resume functionality\\n- Memory management for long-running conversations\\n\\nğŸ›¡ï¸ **Enterprise Features**:\\n- Comprehensive monitoring and observability\\n- Security controls and access management\\n- Deployment flexibility across cloud and on-premise\\n\\nThis makes LangGraph an ideal choice for mission-critical enterprise AI \\nagent deployments.', 'messages': [HumanMessage(content='Please generate a comprehensive summary', additional_kwargs={}, response_metadata={}, id='3713a1a2-4871-4403-bf24-ca0faecc6c31'), AIMessage(content='Generated initial summary for: What are the key advantages of LangGraph for enterprise AI applications?', additional_kwargs={}, response_metadata={}, id='a2bde527-d564-46c2-bd0b-2ffadfa4d6d6'), AIMessage(content='Summary sent for human review', additional_kwargs={}, response_metadata={}, id='d44f98e3-ccc2-4bf6-8d2d-eedb42ea31f4')]}\n",
      "\n",
      "ğŸ¯ æœ€ç»ˆçŠ¶æ€: {'user_question': 'What are the key advantages of LangGraph for enterprise AI applications?', 'generated_summary': 'LangGraph provides significant advantages for \\nenterprise AI applications:\\n\\nğŸ—ï¸ **Enterprise-Grade Architecture**:\\n- Robust state management with built-in persistence\\n- Scalable graph-based workflow orchestration\\n- Production-ready error handling and recovery\\n\\nğŸ”§ **Advanced Integration Capabilities**:\\n- Seamless integration with LangChain ecosystem\\n- Support for multiple LLM providers and tools\\n- Human-in-the-loop workflows for quality assurance\\n\\nâš¡ **Performance & User Experience**:\\n- Real-time streaming capabilities for responsive interfaces\\n- Efficient checkpoint and resume functionality\\n- Memory management for long-running conversations\\n\\nğŸ›¡ï¸ **Enterprise Features**:\\n- Comprehensive monitoring and observability\\n- Security controls and access management\\n- Deployment flexibility across cloud and on-premise\\n\\nThis makes LangGraph an ideal choice for mission-critical enterprise AI \\nagent deployments.', 'messages': [HumanMessage(content='Please generate a comprehensive summary', additional_kwargs={}, response_metadata={}, id='3713a1a2-4871-4403-bf24-ca0faecc6c31'), AIMessage(content='Generated initial summary for: What are the key advantages of LangGraph for enterprise AI applications?', additional_kwargs={}, response_metadata={}, id='a2bde527-d564-46c2-bd0b-2ffadfa4d6d6'), AIMessage(content='Summary sent for human review', additional_kwargs={}, response_metadata={}, id='d44f98e3-ccc2-4bf6-8d2d-eedb42ea31f4')]}\n",
      "ğŸ“ æœ€ç»ˆæ‘˜è¦é•¿åº¦: 907 å­—ç¬¦\n",
      "\n",
      "ğŸ“¨ æ¶ˆæ¯å†å²:\n",
      "  1. Please generate a comprehensive summary\n",
      "  2. Generated initial summary for: What are the key advantages of LangGraph for enterprise AI applications?\n",
      "  3. Summary sent for human review\n",
      "\n",
      "ğŸ äººå·¥åœ¨ç¯æµç¨‹å®Œæˆï¼æ‘˜è¦å·²é€šè¿‡äººå·¥å®¡æ ¸å¹¶æ”¹è¿›ã€‚\n"
     ]
    }
   ],
   "source": [
    "# ğŸ’¡ ç¬¬ä¸‰æ­¥ï¼šä½¿ç”¨ Command æ¢å¤å›¾æ‰§è¡Œï¼Œä¼ å…¥äººå·¥å®¡æ ¸çš„ç»“æœ\n",
    "print(\"ğŸš€ ç¬¬ä¸‰æ­¥ï¼šæ¢å¤å›¾æ‰§è¡Œï¼Œä½¿ç”¨äººå·¥å®¡æ ¸çš„æ‘˜è¦...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ä½¿ç”¨ Command æ¢å¤å›¾æ‰§è¡Œï¼Œä¼ å…¥æ”¹è¿›åçš„æ‘˜è¦\n",
    "final_result = complete_human_loop_graph.invoke(\n",
    "    Command(resume=improved_summary),  # å°†æ”¹è¿›çš„æ‘˜è¦ä½œä¸ºä¸­æ–­èŠ‚ç‚¹çš„è¿”å›å€¼\n",
    "    config=thread_config\n",
    ")\n",
    "\n",
    "print(f\"âœ… æœ€ç»ˆæ‰§è¡Œç»“æœ: {final_result}\")\n",
    "print()\n",
    "\n",
    "# è·å–æœ€ç»ˆçŠ¶æ€\n",
    "final_state = complete_human_loop_graph.get_state(thread_config)\n",
    "print(f\"ğŸ¯ æœ€ç»ˆçŠ¶æ€: {final_state.values}\")\n",
    "print(f\"ğŸ“ æœ€ç»ˆæ‘˜è¦é•¿åº¦: {len(final_result.get('generated_summary', ''))} å­—ç¬¦\")\n",
    "print()\n",
    "\n",
    "# æ˜¾ç¤ºæ¶ˆæ¯å†å²\n",
    "print(\"ğŸ“¨ æ¶ˆæ¯å†å²:\")\n",
    "for i, msg in enumerate(final_result.get('messages', []), 1):\n",
    "    print(f\"  {i}. {msg.content}\")\n",
    "\n",
    "print(f\"\\nğŸ äººå·¥åœ¨ç¯æµç¨‹å®Œæˆï¼æ‘˜è¦å·²é€šè¿‡äººå·¥å®¡æ ¸å¹¶æ”¹è¿›ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wshuffvh1kq",
   "metadata": {},
   "source": [
    "**ğŸ’¡ äººæœºç¯è·¯åä½œçš„å…³é”®ä¼˜åŠ¿**ï¼š\n",
    "\n",
    "1. **è´¨é‡æ§åˆ¶**ï¼šäººç±»å¯ä»¥åœ¨å…³é”®èŠ‚ç‚¹è¿›è¡Œè´¨é‡æ£€æŸ¥å’Œå†³ç­–\n",
    "2. **é£é™©æ§åˆ¶**ï¼šé¿å…æ™ºèƒ½ä½“æ‰§è¡Œé«˜é£é™©æˆ–ä¸å½“æ“ä½œ\n",
    "3. **å­¦ä¹ æœºä¼š**ï¼šé€šè¿‡äººç±»åé¦ˆæ”¹å–„æ™ºèƒ½ä½“çš„å†³ç­–èƒ½åŠ›\n",
    "4. **çµæ´»æ€§**ï¼šæ”¯æŒå¤æ‚çš„å¤šè½®äº¤äº’å’ŒåŠ¨æ€å†³ç­–è·¯å¾„\n",
    "5. **å¯ä¿¡åº¦**ï¼šå¢å¼ºç”¨æˆ·å¯¹æ™ºèƒ½ä½“ç³»ç»Ÿçš„ä¿¡ä»»å’Œæ§åˆ¶æ„Ÿ\n",
    "\n",
    "è¿™ç§äººæœºåä½œæ¨¡å¼ç‰¹åˆ«é€‚ç”¨äºéœ€è¦äººç±»ç›‘ç£å’ŒæŒ‡å¯¼çš„å¤æ‚æ™ºèƒ½ä½“åº”ç”¨åœºæ™¯ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1jcpj9z4ky7h",
   "metadata": {},
   "source": "## ğŸ“š æœ¬ç« æ€»ç»“\n\né€šè¿‡æœ¬ç« çš„å­¦ä¹ ï¼Œæˆ‘ä»¬æ·±å…¥æŒæ¡äº† LangGraph æ¡†æ¶ä¸­æå‡ AI æ™ºèƒ½ä½“äº¤äº’ä½“éªŒçš„ä¸‰å¤§æ ¸å¿ƒæŠ€æœ¯ã€‚é¦–å…ˆå­¦ä¹ äº†æµå¼å¤„ç†æŠ€æœ¯ï¼ŒæŒæ¡äº† `values`ã€`updates`ã€`custom`ã€`messages`ã€`debug` ç­‰å¤šç§æµå¼æ¨¡å¼ï¼Œèƒ½å¤Ÿå…‹æœ LLM å»¶è¿Ÿå¹¶æä¾›å®æ—¶ç”¨æˆ·åé¦ˆã€‚æ¥ç€æ¢è®¨äº†æŒä¹…åŒ–æœºåˆ¶ï¼Œäº†è§£å¦‚ä½•è·¨ä¼šè¯ç»´æŠ¤æ™ºèƒ½ä½“è®°å¿†ã€å®ç°æ—¶é—´æ—…è¡Œè°ƒè¯•å’Œæ–­ç‚¹ç»­ä¼ åŠŸèƒ½ã€‚æœ€åæ·±å…¥ç ”ç©¶äº†äººæœºç¯è·¯åä½œï¼Œå­¦ä¼šåœ¨å…³é”®å†³ç­–ç‚¹å¼•å…¥äººç±»ç›‘ç£ï¼Œé€šè¿‡é™æ€å’ŒåŠ¨æ€ä¸­æ–­æœºåˆ¶ç¡®ä¿æ™ºèƒ½ä½“è¡Œä¸ºçš„å®‰å…¨æ€§å’Œå¯æ§æ€§ã€‚è¿™äº›æŠ€æœ¯çš„ç»“åˆä½¿æˆ‘ä»¬èƒ½å¤Ÿæ„å»ºä¸ä»…æ™ºèƒ½ï¼Œè€Œä¸”å€¼å¾—ä¿¡èµ–ã€å“åº”è¿…é€Ÿã€ç”¨æˆ·å‹å¥½çš„ AI æ™ºèƒ½ä½“ç³»ç»Ÿï¼Œä¸ºåç»­æ„å»ºå¤æ‚çš„å¤šæ™ºèƒ½ä½“åä½œåœºæ™¯å¥ å®šäº†åšå®åŸºç¡€ã€‚"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}