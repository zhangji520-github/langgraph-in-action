{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7434284",
   "metadata": {},
   "source": [
    "# ç¬¬ 3 ç« ï¼šå›¾é©±åŠ¨çš„ AI æ™ºèƒ½ä½“ç³»ç»Ÿ\n",
    "\n",
    "> æœ¬ç¬”è®°æ–‡ä»¶éœ€è¦ä¸ã€ŠLangGraphå®æˆ˜ã€‹çš„ç¬¬ 3 ç« çš„å†…å®¹é…å¥—ä½¿ç”¨ã€‚\n",
    "\n",
    "æˆ‘ä»¬ä¹ æƒ¯äºç”¨çº¿æ€§çš„ã€å› æœçš„è§†è§’å»ç†è§£ä¸–ç•Œï¼Œå¦‚åŒæ‰‹æŒä¸€å¼ ç²—ç•¥çš„åœ°å›¾ï¼Œåœ¨ä¸€æ¡æ¡æ—¢å®šçš„é“è·¯ä¸ŠæŒ‰éƒ¨å°±ç­åœ°å‰è¡Œã€‚ç„¶è€Œï¼ŒçœŸå®çš„ç–†åŸŸè¿œæ¯”åœ°å›¾å¤æ‚ï¼Œå®ƒå……æ»¡äº†æœªçŸ¥çš„å²”è·¯ã€çªå‘çš„çŠ¶å†µï¼Œä»¥åŠæ— æ•°äº¤ç»‡çš„å¯èƒ½æ€§ã€‚å½“æˆ‘ä»¬è¯•å›¾æ„å»ºèƒ½å¤Ÿç†è§£ã€é€‚åº”å¹¶é©¾é©­è¿™ä¸ªå¤æ‚ä¸–ç•Œçš„ AI æ™ºèƒ½ä½“æ—¶ï¼Œçº¿æ€§çš„æ€ç»´æ¨¡å¼ä¾¿æ˜¾å¾—æ‰è¥Ÿè§è‚˜ã€‚**å›¾**ï¼Œæ­£æ˜¯è¿™æ ·ä¸€ç§æ´»åœ°å›¾ï¼Œå®ƒè¶…è¶Šäº†çº¿æ€§ç»“æ„çš„å±€é™ï¼Œä»¥èŠ‚ç‚¹å’Œè¾¹æ„å»ºèµ·ä¸€ä¸ªå……æ»¡å¯èƒ½æ€§çš„ç½‘ç»œï¼Œè®©æˆ‘ä»¬å¾—ä»¥æç»˜æ™ºèƒ½ä½“ç³»ç»Ÿä¸­é‚£äº›éçº¿æ€§çš„ã€åŠ¨æ€çš„ã€é”™ç»¼å¤æ‚çš„è¡Œä¸ºè·¯å¾„ã€‚\n",
    "\n",
    "æœ¬ç« å°†æ·±å…¥æ¢è®¨ LangGraph çš„å›¾è®¡ç®—æ¨¡å‹ï¼Œå­¦ä¹ å¦‚ä½•åˆ©ç”¨çŠ¶æ€ã€èŠ‚ç‚¹ã€è¾¹ã€å‘½ä»¤è¿™å››ä¸ªæ ¸å¿ƒåŸè¯­æ„å»ºå¤æ‚çš„æ™ºèƒ½ä½“ç³»ç»Ÿï¼ŒæŒæ¡å¹¶è¡Œå¤„ç†ã€MapReduceæ¨¡å¼ã€å­å›¾æœºåˆ¶ç­‰é«˜çº§æŠ€æœ¯ï¼Œæœ€ç»ˆæ„å»ºå‡ºèƒ½å¤Ÿåº”å¯¹çœŸå®ä¸–ç•Œå¤æ‚æŒ‘æˆ˜çš„æ™ºèƒ½ä½“åº”ç”¨ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environment-setup",
   "metadata": {},
   "source": [
    "### ğŸš€ ç¯å¢ƒå‡†å¤‡\n",
    "\n",
    "é¦–å…ˆåŠ è½½å¿…è¦çš„ç¯å¢ƒå˜é‡é…ç½®ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3-1",
   "metadata": {},
   "source": [
    "## 3.1 æ ¸å¿ƒåŸè¯­ï¼šçŠ¶æ€ã€èŠ‚ç‚¹ã€è¾¹å’Œå‘½ä»¤\n",
    "\n",
    "LangGraph çš„æ ¸å¿ƒåœ¨äºå…¶ç®€æ´è€Œå¼ºå¤§çš„å›¾è®¡ç®—æ¨¡å‹ï¼Œè¿™ä¸€æ¨¡å‹çš„åŸºçŸ³ç”±å››ä¸ªæ ¸å¿ƒåŸè¯­æ„æˆï¼š**çŠ¶æ€ï¼ˆStateï¼‰**ã€**èŠ‚ç‚¹ï¼ˆNodeï¼‰**ã€**è¾¹ï¼ˆEdgeï¼‰**ï¼Œä»¥åŠ**å‘½ä»¤ï¼ˆCommandï¼‰**ã€‚\n",
    "\n",
    "ç†è§£è¿™å››ä¸ªåŸè¯­çš„æ¦‚å¿µåŠå…¶ç›¸äº’ä½œç”¨æ–¹å¼ï¼Œæ˜¯æŒæ¡ LangGraph å¹¶æ„å»ºå¤æ‚æ™ºèƒ½ä½“ç³»ç»Ÿçš„é‡ä¸­ä¹‹é‡ã€‚å¯ä»¥å°†è¿™å››ä¸ªåŸè¯­æ¯”ä½œä¹é«˜ç§¯æœ¨æœ€åŸºæœ¬çš„ã€ä¹Ÿæ˜¯æœ€æ ¸å¿ƒçš„æ¨¡å—ï¼Œç†è§£äº†å®ƒä»¬ï¼Œå°±å¦‚åŒæŒæ¡äº†ä¹é«˜æ­å»ºçš„\"è¯­è¨€\"ï¼Œåç»­æ‰èƒ½ä½¿ç”¨æ›´é«˜çº§çš„æŠ€å·§ï¼Œæ­å»ºå‡ºå„ç§å„æ ·ç²¾å·§ã€å¤æ‚ã€åŠŸèƒ½å¼ºå¤§çš„æ™ºèƒ½ä½“ç³»ç»Ÿã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3-1-1",
   "metadata": {},
   "source": [
    "### 3.1.1 çŠ¶æ€ï¼ˆStateï¼‰\n",
    "\n",
    "åœ¨ LangGraph ä¸­ï¼ŒçŠ¶æ€æ˜¯è´¯ç©¿æ™ºèƒ½ä½“ç³»ç»Ÿè¿è¡Œå§‹ç»ˆçš„æ ¸å¿ƒæ¦‚å¿µã€‚æˆ‘ä»¬å¯ä»¥å°†å…¶ç†è§£ä¸ºæ™ºèƒ½ä½“çš„\"çŸ­æœŸè®°å¿†\"ã€\"å·¥ä½œè®°å¿†\"æˆ–è€…\"ä¸´æ—¶å…±äº«æ•°æ®ç©ºé—´\"ï¼Œå®ƒæ‰¿è½½ç€æ™ºèƒ½ä½“åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­äº§ç”Ÿçš„å„ç§ä¿¡æ¯ï¼Œä¾‹å¦‚ç”¨æˆ·çš„è¾“å…¥ã€ä¸­é—´è®¡ç®—ç»“æœã€å·¥å…·çš„è¾“å‡ºã€å¯¹è¯å†å²ç­‰ç­‰ã€‚\n",
    "\n",
    "LangGraph åœ¨çŠ¶æ€å®šä¹‰ä¸Šæä¾›äº†æå¤§çš„çµæ´»æ€§ï¼Œå…è®¸å¼€å‘è€…æ ¹æ®å®é™…åº”ç”¨çš„éœ€æ±‚ï¼Œé€‰æ‹©æœ€åˆé€‚çš„æ•°æ®ç»“æ„æ¥è¡¨ç¤ºçŠ¶æ€ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example-3-1",
   "metadata": {},
   "source": [
    "##### ç¤ºä¾‹ 3-1ï¼šä½¿ç”¨ TypedDict å’Œ Pydantic å®šä¹‰çŠ¶æ€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from pydantic import BaseModel, field_validator\n",
    "\n",
    "# ä½¿ç”¨ TypedDict å®šä¹‰çŠ¶æ€\n",
    "class TypedDictState(TypedDict):\n",
    "    user_input: str\n",
    "    agent_response: str\n",
    "    tool_output: str\n",
    "\n",
    "# ä½¿ç”¨ Pydantic å®šä¹‰çŠ¶æ€ï¼Œå¹¶è¿›è¡Œæ•°æ®éªŒè¯\n",
    "class PydanticState(BaseModel):\n",
    "    user_input: str\n",
    "    agent_response: str\n",
    "    tool_output: str\n",
    "    mood: str = \"neutral\"  # é»˜è®¤æƒ…ç»ªçŠ¶æ€ä¸º neutral\n",
    "\n",
    "    @field_validator('mood')\n",
    "    @classmethod\n",
    "    def validate_mood(cls, value):\n",
    "        if value not in [\"happy\", \"sad\", \"neutral\"]:\n",
    "            raise ValueError(\"æƒ…ç»ªçŠ¶æ€å¿…é¡»æ˜¯ 'happy', 'sad' æˆ– 'neutral'\")\n",
    "        return value\n",
    "\n",
    "# æµ‹è¯•çŠ¶æ€å®šä¹‰\n",
    "print(\"TypedDict çŠ¶æ€ç¤ºä¾‹:\")\n",
    "typed_state = {\"user_input\": \"Hello\", \"agent_response\": \"Hi there!\", \"tool_output\": \"weather data\"}\n",
    "print(typed_state)\n",
    "\n",
    "print(\"\\nPydantic çŠ¶æ€ç¤ºä¾‹:\")\n",
    "pydantic_state = PydanticState(\n",
    "    user_input=\"Hello\", \n",
    "    agent_response=\"Hi there!\", \n",
    "    tool_output=\"weather data\",\n",
    "    mood=\"happy\"\n",
    ")\n",
    "print(pydantic_state.model_dump())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explanation-3-1",
   "metadata": {},
   "source": [
    "**ğŸ’¡ æ ¸å¿ƒæ¦‚å¿µè§£æ**ï¼š\n",
    "\n",
    "- **TypedDict**ï¼šå¯ä»¥å¿«é€Ÿå®šä¹‰ç®€å•çš„çŠ¶æ€ç»“æ„ï¼Œæä¾›ç±»å‹æç¤ºä½†ä¸è¿›è¡Œè¿è¡Œæ—¶éªŒè¯\n",
    "- **Pydantic**ï¼šæä¾›æ›´å¼ºå¤§çš„æ•°æ®å»ºæ¨¡å’ŒéªŒè¯èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨è¿è¡Œæ—¶è¿›è¡Œæ•°æ®éªŒè¯ï¼Œç¡®ä¿çŠ¶æ€çš„ç±»å‹å’Œå–å€¼ç¬¦åˆé¢„æœŸ\n",
    "- **çŠ¶æ€çš„ä½œç”¨**ï¼šä½œä¸ºå„èŠ‚ç‚¹é—´ä¿¡æ¯ä¼ é€’çš„æ¡¥æ¢ï¼Œä¹Ÿæ˜¯æ™ºèƒ½ä½“è¿›è¡Œå†³ç­–å’Œè¡Œä¸ºè°ƒæ•´çš„é‡è¦ä¾æ®"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab12b8c",
   "metadata": {},
   "source": [
    "##### ç¤ºä¾‹ 3-2ï¼šä½¿ç”¨å¤šç»“æ„ä½“å®ç°ç§æœ‰çŠ¶æ€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c172e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "import json\n",
    "\n",
    "# å®šä¹‰å…¨å±€çš„å…¬å…±çŠ¶æ€ Schema\n",
    "class OverallState(TypedDict):\n",
    "    user_input: str\n",
    "    agent_response: str\n",
    "\n",
    "# å®šä¹‰èŠ‚ç‚¹çš„ç§æœ‰çŠ¶æ€ Schema\n",
    "class ToolState(TypedDict):\n",
    "    api_key: str\n",
    "    tool_config: dict\n",
    "    user_input: str  # éœ€è¦åŒ…å«ä»å…¬å…±çŠ¶æ€ä¼ é€’çš„æ•°æ®\n",
    "\n",
    "# æ¨¡æ‹Ÿ API å®¢æˆ·ç«¯ç±»\n",
    "class MockAPIClient:\n",
    "    def __init__(self, api_key: str, config: dict):\n",
    "        self.api_key = api_key\n",
    "        self.config = config\n",
    "        print(f\"åˆå§‹åŒ– API å®¢æˆ·ç«¯ï¼ŒAPI Key: {api_key[:8]}..., é…ç½®: {config}\")\n",
    "\n",
    "    def call_api(self, user_input: str) -> str:\n",
    "        # æ¨¡æ‹Ÿ API è°ƒç”¨\n",
    "        response = f\"åŸºäºè¾“å…¥ '{user_input}' å’Œé…ç½® {self.config}ï¼ŒAPI è¿”å›å¤„ç†ç»“æœ\"\n",
    "        print(f\"è°ƒç”¨ APIï¼Œè¾“å…¥: {user_input}\")\n",
    "        return response\n",
    "\n",
    "def create_api_client(api_key: str, tool_config: dict) -> MockAPIClient:\n",
    "    \"\"\"åˆ›å»º API å®¢æˆ·ç«¯çš„å·¥å‚å‡½æ•°\"\"\"\n",
    "    return MockAPIClient(api_key, tool_config)\n",
    "\n",
    "# å®šä¹‰ä¸€ä¸ªä½¿ç”¨ç§æœ‰çŠ¶æ€çš„èŠ‚ç‚¹\n",
    "def tool_node(state: ToolState) -> OverallState:\n",
    "    \"\"\"ä½¿ç”¨ç§æœ‰çŠ¶æ€çš„å·¥å…·èŠ‚ç‚¹\"\"\"\n",
    "    print(f\"å·¥å…·èŠ‚ç‚¹æ¥æ”¶åˆ°ç§æœ‰çŠ¶æ€: {state}\")\n",
    "\n",
    "    # èŠ‚ç‚¹é€»è¾‘ï¼Œä¾‹å¦‚è°ƒç”¨å·¥å…· API å¹¶æ ¹æ® ToolState ä¸­çš„é…ç½®è¿›è¡Œæ“ä½œ\n",
    "    api_client = create_api_client(state['api_key'], state['tool_config'])\n",
    "    response = api_client.call_api(state['user_input'])\n",
    "\n",
    "    return {\"agent_response\": response}  # è¿”å›æ›´æ–°åçš„å…¬å…±çŠ¶æ€\n",
    "\n",
    "# å®šä¹‰ä¸€ä¸ªè¾“å…¥å¤„ç†èŠ‚ç‚¹\n",
    "def input_processor(state: OverallState) -> OverallState:\n",
    "    \"\"\"å¤„ç†ç”¨æˆ·è¾“å…¥çš„èŠ‚ç‚¹\"\"\"\n",
    "    print(f\"è¾“å…¥å¤„ç†èŠ‚ç‚¹æ¥æ”¶åˆ°: {state['user_input']}\")\n",
    "    processed_input = f\"å·²å¤„ç†: {state['user_input']}\"\n",
    "    return {\"user_input\": processed_input}\n",
    "\n",
    "# å®šä¹‰çŠ¶æ€é€‚é…èŠ‚ç‚¹ï¼Œå°†å…¬å…±çŠ¶æ€è½¬æ¢ä¸ºç§æœ‰çŠ¶æ€\n",
    "def state_adapter(state: OverallState) -> dict:\n",
    "    \"\"\"é€‚é…å™¨èŠ‚ç‚¹ï¼šå°†å…¬å…±çŠ¶æ€è½¬æ¢ä¸ºç§æœ‰çŠ¶æ€\"\"\"\n",
    "    from langgraph.constants import Send\n",
    "\n",
    "    # åˆ›å»ºç§æœ‰çŠ¶æ€æ•°æ®\n",
    "    private_state = {\n",
    "        \"api_key\": \"secret_api_key_12345\",\n",
    "        \"tool_config\": {\n",
    "            \"timeout\": 30,\n",
    "            \"retry_count\": 3,\n",
    "            \"endpoint\": \"https://api.example.com\"\n",
    "        },\n",
    "        \"user_input\": state[\"user_input\"]\n",
    "    }\n",
    "\n",
    "    # ä½¿ç”¨ Send å°†ç§æœ‰çŠ¶æ€å‘é€ç»™å·¥å…·èŠ‚ç‚¹\n",
    "    return Send(\"tool_node\", private_state)\n",
    "\n",
    "# æ„å»ºå›¾\n",
    "builder = StateGraph(OverallState)\n",
    "\n",
    "# æ·»åŠ èŠ‚ç‚¹\n",
    "builder.add_node(\"input_processor\", input_processor)\n",
    "builder.add_node(\"state_adapter\", state_adapter)\n",
    "builder.add_node(\"tool_node\", tool_node)\n",
    "\n",
    "# å®šä¹‰è¾¹\n",
    "builder.add_edge(START, \"input_processor\")\n",
    "builder.add_conditional_edges(\"input_processor\", state_adapter, [\"tool_node\"])\n",
    "builder.add_edge(\"tool_node\", END)\n",
    "\n",
    "# ç¼–è¯‘å›¾\n",
    "graph = builder.compile()\n",
    "\n",
    "# æµ‹è¯•è¿è¡Œ\n",
    "print(\"=== å¤šç»“æ„ä½“çŠ¶æ€ç®¡ç†ç¤ºä¾‹ ===\\n\")\n",
    "\n",
    "# åˆå§‹çŠ¶æ€\n",
    "initial_state = {\n",
    "    \"user_input\": \"æŸ¥è¯¢å¤©æ°”ä¿¡æ¯\",\n",
    "    \"agent_response\": \"\"\n",
    "}\n",
    "\n",
    "print(f\"åˆå§‹çŠ¶æ€: {initial_state}\\n\")\n",
    "\n",
    "# è¿è¡Œå›¾\n",
    "try:\n",
    "    final_state = graph.invoke(initial_state)\n",
    "    print(f\"\\næœ€ç»ˆçŠ¶æ€: {final_state}\")\n",
    "\n",
    "    print(f\"\\n=== æ‰§è¡Œç»“æœ ===\")\n",
    "    print(f\"ç”¨æˆ·è¾“å…¥: {final_state['user_input']}\")\n",
    "    print(f\"æ™ºèƒ½ä½“å“åº”: {final_state['agent_response']}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"æ‰§è¡Œå‡ºé”™: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d9df6d",
   "metadata": {},
   "source": [
    "è¿™ä¸ªå®Œæ•´çš„ç¤ºä¾‹å±•ç¤ºäº†ï¼š\n",
    "\n",
    "1. å¤šç»“æ„ä½“å®šä¹‰ï¼š\n",
    "  - `OverallState`ï¼šå…¨å±€å…¬å…±çŠ¶æ€ï¼ŒåŒ…å«ç”¨æˆ·è¾“å…¥å’Œæ™ºèƒ½ä½“å“åº”\n",
    "  - `ToolState`ï¼šç§æœ‰çŠ¶æ€ï¼ŒåŒ…å« API å¯†é’¥ã€å·¥å…·é…ç½®å’Œç”¨æˆ·è¾“å…¥\n",
    "2. çŠ¶æ€è½¬æ¢æœºåˆ¶ï¼š\n",
    "  - `state_adapter` èŠ‚ç‚¹è´Ÿè´£å°†å…¬å…±çŠ¶æ€è½¬æ¢ä¸ºç§æœ‰çŠ¶æ€\n",
    "  - ä½¿ç”¨ Send API å°†ç§æœ‰çŠ¶æ€å‘é€ç»™ç‰¹å®šèŠ‚ç‚¹\n",
    "3. ç§æœ‰çŠ¶æ€çš„ä½¿ç”¨ï¼š\n",
    "  - `tool_node` æ¥æ”¶ç§æœ‰çŠ¶æ€ï¼ŒåŒ…å«æ•æ„Ÿä¿¡æ¯ï¼ˆAPI å¯†é’¥ã€é…ç½®ï¼‰\n",
    "  - èŠ‚ç‚¹å¤„ç†å®Œæˆåè¿”å›å…¬å…±çŠ¶æ€æ ¼å¼çš„æ›´æ–°\n",
    "4. å®Œæ•´çš„å·¥ä½œæµï¼š\n",
    "  - è¾“å…¥å¤„ç† â†’ çŠ¶æ€é€‚é… â†’ å·¥å…·è°ƒç”¨ â†’ ç»“æœè¿”å›"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740f1a88",
   "metadata": {},
   "source": [
    "##### ç¤ºä¾‹ 3-3ï¼šå®šä¹‰è¾“å…¥/è¾“å‡ºç»“æ„ä½“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "196d3c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—ï¸ æ„å»º LangGraphï¼ˆå¸¦è¾“å…¥è¾“å‡ºç»“æ„ä½“çº¦æŸï¼‰...\n",
      "âœ… å›¾æ„å»ºå®Œæˆï¼\n",
      "\n",
      "=== ğŸš€ è¾“å…¥è¾“å‡ºç»“æ„ä½“çº¦æŸç¤ºä¾‹ ===\n",
      "ğŸ“¥ è¾“å…¥æ•°æ® (InputSchema): {'user_query': 'ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½'}\n",
      "ğŸ” æœç´¢èŠ‚ç‚¹: å¤„ç†æŸ¥è¯¢ 'ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½'\n",
      "ğŸ“Š æœç´¢å®Œæˆï¼Œæ‰¾åˆ° 3 æ¡ç»“æœ\n",
      "ğŸ¤– LLM èŠ‚ç‚¹: åŸºäº 3 æ¡æœç´¢ç»“æœç”Ÿæˆå›å¤\n",
      "ğŸ’­ LLM å¤„ç†å®Œæˆï¼Œç”Ÿæˆ 80 å­—ç¬¦çš„å›å¤\n",
      "ğŸ“ ç”Ÿæˆå›å¤: åŸºäºæ‚¨çš„æŸ¥è¯¢ 'ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½'ï¼Œæˆ‘æ‰¾åˆ°äº†ä»¥ä¸‹ä¿¡æ¯ï¼šæœç´¢ç»“æœ1: å…³äº 'ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½' çš„ä¿¡æ¯ ...\n",
      "\n",
      "ğŸ“¤ è¾“å‡ºæ•°æ® (OutputSchema): {'llm_response': \"åŸºäºæ‚¨çš„æŸ¥è¯¢ 'ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½'ï¼Œæˆ‘æ‰¾åˆ°äº†ä»¥ä¸‹ä¿¡æ¯ï¼šæœç´¢ç»“æœ1: å…³äº 'ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½' çš„ä¿¡æ¯ | æœç´¢ç»“æœ2: ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ ç›¸å…³æ•°æ®ã€‚å¸Œæœ›è¿™èƒ½å¸®åˆ°æ‚¨ï¼\"}\n",
      "ğŸ“Š è¾“å‡ºç±»å‹: <class 'dict'>\n",
      "ğŸ“‹ è¾“å‡ºé”®: ['llm_response']\n"
     ]
    }
   ],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# å®šä¹‰å†…éƒ¨çš„ã€å…¨é¢çš„çŠ¶æ€ç»“æ„ä½“\n",
    "class InternalState(TypedDict):\n",
    "    user_query: str\n",
    "    search_results: list[str]\n",
    "    llm_response: str\n",
    "    debug_info: str  # å†…éƒ¨è°ƒè¯•ä¿¡æ¯ï¼Œä¸éœ€è¦å¯¹å¤–æš´éœ²\n",
    "\n",
    "# å®šä¹‰è¾“å…¥ç»“æ„ä½“ (åªåŒ…å« user_query)\n",
    "class InputSchema(TypedDict):\n",
    "    user_query: str\n",
    "\n",
    "# å®šä¹‰è¾“å‡ºç»“æ„ä½“ (åªåŒ…å« llm_response)\n",
    "class OutputSchema(TypedDict):\n",
    "    llm_response: str\n",
    "\n",
    "# æ¨¡æ‹Ÿæœç´¢èŠ‚ç‚¹\n",
    "def search_node(state: InternalState) -> InternalState:\n",
    "    query = state[\"user_query\"]\n",
    "    print(f\"ğŸ” æœç´¢èŠ‚ç‚¹: å¤„ç†æŸ¥è¯¢ '{query}'\")\n",
    "\n",
    "    # æ¨¡æ‹Ÿæœç´¢ç»“æœ\n",
    "    mock_results = [\n",
    "        f\"æœç´¢ç»“æœ1: å…³äº '{query}' çš„ä¿¡æ¯\",\n",
    "        f\"æœç´¢ç»“æœ2: {query} ç›¸å…³æ•°æ®\",\n",
    "        f\"æœç´¢ç»“æœ3: {query} è¯¦ç»†è¯´æ˜\"\n",
    "    ]\n",
    "\n",
    "    debug = f\"æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(mock_results)} æ¡ç»“æœ\"\n",
    "    print(f\"ğŸ“Š {debug}\")\n",
    "\n",
    "    return {\n",
    "        \"search_results\": mock_results,\n",
    "        \"debug_info\": debug\n",
    "    }\n",
    "\n",
    "# LLM å¤„ç†èŠ‚ç‚¹\n",
    "def llm_node(state: InternalState) -> InternalState:\n",
    "    query = state[\"user_query\"]\n",
    "    results = state[\"search_results\"]\n",
    "\n",
    "    print(f\"ğŸ¤– LLM èŠ‚ç‚¹: åŸºäº {len(results)} æ¡æœç´¢ç»“æœç”Ÿæˆå›å¤\")\n",
    "\n",
    "    # æ¨¡æ‹Ÿ LLM ç”Ÿæˆå“åº”\n",
    "    combined_info = \" | \".join(results[:2])  # ä½¿ç”¨å‰ä¸¤æ¡ç»“æœ\n",
    "    response = f\"åŸºäºæ‚¨çš„æŸ¥è¯¢ '{query}'ï¼Œæˆ‘æ‰¾åˆ°äº†ä»¥ä¸‹ä¿¡æ¯ï¼š{combined_info}ã€‚å¸Œæœ›è¿™èƒ½å¸®åˆ°æ‚¨ï¼\"\n",
    "\n",
    "    debug = f\"LLM å¤„ç†å®Œæˆï¼Œç”Ÿæˆ {len(response)} å­—ç¬¦çš„å›å¤\"\n",
    "    print(f\"ğŸ’­ {debug}\")\n",
    "    print(f\"ğŸ“ ç”Ÿæˆå›å¤: {response[:50]}...\")\n",
    "\n",
    "    return {\n",
    "        \"llm_response\": response,\n",
    "        \"debug_info\": state.get(\"debug_info\", \"\") + f\" | {debug}\"\n",
    "    }\n",
    "\n",
    "# åˆ›å»ºå›¾å¹¶æŒ‡å®šè¾“å…¥è¾“å‡ºç»“æ„ä½“\n",
    "print(\"ğŸ—ï¸ æ„å»º LangGraphï¼ˆå¸¦è¾“å…¥è¾“å‡ºç»“æ„ä½“çº¦æŸï¼‰...\")\n",
    "builder = StateGraph(\n",
    "    state_schema=InternalState,\n",
    "    input_schema=InputSchema,\n",
    "    output_schema=OutputSchema\n",
    ")\n",
    "\n",
    "# æ·»åŠ èŠ‚ç‚¹\n",
    "builder.add_node(\"search\", search_node)\n",
    "builder.add_node(\"llm\", llm_node)\n",
    "\n",
    "# å®šä¹‰è¾¹\n",
    "builder.add_edge(START, \"search\")\n",
    "builder.add_edge(\"search\", \"llm\")\n",
    "builder.add_edge(\"llm\", END)\n",
    "\n",
    "# ç¼–è¯‘å›¾\n",
    "graph = builder.compile()\n",
    "print(\"âœ… å›¾æ„å»ºå®Œæˆï¼\")\n",
    "\n",
    "# æµ‹è¯•è¿è¡Œ\n",
    "print(\"\\n=== ğŸš€ è¾“å…¥è¾“å‡ºç»“æ„ä½“çº¦æŸç¤ºä¾‹ ===\")\n",
    "\n",
    "# åˆ›å»ºç¬¦åˆ InputSchema çš„è¾“å…¥\n",
    "input_data = {\"user_query\": \"ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½\"}\n",
    "print(f\"ğŸ“¥ è¾“å…¥æ•°æ® (InputSchema): {input_data}\")\n",
    "\n",
    "# è¿è¡Œå›¾\n",
    "result = graph.invoke(input_data)\n",
    "# ä¿å­˜å›¾\n",
    "from draw import save_graph_as_png\n",
    "save_graph_as_png(graph, \"./graphs/c3/input_output_state.png\")\n",
    "\n",
    "# ç»“æœè‡ªåŠ¨ç¬¦åˆ OutputSchema æ ¼å¼\n",
    "print(f\"\\nğŸ“¤ è¾“å‡ºæ•°æ® (OutputSchema): {result}\")\n",
    "print(f\"ğŸ“Š è¾“å‡ºç±»å‹: {type(result)}\")\n",
    "print(f\"ğŸ“‹ è¾“å‡ºé”®: {list(result.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a0df85",
   "metadata": {},
   "source": [
    "1. ä¸‰å±‚çŠ¶æ€ç»“æ„ä½“ï¼š\n",
    "  - `InternalState`ï¼šå†…éƒ¨å®Œæ•´çŠ¶æ€ï¼ŒåŒ…å«æ‰€æœ‰ä¸­é—´æ•°æ®\n",
    "  - `InputSchema`ï¼šå¤–éƒ¨è¾“å…¥æ¥å£ï¼Œåªéœ€è¦ç”¨æˆ·æŸ¥è¯¢\n",
    "  - `OutputSchema`ï¼šå¤–éƒ¨è¾“å‡ºæ¥å£ï¼Œåªè¿”å›æœ€ç»ˆå›å¤\n",
    "2. çŠ¶æ€å°è£…ï¼š\n",
    "  - å†…éƒ¨èŠ‚ç‚¹å¯ä»¥è®¿é—®å’Œä¿®æ”¹å®Œæ•´çš„å†…éƒ¨çŠ¶æ€\n",
    "  - å¤–éƒ¨åªèƒ½çœ‹åˆ°å®šä¹‰çš„è¾“å…¥è¾“å‡ºæ ¼å¼\n",
    "  - è°ƒè¯•ä¿¡æ¯ã€æœç´¢ç»“æœç­‰ä¸­é—´æ•°æ®è¢«éšè—\n",
    "3. å®é™…å·¥ä½œæµç¨‹ï¼š\n",
    "  - æœç´¢èŠ‚ç‚¹æ¨¡æ‹Ÿä¿¡æ¯æ£€ç´¢\n",
    "  - LLM èŠ‚ç‚¹åŸºäºæœç´¢ç»“æœç”Ÿæˆå›å¤\n",
    "  - å®Œæ•´çš„çŠ¶æ€ç®¡ç†å’Œæ•°æ®æµ\n",
    "4. è¾“å…¥è¾“å‡ºçº¦æŸéªŒè¯ï¼š\n",
    "  - è¾“å…¥åªéœ€è¦ç¬¦åˆ `InputSchema`\n",
    "  - è¾“å‡ºè‡ªåŠ¨ç¬¦åˆ `OutputSchema`\n",
    "  - å†…éƒ¨å¤æ‚æ€§è¢«å®Œå…¨å°è£…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3-1-1-3",
   "metadata": {},
   "source": [
    "#### 3.1.1.3 çŠ¶æ€ Reducer \n",
    "\n",
    "çŠ¶æ€ Reducer æ˜¯ LangGraph æä¾›çš„ç”¨äºè‡ªå®šä¹‰çŠ¶æ€æ›´æ–°é€»è¾‘çš„æ ¸å¿ƒæœºåˆ¶ã€‚å®ƒå…è®¸æˆ‘ä»¬ç²¾ç»†åœ°æ§åˆ¶çŠ¶æ€åœ¨èŠ‚ç‚¹æ‰§è¡Œè¿‡ç¨‹ä¸­çš„æ¼”å˜æ–¹å¼ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¹¶å‘çŠ¶æ€æ›´æ–°ã€å¤æ‚æ•°æ®ç»“æ„ä»¥åŠéœ€è¦ç‰¹å®šåˆå¹¶ç­–ç•¥çš„åœºæ™¯ä¸‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example-3-4",
   "metadata": {},
   "source": [
    "##### ç¤ºä¾‹ 3-4ï¼šä½¿ç”¨çŠ¶æ€ Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict, Annotated\n",
    "from operator import add\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "\n",
    "# å®šä¹‰çŠ¶æ€ Schemaï¼Œå¹¶ä¸º 'message_history' é”®æŒ‡å®š add_messages Reducer\n",
    "class ChatState(TypedDict):\n",
    "    message_history: Annotated[list[BaseMessage], add_messages]\n",
    "    user_intent: str\n",
    "    tool_output: str\n",
    "\n",
    "# æ¼”ç¤º add_messages Reducer çš„å·¥ä½œæ–¹å¼\n",
    "print(\"æ¼”ç¤º add_messages Reducerï¼š\")\n",
    "\n",
    "# åˆå§‹çŠ¶æ€\n",
    "initial_state = {\n",
    "    \"message_history\": [HumanMessage(content=\"Hello\")],\n",
    "    \"user_intent\": \"greeting\",\n",
    "    \"tool_output\": \"\"\n",
    "}\n",
    "\n",
    "print(\"åˆå§‹çŠ¶æ€:\")\n",
    "for msg in initial_state[\"message_history\"]:\n",
    "    print(f\"  {type(msg).__name__}: {msg.content}\")\n",
    "\n",
    "# æ–°çš„æ¶ˆæ¯æ›´æ–°\n",
    "new_messages = [AIMessage(content=\"Hi there! How can I help you?\")]\n",
    "print(\"\\næ·»åŠ æ–°æ¶ˆæ¯:\")\n",
    "for msg in new_messages:\n",
    "    print(f\"  {type(msg).__name__}: {msg.content}\")\n",
    "\n",
    "# add_messages Reducer ä¼šè‡ªåŠ¨åˆå¹¶æ¶ˆæ¯\n",
    "updated_messages = add_messages(initial_state[\"message_history\"], new_messages)\n",
    "print(\"\\nåˆå¹¶åçš„æ¶ˆæ¯å†å²:\")\n",
    "for msg in updated_messages:\n",
    "    print(f\"  {type(msg).__name__}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example-3-5",
   "metadata": {},
   "source": [
    "##### ç¤ºä¾‹ 3-5 & 3-6ï¼šè‡ªå®šä¹‰ Reducer å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reducer_extend_unique(left: list[str] | None, right: list[str] | None) -> list[str]:\n",
    "    \"\"\"\n",
    "    è‡ªå®šä¹‰ Reducer å‡½æ•°ï¼Œç”¨äºåˆå¹¶ä¸¤ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨ï¼Œå¹¶è¿›è¡Œå»é‡\n",
    "    \"\"\"\n",
    "    existing_items = left if left else []  # å¦‚æœ left ä¸º Noneï¼Œåˆ™åˆå§‹åŒ–ä¸ºç©ºåˆ—è¡¨\n",
    "    new_items = right if right else []     # å¦‚æœ right ä¸º Noneï¼Œåˆ™åˆå§‹åŒ–ä¸ºç©ºåˆ—è¡¨\n",
    "    combined_items = existing_items + new_items\n",
    "    return list(set(combined_items))       # ä½¿ç”¨ set å»é‡å¹¶è½¬æ¢ä¸º list è¿”å›\n",
    "\n",
    "# æµ‹è¯•è‡ªå®šä¹‰ Reducer\n",
    "print(\"æµ‹è¯•è‡ªå®šä¹‰ Reducerï¼š\")\n",
    "\n",
    "existing_list = [\"apple\", \"banana\", \"orange\"]\n",
    "new_list = [\"banana\", \"grape\", \"apple\", \"mango\"]\n",
    "\n",
    "print(f\"å·²æœ‰åˆ—è¡¨: {existing_list}\")\n",
    "print(f\"æ–°å¢åˆ—è¡¨: {new_list}\")\n",
    "\n",
    "result = reducer_extend_unique(existing_list, new_list)\n",
    "print(f\"åˆå¹¶å»é‡ç»“æœ: {result}\")\n",
    "\n",
    "# åœ¨çŠ¶æ€ç»“æ„ä½“ä¸­åº”ç”¨è‡ªå®šä¹‰ Reducer\n",
    "class ChatStateWithCustomReducer(TypedDict):\n",
    "    message_history: Annotated[list[BaseMessage], add_messages]\n",
    "    user_intent: str\n",
    "    tool_output: str\n",
    "    item_list: Annotated[list[str], reducer_extend_unique]  # åº”ç”¨è‡ªå®šä¹‰ Reducer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3-1-1-4",
   "metadata": {},
   "source": [
    "#### 3.1.1.4 Message ä¸ MessagesState\n",
    "\n",
    "åœ¨æ„å»ºå¯¹è¯å‹ AI æ™ºèƒ½ä½“æ—¶ï¼Œå¯¹è¯å†å²è‡³å…³é‡è¦ã€‚LangGraph å¼•å…¥äº†æ¶ˆæ¯ï¼ˆMessageï¼‰å’Œ MessagesState çš„æ¦‚å¿µï¼Œä¸“é—¨ä¼˜åŒ–å¯¹è¯åœºæ™¯çš„çŠ¶æ€ç®¡ç†ã€‚\n",
    "\n",
    "LangChain å®šä¹‰äº†å¤šç§æ¶ˆæ¯ç±»å‹ï¼š\n",
    "- `HumanMessage`ï¼šä»£è¡¨äººç±»ç”¨æˆ·çš„æ¶ˆæ¯\n",
    "- `AIMessage`ï¼šä»£è¡¨ AI æ¨¡å‹ç”Ÿæˆçš„æ¶ˆæ¯\n",
    "- `ToolMessage`ï¼šä»£è¡¨å·¥å…·æ‰§è¡Œåçš„è¾“å‡ºç»“æœæ¶ˆæ¯\n",
    "- `SystemMessage`ï¼šä»£è¡¨ç³»ç»Ÿå‘å‡ºçš„æ¶ˆæ¯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example-3-7",
   "metadata": {},
   "source": [
    "##### ç¤ºä¾‹ 3-7ï¼šä½¿ç”¨ MessagesState å®šä¹‰çŠ¶æ€ç»“æ„ä½“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ¼”ç¤ºä¸åŒç±»å‹çš„æ¶ˆæ¯ï¼š\n",
      "1. SystemMessage: ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¤©æ°”åŠ©æ‰‹ï¼Œè¯·å‹å¥½åœ°å›å¤ç”¨æˆ·\n",
      "2. HumanMessage: ä½ å¥½ï¼Œæˆ‘æƒ³æŸ¥è¯¢ä»Šå¤©çš„å¤©æ°”\n",
      "3. AIMessage: å¥½çš„ï¼Œæˆ‘æ¥å¸®ä½ æŸ¥è¯¢å¤©æ°”ä¿¡æ¯\n",
      "4. ToolMessage: åŒ—äº¬ä»Šå¤©æ™´è½¬å¤šäº‘ï¼Œæ¸©åº¦ 20-28Â°C\n",
      "\n",
      "æ¼”ç¤º MessagesState çš„çŠ¶æ€ç®¡ç†ï¼š\n",
      "messages:[SystemMessage(content='ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¤©æ°”åŠ©æ‰‹ï¼Œè¯·å‹å¥½åœ°å›å¤ç”¨æˆ·', additional_kwargs={}, response_metadata={}), HumanMessage(content='ä½ å¥½ï¼Œæˆ‘æƒ³æŸ¥è¯¢ä»Šå¤©çš„å¤©æ°”', additional_kwargs={}, response_metadata={}), AIMessage(content='å¥½çš„ï¼Œæˆ‘æ¥å¸®ä½ æŸ¥è¯¢å¤©æ°”ä¿¡æ¯', additional_kwargs={}, response_metadata={}), ToolMessage(content='åŒ—äº¬ä»Šå¤©æ™´è½¬å¤šäº‘ï¼Œæ¸©åº¦ 20-28Â°C', tool_call_id='weather_001')]\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, SystemMessage\n",
    "\n",
    "class MyChatState(MessagesState):\n",
    "    \"\"\"\n",
    "    è‡ªå®šä¹‰çš„ ChatState, ç»§æ‰¿è‡ª MessagesState, \n",
    "    è‡ªåŠ¨åŒ…å« messages çŠ¶æ€é”®å’Œ add_messages Reducer\n",
    "    \"\"\"\n",
    "    user_intent: str\n",
    "    tool_output: str\n",
    "    # ... å¯ä»¥æ·»åŠ å…¶ä»–è‡ªå®šä¹‰çš„çŠ¶æ€é”® ...\n",
    "\n",
    "# æ¼”ç¤ºä¸åŒç±»å‹çš„æ¶ˆæ¯\n",
    "print(\"æ¼”ç¤ºä¸åŒç±»å‹çš„æ¶ˆæ¯ï¼š\")\n",
    "\n",
    "# åˆ›å»ºä¸åŒç±»å‹çš„æ¶ˆæ¯\n",
    "human_msg = HumanMessage(content=\"ä½ å¥½ï¼Œæˆ‘æƒ³æŸ¥è¯¢ä»Šå¤©çš„å¤©æ°”\")\n",
    "system_msg = SystemMessage(content=\"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¤©æ°”åŠ©æ‰‹ï¼Œè¯·å‹å¥½åœ°å›å¤ç”¨æˆ·\")\n",
    "ai_msg = AIMessage(content=\"å¥½çš„ï¼Œæˆ‘æ¥å¸®ä½ æŸ¥è¯¢å¤©æ°”ä¿¡æ¯\")\n",
    "tool_msg = ToolMessage(content=\"åŒ—äº¬ä»Šå¤©æ™´è½¬å¤šäº‘ï¼Œæ¸©åº¦ 20-28Â°C\", tool_call_id=\"weather_001\")\n",
    "\n",
    "messages = [system_msg, human_msg, ai_msg, tool_msg]\n",
    "\n",
    "for i, msg in enumerate(messages, 1):\n",
    "    print(f\"{i}. {type(msg).__name__}: {msg.content}\")\n",
    "\n",
    "# æ¼”ç¤º MessagesState çš„ä½¿ç”¨\n",
    "print(\"\\næ¼”ç¤º MessagesState çš„çŠ¶æ€ç®¡ç†ï¼š\")\n",
    "chat_state = {\n",
    "    \"messages\": messages,\n",
    "    \"user_intent\": \"weather_query\",\n",
    "    \"tool_output\": \"weather_data_retrieved\"\n",
    "}\n",
    "\n",
    "# print(f\"å¯¹è¯è½®æ¬¡: {len(chat_state['messages'])}\")\n",
    "# print(f\"ç”¨æˆ·æ„å›¾: {chat_state['user_intent']}\")\n",
    "# print(f\"å·¥å…·è¾“å‡º: {chat_state['tool_output']}\")\n",
    "print(f'messages:{chat_state['messages']}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explanation-3-7",
   "metadata": {},
   "source": [
    "**ğŸ’¡ MessagesState æ ¸å¿ƒç‰¹æ€§**ï¼š\n",
    "\n",
    "- **å†…ç½® `messages` çŠ¶æ€é”®**ï¼šè‡ªåŠ¨æä¾›æ¶ˆæ¯åˆ—è¡¨ç®¡ç†\n",
    "- **é»˜è®¤ `add_messages` Reducer**ï¼šè‡ªåŠ¨å¤„ç†æ¶ˆæ¯è¿½åŠ ã€æ›´æ–°å’Œå»é‡\n",
    "- **æ¶ˆæ¯åºåˆ—åŒ–ä¸ååºåˆ—åŒ–**ï¼šæ”¯æŒ JSON å…¼å®¹çš„å­—å…¸æ ¼å¼ä¼ é€’æ¶ˆæ¯æ•°æ®\n",
    "- **å¯æ‰©å±•æ€§**ï¼šå¯ä»¥è‡ªç”±æ·»åŠ å…¶ä»–è‡ªå®šä¹‰çŠ¶æ€é”®"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d350119",
   "metadata": {},
   "source": [
    "##### ç¤ºä¾‹ 3-8 & 3-9ï¼šåœ¨ä½¿ç”¨ MessagesState çš„èŠ‚ç‚¹ä¸­ä½¿ç”¨ trim_messages å’Œ RemoveMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e673c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, trim_messages, RemoveMessage\n",
    "from langgraph.graph import MessagesState, StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages.utils import count_tokens_approximately\n",
    "\n",
    "llm = ChatOpenAI(model=\"Qwen/Qwen3-8B\")\n",
    "\n",
    "# ä½¿ç”¨ trim_messages çš„èŠ‚ç‚¹\n",
    "def llm_node_with_trim(state: MessagesState):\n",
    "    print(\"ğŸ¤– LLMèŠ‚ç‚¹ (ä½¿ç”¨ trim_messages)\")\n",
    "    message_history = state['messages']\n",
    "    print(f\"ğŸ“¥ æ¥æ”¶åˆ° {len(message_history)} æ¡æ¶ˆæ¯\")\n",
    "\n",
    "    # æ˜¾ç¤ºåŸå§‹æ¶ˆæ¯\n",
    "    for i, msg in enumerate(message_history):\n",
    "        content_preview = msg.content[:50] + \"...\" if len(msg.content) > 50 else msg.content\n",
    "        print(f\"  {i+1}. [{msg.__class__.__name__}] {content_preview}\")\n",
    "\n",
    "    # ä½¿ç”¨ trim_messages ä¿®å‰ªæ¶ˆæ¯å†å²\n",
    "    trimmed_messages = trim_messages(\n",
    "        message_history,\n",
    "        max_tokens=200,  # é™ä½é™åˆ¶ä»¥ä¾¿çœ‹åˆ°ä¿®å‰ªæ•ˆæœ\n",
    "        strategy=\"last\",\n",
    "        token_counter=count_tokens_approximately,\n",
    "        allow_partial=False\n",
    "    )\n",
    "\n",
    "    print(f\"âœ‚ï¸ ä¿®å‰ªåä¿ç•™ {len(trimmed_messages)} æ¡æ¶ˆæ¯ (tokené™åˆ¶: 200)\")\n",
    "\n",
    "    # ç”Ÿæˆå›å¤\n",
    "    llm_response = llm.invoke(trimmed_messages)\n",
    "    print(f\"ğŸ’­ ç”Ÿæˆå›å¤: {llm_response.content}\")\n",
    "\n",
    "    return {\"messages\": [llm_response]}\n",
    "\n",
    "# ä½¿ç”¨ filter_messages çš„èŠ‚ç‚¹ (åŸºäº RemoveMessage)\n",
    "def filter_node(state: MessagesState):\n",
    "    print(\"\\nğŸ”§ è¿‡æ»¤èŠ‚ç‚¹ (ä½¿ç”¨ RemoveMessage)\")\n",
    "    message_history = state['messages']\n",
    "    print(f\"ğŸ“¥ æ¥æ”¶åˆ° {len(message_history)} æ¡æ¶ˆæ¯\")\n",
    "\n",
    "    remove_messages = []\n",
    "\n",
    "    # è¿‡æ»¤ç­–ç•¥ï¼šç§»é™¤åŒ…å«\"ä½ å¥½\"æˆ–\"å†è§\"çš„å¯’æš„æ¶ˆæ¯\n",
    "    for msg in message_history:\n",
    "        if any(greeting in msg.content.lower() for greeting in [\"ä½ å¥½\",\n",
    "\"å†è§\", \"hello\", \"bye\"]):\n",
    "            print(f\"ğŸ—‘ï¸ æ ‡è®°ç§»é™¤å¯’æš„æ¶ˆæ¯: {msg.content[:30]}...\")\n",
    "            remove_messages.append(RemoveMessage(id=msg.id))\n",
    "        # ç§»é™¤è¿‡é•¿çš„æ¶ˆæ¯\n",
    "        elif len(msg.content) > 100:\n",
    "            print(f\"ğŸ—‘ï¸ æ ‡è®°ç§»é™¤è¿‡é•¿æ¶ˆæ¯: {msg.content[:30]}...\")\n",
    "            remove_messages.append(RemoveMessage(id=msg.id))\n",
    "\n",
    "    if remove_messages:\n",
    "        print(f\"ğŸ“Š å°†ç§»é™¤ {len(remove_messages)} æ¡æ¶ˆæ¯\")\n",
    "        return {\"messages\": remove_messages}\n",
    "    else:\n",
    "        print(\"âœ… æ²¡æœ‰éœ€è¦ç§»é™¤çš„æ¶ˆæ¯\")\n",
    "        return {}\n",
    "\n",
    "# æ·»åŠ ç”¨æˆ·æ¶ˆæ¯çš„èŠ‚ç‚¹\n",
    "def add_user_message(state: MessagesState):\n",
    "    print(\"\\nğŸ‘¤ æ·»åŠ ç”¨æˆ·æ¶ˆæ¯èŠ‚ç‚¹\")\n",
    "    new_message = HumanMessage(content=\"æˆ‘æƒ³äº†è§£äººå·¥æ™ºèƒ½çš„æœ€æ–°å‘å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨è‡ªç„¶è¯­è¨€å¤„ç†æ–¹é¢çš„çªç ´ã€‚\")\n",
    "    print(f\"â• æ·»åŠ æ¶ˆæ¯: {new_message.content}\")\n",
    "    return {\"messages\": [new_message]}\n",
    "\n",
    "# åˆ›å»ºå›¾\n",
    "print(\"ğŸ—ï¸ æ„å»ºæ¶ˆæ¯ç®¡ç†ç¤ºä¾‹å›¾...\")\n",
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "# æ·»åŠ èŠ‚ç‚¹\n",
    "builder.add_node(\"add_message\", add_user_message)\n",
    "builder.add_node(\"filter\", filter_node)\n",
    "builder.add_node(\"llm_trim\", llm_node_with_trim)\n",
    "\n",
    "# å®šä¹‰è¾¹\n",
    "builder.add_edge(START, \"add_message\")\n",
    "builder.add_edge(\"add_message\", \"filter\")\n",
    "builder.add_edge(\"filter\", \"llm_trim\")\n",
    "builder.add_edge(\"llm_trim\", END)\n",
    "\n",
    "# ç¼–è¯‘å›¾\n",
    "graph = builder.compile()\n",
    "print(\"âœ… å›¾æ„å»ºå®Œæˆï¼\")\n",
    "\n",
    "# å‡†å¤‡åˆå§‹æ¶ˆæ¯å†å²\n",
    "print(\"\\n=== ğŸš€ æ¶ˆæ¯çŠ¶æ€ç®¡ç†ç¤ºä¾‹ ===\")\n",
    "\n",
    "initial_messages = [\n",
    "    SystemMessage(content=\"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œæ“…é•¿å›ç­”å„ç§é—®é¢˜ã€‚\"),\n",
    "    HumanMessage(content=\"ä½ å¥½ï¼å¾ˆé«˜å…´è§åˆ°ä½ ã€‚\"),\n",
    "    AIMessage(content=\"ä½ å¥½ï¼æˆ‘ä¹Ÿå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ã€‚æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ\"),\n",
    "    HumanMessage(content=\"è¿™æ˜¯ä¸€æ¡å¾ˆé•¿å¾ˆé•¿å¾ˆé•¿å¾ˆé•¿å¾ˆé•¿å¾ˆé•¿å¾ˆé•¿å¾ˆé•¿å¾ˆé•¿å¾ˆé•¿å¾ˆé•¿å¾ˆé•¿å¾ˆé•¿å¾ˆé•¿å¾ˆé•¿å¾ˆé•¿å¾ˆé•¿å¾ˆé•¿çš„æµ‹è¯•æ¶ˆæ¯ï¼Œç”¨æ¥æµ‹è¯•è¿‡æ»¤åŠŸèƒ½ã€‚\"),\n",
    "    AIMessage(content=\"æˆ‘æ˜ç™½äº†æ‚¨çš„æµ‹è¯•æ¶ˆæ¯ã€‚\"),\n",
    "    HumanMessage(content=\"å†è§ï¼\"),\n",
    "]\n",
    "\n",
    "print(\"ğŸ“‹ åˆå§‹æ¶ˆæ¯å†å²:\")\n",
    "for i, msg in enumerate(initial_messages):\n",
    "    content_preview = msg.content[:40] + \"...\" if len(msg.content) > 40 else msg.content\n",
    "    print(f\"  {i+1}. [{msg.__class__.__name__}] {content_preview}\")\n",
    "\n",
    "# è¿è¡Œå›¾\n",
    "result = graph.invoke({\"messages\": initial_messages})\n",
    "\n",
    "print(f\"\\n=== âœ¨ æœ€ç»ˆç»“æœ ===\")\n",
    "print(f\"ğŸ“Š æœ€ç»ˆæ¶ˆæ¯å†å²åŒ…å« {len(result['messages'])} æ¡æ¶ˆæ¯:\")\n",
    "for i, msg in enumerate(result['messages']):\n",
    "    content_preview = msg.content[:50] + \"...\" if len(msg.content) > 50 else msg.content\n",
    "    print(f\"  {i+1}. [{msg.__class__.__name__}] {content_preview}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e029ef6",
   "metadata": {},
   "source": [
    "1. `trim_messages` åŠŸèƒ½ï¼š\n",
    "- æ ¹æ® Token é™åˆ¶è‡ªåŠ¨ä¿®å‰ªæ¶ˆæ¯å†å²\n",
    "- ä½¿ç”¨ `\"last\"` ç­–ç•¥ä¿ç•™æœ€è¿‘çš„æ¶ˆæ¯\n",
    "- æ¨¡æ‹Ÿ Token è®¡æ•°å™¨è¿›è¡Œ Token ä¼°ç®—\n",
    "2. `filter_messages` (`RemoveMessage`) åŠŸèƒ½ï¼š\n",
    "- æ ¹æ®å†…å®¹è§„åˆ™è¿‡æ»¤æ¶ˆæ¯ï¼ˆç§»é™¤å¯’æš„è¯­ï¼‰\n",
    "- æ ¹æ®é•¿åº¦è§„åˆ™è¿‡æ»¤æ¶ˆæ¯ï¼ˆç§»é™¤è¿‡é•¿æ¶ˆæ¯ï¼‰\n",
    "- ä½¿ç”¨ `RemoveMessage` æ ‡è®°è¦åˆ é™¤çš„æ¶ˆæ¯\n",
    "3. `MessagesState` è‡ªåŠ¨ç®¡ç†ï¼š\n",
    "- è‡ªåŠ¨ä½¿ç”¨ `add_messages` Reducer\n",
    "- æ”¯æŒæ¶ˆæ¯çš„æ·»åŠ å’Œç§»é™¤æ“ä½œ\n",
    "- ç»´æŠ¤å®Œæ•´çš„æ¶ˆæ¯å†å²\n",
    "4. å®é™…å·¥ä½œæµç¨‹ï¼š\n",
    "- æ·»åŠ æ–°çš„ç”¨æˆ·æ¶ˆæ¯\n",
    "- è¿‡æ»¤ä¸éœ€è¦çš„æ¶ˆæ¯\n",
    "- ä½¿ç”¨ä¿®å‰ªåçš„æ¶ˆæ¯ç”Ÿæˆ LLM å›å¤\n",
    "- å±•ç¤ºå®Œæ•´çš„æ¶ˆæ¯ç®¡ç†æµç¨‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3-1-2",
   "metadata": {},
   "source": [
    "### 3.1.2 èŠ‚ç‚¹ï¼ˆNodeï¼‰\n",
    "\n",
    "èŠ‚ç‚¹æ˜¯ LangGraph å›¾ç»“æ„ä¸­çš„åŸºæœ¬è®¡ç®—å•å…ƒã€‚æ¯ä¸€ä¸ªèŠ‚ç‚¹éƒ½å°è£…äº†ä¸€ä¸ªç‹¬ç«‹çš„è®¡ç®—é€»è¾‘ï¼Œä¾‹å¦‚è°ƒç”¨è¯­è¨€æ¨¡å‹ã€æ‰§è¡Œå·¥å…·ã€è¿›è¡Œæ¡ä»¶åˆ¤æ–­ã€æˆ–è€…ä»…ä»…æ˜¯ä¸€ä¸ªç®€å•çš„æ•°æ®å¤„ç†å‡½æ•°ã€‚\n",
    "\n",
    "åœ¨ LangGraph ä¸­ï¼ŒèŠ‚ç‚¹æœ¬è´¨ä¸Šå°±æ˜¯ä¸€ä¸ª Python å‡½æ•°ã€‚è¿™ä¸ªå‡½æ•°æ¥æ”¶å½“å‰çš„çŠ¶æ€ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›ä¸€ä¸ªæ–°çš„çŠ¶æ€ï¼ˆæˆ–è€…çŠ¶æ€çš„æ›´æ–°éƒ¨åˆ†ï¼‰ä½œä¸ºè¾“å‡ºã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example-3-10",
   "metadata": {},
   "source": [
    "##### ç¤ºä¾‹ 3-10ï¼šèŠ‚ç‚¹å‡½æ•°çš„åŸºæœ¬ç»“æ„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_node(state):\n",
    "    \"\"\"\n",
    "    èŠ‚ç‚¹å‡½æ•°ç¤ºä¾‹\n",
    "    \"\"\"\n",
    "    # ä»çŠ¶æ€ä¸­è¯»å–æ•°æ®\n",
    "    input_data = state.get(\"some_key\", \"default_value\")\n",
    "    \n",
    "    # æ‰§è¡ŒèŠ‚ç‚¹è®¡ç®—é€»è¾‘\n",
    "    def process_data(data):\n",
    "        return f\"å¤„ç†åçš„æ•°æ®: {data.upper()}\"\n",
    "    \n",
    "    output_data = process_data(input_data)\n",
    "    \n",
    "    # è¿”å›æ–°çš„çŠ¶æ€ï¼ˆæˆ–çŠ¶æ€çš„æ›´æ–°éƒ¨åˆ†ï¼‰\n",
    "    return {\"some_key\": output_data, \"another_key\": \"new_value\"}\n",
    "\n",
    "# æµ‹è¯•èŠ‚ç‚¹å‡½æ•°\n",
    "print(\"æµ‹è¯•èŠ‚ç‚¹å‡½æ•°ï¼š\")\n",
    "test_state = {\"some_key\": \"hello world\", \"existing_key\": \"existing_value\"}\n",
    "print(f\"è¾“å…¥çŠ¶æ€: {test_state}\")\n",
    "\n",
    "result = my_node(test_state)\n",
    "print(f\"èŠ‚ç‚¹è¾“å‡º: {result}\")\n",
    "\n",
    "# æ¨¡æ‹ŸçŠ¶æ€æ›´æ–°ï¼ˆLangGraphä¼šè‡ªåŠ¨å¤„ç†çŠ¶æ€åˆå¹¶ï¼‰\n",
    "updated_state = {**test_state, **result}\n",
    "print(f\"æ›´æ–°åçŠ¶æ€: {updated_state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example-3-11",
   "metadata": {},
   "source": [
    "##### ç¤ºä¾‹ 3-11ï¼šä¸€ä¸ªåŒ…å« LLM èŠ‚ç‚¹çš„ LangGraph å›¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# å®šä¹‰çŠ¶æ€ç»“æ„ä½“ \n",
    "class ChatState(MessagesState):\n",
    "    user_question: str  # ç”¨æˆ·é—®é¢˜\n",
    "    llm_response: str   # LLMå›å¤\n",
    "\n",
    "# å®šä¹‰ LLM èŠ‚ç‚¹ \n",
    "def llm_node(state):\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"human\", \"{question}\")\n",
    "    ])\n",
    "    model = ChatOpenAI(model=\"Qwen/Qwen3-8B\")\n",
    "    chain = prompt | model\n",
    "    \n",
    "    response = chain.invoke({\"question\": state['user_question']}).content\n",
    "    return {\"llm_response\": response}\n",
    "\n",
    "# æ„å»ºå›¾ \n",
    "builder = StateGraph(ChatState)\n",
    "builder.add_node(\"llm_node\", llm_node)\n",
    "builder.add_edge(START, \"llm_node\")\n",
    "builder.add_edge(\"llm_node\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "print(\"LangGraph å›¾æ„å»ºå®Œæˆ\")\n",
    "print(\"èŠ‚ç‚¹: llm_node\")\n",
    "print(\"è¾¹: START -> llm_node -> END\")\n",
    "\n",
    "# æµ‹è¯•å›¾çš„æ‰§è¡Œ\n",
    "print(\"\\næµ‹è¯•å›¾æ‰§è¡Œï¼š\")\n",
    "try:\n",
    "    result = graph.invoke({\"user_question\": \"ä½ å¥½ï¼ŒLangGraphï¼\"})\n",
    "    print(f\"æ‰§è¡Œç»“æœ: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"éœ€è¦é…ç½®APIå¯†é’¥æ‰èƒ½å®é™…è¿è¡ŒLLM: {e}\")\n",
    "    print(\"å›¾ç»“æ„å·²æˆåŠŸåˆ›å»ºï¼Œå¯ä»¥åœ¨é…ç½®APIåè¿è¡Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e7a7e5",
   "metadata": {},
   "source": [
    "##### ç¤ºä¾‹ 3-12ï¼šä¸º LangGraph èŠ‚ç‚¹é…ç½®é‡è¯•ç­–ç•¥çš„ä»£ç ç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99a1f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import sqlite3\n",
    "import random\n",
    "import time\n",
    "from typing import Annotated, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.pregel import RetryPolicy\n",
    "\n",
    "# æ¨¡æ‹Ÿæ•°æ®åº“ç±»\n",
    "class MockSQLDatabase:\n",
    "    def __init__(self):\n",
    "        self.connection_stable = False\n",
    "        self.call_count = 0\n",
    "\n",
    "    def run(self, query):\n",
    "        self.call_count += 1\n",
    "        print(f\"ğŸ—„ï¸ æ•°æ®åº“æŸ¥è¯¢ (ç¬¬{self.call_count}æ¬¡): {query}\")\n",
    "\n",
    "        # æ¨¡æ‹Ÿä¸ç¨³å®šçš„æ•°æ®åº“è¿æ¥ - å‰2æ¬¡è°ƒç”¨ä¼šå¤±è´¥\n",
    "        if self.call_count <= 2:\n",
    "            print(f\"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥ (æ¨¡æ‹Ÿé”™è¯¯)\")\n",
    "            raise sqlite3.OperationalError(\"æ•°æ®åº“è¿æ¥è¶…æ—¶\")\n",
    "\n",
    "        print(f\"âœ… æ•°æ®åº“æŸ¥è¯¢æˆåŠŸ\")\n",
    "        return \"è‰ºæœ¯å®¶æ•°æ®: Van Gogh, Picasso, Da Vinci, Monet, Renoir\"\n",
    "\n",
    "# æ¨¡æ‹Ÿ LLM ç±»\n",
    "class MockChatOpenAI:\n",
    "    def __init__(self, model=\"mock-model\"):\n",
    "        self.model = model\n",
    "        self.call_count = 0\n",
    "\n",
    "    def invoke(self, messages):\n",
    "        self.call_count += 1\n",
    "        print(f\"ğŸ¤– LLMè°ƒç”¨ (ç¬¬{self.call_count}æ¬¡)\")\n",
    "\n",
    "        # æ¨¡æ‹Ÿ LLM å¶å°”å¤±è´¥ - 30% æ¦‚ç‡å¤±è´¥\n",
    "        if random.random() < 0.3:\n",
    "            print(f\"âŒ LLMæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ (æ¨¡æ‹Ÿé”™è¯¯)\")\n",
    "            raise ConnectionError(\"LLMæœåŠ¡è¿æ¥å¤±è´¥\")\n",
    "\n",
    "        last_message = messages[-1] if messages else None\n",
    "        content = f\"åŸºäºæŸ¥è¯¢ç»“æœï¼Œæˆ‘ä¸ºæ‚¨æ‰¾åˆ°äº†ç›¸å…³çš„è‰ºæœ¯å®¶ä¿¡æ¯ã€‚è¿™æ˜¯ç¬¬{self.call_count}æ¬¡æˆåŠŸè°ƒç”¨çš„å“åº”ã€‚\"\n",
    "        print(f\"âœ… LLMå“åº”ç”ŸæˆæˆåŠŸ\")\n",
    "        return AIMessage(content=content)\n",
    "\n",
    "# åˆå§‹åŒ–æ¨¡æ‹Ÿç»„ä»¶\n",
    "db = MockSQLDatabase()\n",
    "model = MockChatOpenAI(model=\"Mock-GPT-4\")\n",
    "\n",
    "# å®šä¹‰å›¾çš„çŠ¶æ€\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "\n",
    "def query_database(state):\n",
    "    \"\"\"æŸ¥è¯¢æ•°æ®åº“èŠ‚ç‚¹ - é…ç½®äº†ç‰¹å®šå¼‚å¸¸é‡è¯•\"\"\"\n",
    "    print(f\"\\nğŸ“Š æ‰§è¡Œæ•°æ®åº“æŸ¥è¯¢èŠ‚ç‚¹...\")\n",
    "    query_result = db.run(\"SELECT * FROM Artist LIMIT 10;\")\n",
    "    return {\"messages\": [AIMessage(content=f\"æ•°æ®åº“æŸ¥è¯¢ç»“æœ: {query_result}\")]}\n",
    "\n",
    "def call_model(state):\n",
    "    \"\"\"è°ƒç”¨æ¨¡å‹èŠ‚ç‚¹ - é…ç½®äº†æœ€å¤§é‡è¯•æ¬¡æ•°\"\"\"\n",
    "    print(f\"\\nğŸ§  æ‰§è¡Œæ¨¡å‹è°ƒç”¨èŠ‚ç‚¹...\")\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def user_input_node(state):\n",
    "    \"\"\"ç”¨æˆ·è¾“å…¥èŠ‚ç‚¹\"\"\"\n",
    "    print(f\"\\nğŸ‘¤ æ·»åŠ ç”¨æˆ·è¾“å…¥...\")\n",
    "    user_message = HumanMessage(content=\"è¯·å¸®æˆ‘æŸ¥è¯¢ä¸€äº›è‘—åè‰ºæœ¯å®¶çš„ä¿¡æ¯\")\n",
    "    print(f\"ğŸ“ ç”¨æˆ·é—®é¢˜: {user_message.content}\")\n",
    "    return {\"messages\": [user_message]}\n",
    "\n",
    "# å®šä¹‰å›¾ builder\n",
    "print(\"ğŸ—ï¸ æ„å»ºå¸¦é‡è¯•ç­–ç•¥çš„ LangGraph...\")\n",
    "builder = StateGraph(AgentState)\n",
    "\n",
    "# æ·»åŠ ç”¨æˆ·è¾“å…¥èŠ‚ç‚¹\n",
    "builder.add_node(\"user_input\", user_input_node)\n",
    "\n",
    "# ä¸º call_model èŠ‚ç‚¹é…ç½®é‡è¯•ç­–ç•¥: æœ€å¤§é‡è¯• 5 æ¬¡ï¼ŒåŒ…å«é€€é¿ç­–ç•¥\n",
    "builder.add_node(\n",
    "    \"model\",\n",
    "    call_model,\n",
    "    retry=RetryPolicy(\n",
    "        max_attempts=5,           # æœ€å¤§é‡è¯•5æ¬¡\n",
    "        initial_interval=0.5,     # åˆå§‹é‡è¯•é—´éš”0.5ç§’\n",
    "        backoff_factor=2.0,       # é€€é¿å› å­2.0 (æŒ‡æ•°é€€é¿)\n",
    "        max_interval=8.0,         # æœ€å¤§é‡è¯•é—´éš”8ç§’\n",
    "        jitter=True              # æ·»åŠ éšæœºæŠ–åŠ¨\n",
    "    )\n",
    ")\n",
    "\n",
    "# ä¸º query_database èŠ‚ç‚¹é…ç½®é‡è¯•ç­–ç•¥: é’ˆå¯¹ sqlite3.OperationalError å¼‚å¸¸è¿›è¡Œé‡è¯•\n",
    "builder.add_node(\n",
    "    \"query_database\",\n",
    "    query_database,\n",
    "    retry=RetryPolicy(\n",
    "        retry_on=sqlite3.OperationalError,  # åªå¯¹æ•°æ®åº“æ“ä½œé”™è¯¯é‡è¯•\n",
    "        max_attempts=4,                     # æœ€å¤§é‡è¯•4æ¬¡\n",
    "        initial_interval=1.0,               # åˆå§‹é—´éš”1ç§’\n",
    "        backoff_factor=1.5                  # è¾ƒå°çš„é€€é¿å› å­\n",
    "    )\n",
    ")\n",
    "\n",
    "# å®šä¹‰è¾¹\n",
    "builder.add_edge(START, \"user_input\")\n",
    "builder.add_edge(\"user_input\", \"model\")\n",
    "builder.add_edge(\"model\", \"query_database\")\n",
    "builder.add_edge(\"query_database\", END)\n",
    "\n",
    "# ç¼–è¯‘å›¾\n",
    "graph = builder.compile()\n",
    "print(\"âœ… å›¾æ„å»ºå®Œæˆï¼\")\n",
    "\n",
    "# æµ‹è¯•è¿è¡Œ\n",
    "print(\"\\n=== ğŸš€ é‡è¯•ç­–ç•¥æ¼”ç¤º ===\")\n",
    "print(\"ğŸ“‹ æµ‹è¯•åœºæ™¯:\")\n",
    "print(\"  - æ•°æ®åº“èŠ‚ç‚¹: å‰2æ¬¡è°ƒç”¨ä¼šå¤±è´¥ï¼Œç¬¬3æ¬¡æˆåŠŸ\")\n",
    "print(\"  - æ¨¡å‹èŠ‚ç‚¹: 30% æ¦‚ç‡å¤±è´¥ï¼Œä¼šè‡ªåŠ¨é‡è¯•\")\n",
    "print(\"  - ä¸¤ä¸ªèŠ‚ç‚¹éƒ½é…ç½®äº†ä¸åŒçš„é‡è¯•ç­–ç•¥\\n\")\n",
    "\n",
    "try:\n",
    "    # è¿è¡Œå›¾\n",
    "    result = graph.invoke({\"messages\": []})\n",
    "\n",
    "    print(f\"\\n=== âœ¨ æ‰§è¡Œå®Œæˆ ===\")\n",
    "    print(f\"ğŸ“Š æœ€ç»ˆæ¶ˆæ¯æ•°é‡: {len(result['messages'])}\")\n",
    "    for i, msg in enumerate(result['messages']):\n",
    "        print(f\"  {i+1}. [{msg.__class__.__name__}] {msg.content[:60]}...\")\n",
    "\n",
    "    print(f\"\\n=== ğŸ“ˆ é‡è¯•ç»Ÿè®¡ ===\")\n",
    "    print(f\"ğŸ—„ï¸ æ•°æ®åº“è°ƒç”¨æ¬¡æ•°: {db.call_count}\")\n",
    "    print(f\"ğŸ¤– æ¨¡å‹è°ƒç”¨æ¬¡æ•°: {model.call_count}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ æ‰§è¡Œå¤±è´¥: {e}\")\n",
    "    print(f\"ğŸ—„ï¸ æ•°æ®åº“è°ƒç”¨æ¬¡æ•°: {db.call_count}\")\n",
    "    print(f\"ğŸ¤– æ¨¡å‹è°ƒç”¨æ¬¡æ•°: {model.call_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533b4744",
   "metadata": {},
   "source": [
    "1. `RetryPolicy` é…ç½®ï¼š\n",
    "- `model` èŠ‚ç‚¹ï¼šé€šç”¨é‡è¯•ç­–ç•¥ï¼Œå¤„ç†å„ç§å¼‚å¸¸\n",
    "- `query_database` èŠ‚ç‚¹ï¼šé’ˆå¯¹ç‰¹å®šæ•°æ®åº“å¼‚å¸¸çš„é‡è¯•ç­–ç•¥\n",
    "2. ä¸åŒçš„é‡è¯•å‚æ•°ï¼š\n",
    "- æœ€å¤§é‡è¯•æ¬¡æ•°ã€åˆå§‹é—´éš”ã€é€€é¿å› å­ç­‰\n",
    "- å±•ç¤ºæŒ‡æ•°é€€é¿å’ŒæŠ–åŠ¨æœºåˆ¶\n",
    "3. æ¨¡æ‹Ÿå¤±è´¥åœºæ™¯ï¼š\n",
    "- æ•°æ®åº“è¿æ¥ä¸ç¨³å®šï¼ˆå‰å‡ æ¬¡å¿…ç„¶å¤±è´¥ï¼‰\n",
    "- LLM æœåŠ¡å¶å°”ä¸å¯ç”¨ï¼ˆéšæœºå¤±è´¥ï¼‰\n",
    "4. é‡è¯•æ•ˆæœæ¼”ç¤ºï¼š\n",
    "- æ˜¾ç¤ºæ¯æ¬¡é‡è¯•çš„è¿‡ç¨‹\n",
    "- ç»Ÿè®¡å®é™…è°ƒç”¨æ¬¡æ•°\n",
    "- å±•ç¤ºé‡è¯•ç­–ç•¥çš„å®é™…æ•ˆæœ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3-1-3",
   "metadata": {},
   "source": [
    "### 3.1.3 è¾¹ï¼ˆEdgeï¼‰\n",
    "\n",
    "è¾¹åœ¨ LangGraph ä¸­è´Ÿè´£è¿æ¥ä¸åŒçš„èŠ‚ç‚¹ï¼Œå®šä¹‰æ™ºèƒ½ä½“ç³»ç»Ÿçš„æ‰§è¡Œæµç¨‹ã€‚è¾¹å†³å®šäº†åœ¨æ‰§è¡Œå®Œä¸€ä¸ªèŠ‚ç‚¹ä¹‹åï¼Œä¸‹ä¸€æ­¥åº”è¯¥æ‰§è¡Œå“ªä¸ªèŠ‚ç‚¹ã€‚\n",
    "\n",
    "LangGraph ä¸»è¦æ”¯æŒä¸¤ç§ç±»å‹çš„è¾¹ï¼š\n",
    "- **æ™®é€šè¾¹**ï¼šå®šä¹‰èŠ‚ç‚¹é—´å›ºå®šçš„ã€æ— æ¡ä»¶çš„è¿æ¥å…³ç³»\n",
    "- **æ¡ä»¶è¾¹**ï¼šæä¾›åŸºäºçŠ¶æ€åŠ¨æ€è·¯ç”±çš„èƒ½åŠ›"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example-3-13",
   "metadata": {},
   "source": [
    "##### ç¤ºä¾‹ 3-13ï¼šæ„å›¾è¯†åˆ«ä¸æŠ€èƒ½è·¯ç”±æµç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "# å®šä¹‰çŠ¶æ€\n",
    "class IntentState(TypedDict):\n",
    "    user_input: str\n",
    "    user_intent: str\n",
    "    response: str\n",
    "\n",
    "# æ„å›¾è¯†åˆ«èŠ‚ç‚¹\n",
    "def intent_recognition_node(state):\n",
    "    user_input = state['user_input'].lower()\n",
    "    \n",
    "    # ç®€å•çš„æ„å›¾è¯†åˆ«é€»è¾‘\n",
    "    if 'å¤©æ°”' in user_input or 'weather' in user_input:\n",
    "        intent = \"æŸ¥è¯¢å¤©æ°”\"\n",
    "    elif 'æœºç¥¨' in user_input or 'flight' in user_input:\n",
    "        intent = \"é¢„è®¢æœºç¥¨\"\n",
    "    elif 'æŠ•è¯‰' in user_input or 'complaint' in user_input:\n",
    "        intent = \"æŠ•è¯‰å»ºè®®\"\n",
    "    else:\n",
    "        intent = \"æœªçŸ¥\"\n",
    "    \n",
    "    return {\"user_intent\": intent}\n",
    "\n",
    "# æŠ€èƒ½èŠ‚ç‚¹\n",
    "def weather_query_node(state):\n",
    "    return {\"response\": \"ä»Šå¤©å¤©æ°”æ™´æœ—ï¼Œæ¸©åº¦ 22-28Â°C\"}\n",
    "\n",
    "def flight_booking_node(state):\n",
    "    return {\"response\": \"ä¸ºæ‚¨æŸ¥æ‰¾åˆé€‚çš„æœºç¥¨é€‰é¡¹...\"}\n",
    "\n",
    "def complaint_suggestion_node(state):\n",
    "    return {\"response\": \"æ„Ÿè°¢æ‚¨çš„åé¦ˆï¼Œæˆ‘ä»¬ä¼šè®¤çœŸå¤„ç†æ‚¨çš„å»ºè®®\"}\n",
    "\n",
    "# æ¡ä»¶è·¯ç”±å‡½æ•°\n",
    "def route_to_skill(state):\n",
    "    \"\"\"æ¡ä»¶å‡½æ•°ï¼Œæ ¹æ®ç”¨æˆ·æ„å›¾è·¯ç”±åˆ°ä¸åŒçš„æŠ€èƒ½èŠ‚ç‚¹\"\"\"\n",
    "    user_intent = state['user_intent']\n",
    "    if user_intent == \"æŸ¥è¯¢å¤©æ°”\":\n",
    "        return \"weather_query_node\"  # è·³è½¬åˆ°æŸ¥è¯¢å¤©æ°”èŠ‚ç‚¹\n",
    "    elif user_intent == \"é¢„è®¢æœºç¥¨\":\n",
    "        return \"flight_booking_node\"  # è·³è½¬åˆ°é¢„è®¢æœºç¥¨èŠ‚ç‚¹\n",
    "    elif user_intent == \"æŠ•è¯‰å»ºè®®\":\n",
    "        return \"complaint_suggestion_node\"  # è·³è½¬åˆ°æŠ•è¯‰å»ºè®®å¤„ç†èŠ‚ç‚¹\n",
    "    else:\n",
    "        return END  # æ— æ³•è¯†åˆ«æ„å›¾ï¼Œç»“æŸæµç¨‹\n",
    "\n",
    "# æ„å»ºå›¾\n",
    "builder = StateGraph(IntentState)\n",
    "\n",
    "# æ·»åŠ èŠ‚ç‚¹\n",
    "builder.add_node(\"intent_recognition_node\", intent_recognition_node)\n",
    "builder.add_node(\"weather_query_node\", weather_query_node)\n",
    "builder.add_node(\"flight_booking_node\", flight_booking_node)\n",
    "builder.add_node(\"complaint_suggestion_node\", complaint_suggestion_node)\n",
    "\n",
    "# æ·»åŠ è¾¹\n",
    "builder.add_edge(START, \"intent_recognition_node\")\n",
    "builder.add_conditional_edges(\n",
    "    \"intent_recognition_node\", \n",
    "    route_to_skill,\n",
    "    [\"weather_query_node\", \"flight_booking_node\", \"complaint_suggestion_node\", END]\n",
    ")\n",
    "builder.add_edge(\"weather_query_node\", END)\n",
    "builder.add_edge(\"flight_booking_node\", END)\n",
    "builder.add_edge(\"complaint_suggestion_node\", END)\n",
    "\n",
    "# ç¼–è¯‘å›¾\n",
    "intent_graph = builder.compile()\n",
    "\n",
    "print(\"æ„å›¾è¯†åˆ«ä¸æŠ€èƒ½è·¯ç”±å›¾æ„å»ºå®Œæˆ\")\n",
    "\n",
    "# æµ‹è¯•ä¸åŒçš„ç”¨æˆ·è¾“å…¥\n",
    "test_inputs = [\n",
    "    \"ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ\",\n",
    "    \"æˆ‘è¦é¢„è®¢æ˜å¤©åˆ°åŒ—äº¬çš„æœºç¥¨\",\n",
    "    \"æˆ‘è¦æŠ•è¯‰ä½ ä»¬çš„æœåŠ¡\",\n",
    "    \"ä½ å¥½\"\n",
    "]\n",
    "\n",
    "for user_input in test_inputs:\n",
    "    print(f\"\\nç”¨æˆ·è¾“å…¥: {user_input}\")\n",
    "    result = intent_graph.invoke({\"user_input\": user_input})\n",
    "    print(f\"è¯†åˆ«æ„å›¾: {result.get('user_intent', 'æœªè¯†åˆ«')}\")\n",
    "    print(f\"ç³»ç»Ÿå›å¤: {result.get('response', 'æ— å›å¤')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explanation-3-13",
   "metadata": {},
   "source": [
    "**ğŸ’¡ è¾¹çš„ç±»å‹å’Œåº”ç”¨åœºæ™¯**ï¼š\n",
    "\n",
    "- **æ¡ä»¶è¾¹**ï¼š`add_conditional_edges()` å®ç°åŠ¨æ€è·¯ç”±ï¼Œæ ¹æ®è¿è¡Œæ—¶çŠ¶æ€é€‰æ‹©è·¯å¾„\n",
    "- **æ™®é€šè¾¹**ï¼š`add_edge()` åˆ›å»ºå›ºå®šè¿æ¥ï¼Œé€‚ç”¨äºç¡®å®šçš„æµç¨‹åºåˆ—\n",
    "- **åº”ç”¨åœºæ™¯**ï¼šæ„å›¾è¯†åˆ«ã€å·¥å…·é€‰æ‹©ã€é”™è¯¯å¤„ç†ã€æµç¨‹åˆ†æ”¯ç­‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3-1-4",
   "metadata": {},
   "source": [
    "### 3.1.4 å‘½ä»¤ï¼ˆCommandï¼‰\n",
    "\n",
    "å‘½ä»¤ï¼ˆCommandï¼‰æ˜¯ LangGraph æ–°æ¨å‡ºçš„å¼ºå¤§å·¥å…·ï¼Œå®ƒå…è®¸æˆ‘ä»¬å°†çŠ¶æ€æ›´æ–°å’Œæµç¨‹æ§åˆ¶é€»è¾‘æ•´åˆåˆ°åŒä¸€ä¸ªèŠ‚ç‚¹ä¸­ã€‚Command æ‰“ç ´äº†èŠ‚ç‚¹å’Œè¾¹åŠŸèƒ½ä¸Šçš„ä¼ ç»Ÿåˆ†å·¥ï¼Œèµ‹äºˆäº†èŠ‚ç‚¹æ›´å¼ºå¤§çš„æµç¨‹æ§åˆ¶èƒ½åŠ›ã€‚\n",
    "\n",
    "ä¸€ä¸ª `Command` å¯¹è±¡ä¸»è¦åŒ…å«ä»¥ä¸‹ä¸¤ä¸ªéƒ¨åˆ†ï¼š\n",
    "- `update`ï¼ˆçŠ¶æ€æ›´æ–°ï¼‰ï¼šæŒ‡å®šéœ€è¦æ›´æ–°çš„çŠ¶æ€é”®å€¼å¯¹\n",
    "- `goto`ï¼ˆæµç¨‹è·³è½¬ï¼‰ï¼šæŒ‡å®šä¸‹ä¸€æ­¥è¦æ‰§è¡Œçš„èŠ‚ç‚¹åç§°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example-3-15",
   "metadata": {},
   "source": [
    "##### ç¤ºä¾‹ 3-15ï¼šä½¿ç”¨ Command çš„èŠ‚ç‚¹å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.types import Command\n",
    "from typing import Literal\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "# å®šä¹‰çŠ¶æ€\n",
    "class CommandState(TypedDict):\n",
    "    input_data: str\n",
    "    processed_data: str\n",
    "    decision_result: str\n",
    "\n",
    "def decide_next_node(state):\n",
    "    \"\"\"æ ¹æ®çŠ¶æ€å†³å®šä¸‹ä¸€ä¸ªèŠ‚ç‚¹\"\"\"\n",
    "    input_data = state['input_data'].lower()\n",
    "    if 'process_a' in input_data:\n",
    "        return 'node_a'\n",
    "    elif 'process_b' in input_data:\n",
    "        return 'node_b'\n",
    "    else:\n",
    "        return 'node_c'\n",
    "\n",
    "def decision_node(state) -> Command[Literal[\"node_a\", \"node_b\", \"node_c\"]]:\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨ Command çš„èŠ‚ç‚¹å‡½æ•°ç¤ºä¾‹\n",
    "    \"\"\"\n",
    "    # å¤„ç†è¾“å…¥æ•°æ®\n",
    "    input_data = state['input_data']\n",
    "    processed_result = f\"å·²å¤„ç†: {input_data}\"\n",
    "    \n",
    "    # å†³å®šä¸‹ä¸€ä¸ªèŠ‚ç‚¹\n",
    "    next_node_name = decide_next_node(state)\n",
    "    \n",
    "    return Command(\n",
    "        update={\"processed_data\": processed_result, \"decision_result\": f\"å†³å®šè·³è½¬åˆ°: {next_node_name}\"},  # çŠ¶æ€æ›´æ–°\n",
    "        goto=next_node_name  # æµç¨‹è·³è½¬æŒ‡ä»¤\n",
    "    )\n",
    "\n",
    "# ç›®æ ‡èŠ‚ç‚¹\n",
    "def node_a(state):\n",
    "    return {\"processed_data\": state['processed_data'] + \" -> ç»è¿‡èŠ‚ç‚¹Aå¤„ç†\"}\n",
    "\n",
    "def node_b(state):\n",
    "    return {\"processed_data\": state['processed_data'] + \" -> ç»è¿‡èŠ‚ç‚¹Bå¤„ç†\"}\n",
    "\n",
    "def node_c(state):\n",
    "    return {\"processed_data\": state['processed_data'] + \" -> ç»è¿‡èŠ‚ç‚¹Cå¤„ç†\"}\n",
    "\n",
    "# æ„å»ºä½¿ç”¨ Command çš„å›¾\n",
    "builder = StateGraph(CommandState)\n",
    "\n",
    "# æ·»åŠ èŠ‚ç‚¹\n",
    "builder.add_node(\"decision_node\", decision_node)\n",
    "builder.add_node(\"node_a\", node_a)\n",
    "builder.add_node(\"node_b\", node_b)\n",
    "builder.add_node(\"node_c\", node_c)\n",
    "\n",
    "# æ·»åŠ è¾¹\n",
    "builder.add_edge(START, \"decision_node\")\n",
    "builder.add_edge(\"node_a\", END)\n",
    "builder.add_edge(\"node_b\", END)\n",
    "builder.add_edge(\"node_c\", END)\n",
    "\n",
    "# ç¼–è¯‘å›¾\n",
    "command_graph = builder.compile()\n",
    "\n",
    "print(\"Command ç¤ºä¾‹å›¾æ„å»ºå®Œæˆ\")\n",
    "\n",
    "# æµ‹è¯•ä¸åŒçš„è¾“å…¥\n",
    "test_inputs = [\n",
    "    \"è¯·æ‰§è¡Œ process_a æ“ä½œ\",\n",
    "    \"éœ€è¦ process_b å¤„ç†\",\n",
    "    \"å…¶ä»–ç±»å‹çš„æ“ä½œ\"\n",
    "]\n",
    "\n",
    "for input_data in test_inputs:\n",
    "    print(f\"\\nè¾“å…¥æ•°æ®: {input_data}\")\n",
    "    result = command_graph.invoke({\"input_data\": input_data})\n",
    "    print(f\"å†³ç­–ç»“æœ: {result.get('decision_result', 'æ— ')}\")\n",
    "    print(f\"å¤„ç†ç»“æœ: {result.get('processed_data', 'æ— ')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explanation-3-15",
   "metadata": {},
   "source": [
    "**ğŸ’¡ Command çš„æ ¸å¿ƒä»·å€¼**ï¼š\n",
    "\n",
    "- **å†…èšæ€§**ï¼šå°†çŠ¶æ€æ›´æ–°å’Œæµç¨‹æ§åˆ¶é€»è¾‘å°è£…åœ¨åŒä¸€ä¸ªèŠ‚ç‚¹ä¸­\n",
    "- **ç±»å‹å®‰å…¨**ï¼šé€šè¿‡ `Literal` ç±»å‹æç¤ºç¡®ä¿è·³è½¬ç›®æ ‡çš„æ­£ç¡®æ€§\n",
    "- **å¯è§†åŒ–æ”¯æŒ**ï¼šLangGraph å¯ä»¥é™æ€åˆ†æ Command çš„ç±»å‹æç¤ºç”Ÿæˆå‡†ç¡®çš„æµç¨‹å›¾\n",
    "- **çµæ´»æ€§**ï¼šç‰¹åˆ«é€‚åˆå¤šæ™ºèƒ½ä½“åä½œä¸­çš„\"äº¤æ¥\"åœºæ™¯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4rtx30sakwu",
   "metadata": {},
   "source": [
    "## 3.2 æµç¨‹æ§åˆ¶ï¼šåˆ†æ”¯ä¸å¹¶å‘\n",
    "\n",
    "åœ¨ä¸Šä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬äº†è§£äº† LangGraph çš„æ ¸å¿ƒåŸè¯­ï¼šçŠ¶æ€ã€èŠ‚ç‚¹ã€è¾¹å’Œå‘½ä»¤ã€‚æœ‰äº†è¿™äº›åŸè¯­ï¼Œæˆ‘ä»¬å°±å¯ä»¥æ„å»ºç®€å•çš„çº¿æ€§æµç¨‹ã€‚ä½†åœ¨å®é™…çš„æ™ºèƒ½ä½“ç³»ç»Ÿä¸­ï¼Œæµç¨‹å¾€å¾€ä¸æ˜¯ç®€å•çš„çº¿æ€§æ‰§è¡Œï¼Œè€Œæ˜¯ä¼šæ ¹æ®ä¸åŒçš„æƒ…å†µå’Œæ¡ä»¶ï¼Œå‡ºç°åˆ†æ”¯ã€å¹¶è¡Œã€å¾ªç¯ç­‰å¤æ‚çš„æ§åˆ¶æµã€‚\n",
    "\n",
    "æœ¬èŠ‚æˆ‘ä»¬å°†é‡ç‚¹æ¢è®¨ LangGraph ä¸­çš„æµç¨‹æ§åˆ¶æœºåˆ¶ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•åˆ©ç”¨ LangGraph å®ç°å¹¶è¡Œåˆ†æ”¯ (Parallel Branching)ï¼Œæ„å»ºèƒ½å¤Ÿå¹¶å‘æ‰§è¡Œå¤šä¸ªä»»åŠ¡ã€æé«˜ç³»ç»Ÿæ•ˆç‡çš„æ™ºèƒ½ä½“ç³»ç»Ÿã€‚\n",
    "\n",
    "åœ¨æœ€ç®€å•çš„ LangGraph æµç¨‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨æ™®é€šè¾¹å°†èŠ‚ç‚¹ä¸²è”èµ·æ¥ï¼Œå½¢æˆä¸€æ¡çº¿æ€§çš„æ‰§è¡Œè·¯å¾„ã€‚ä½†çº¿æ€§æµç¨‹æœ‰å…¶å±€é™æ€§ï¼š\n",
    "\n",
    "- **æ— æ³•å¤„ç†æ¡ä»¶åˆ¤æ–­**ï¼šçº¿æ€§æµç¨‹æ— æ³•æ ¹æ®çŠ¶æ€æˆ–å¤–éƒ¨æ¡ä»¶ï¼ŒåŠ¨æ€åœ°é€‰æ‹©ä¸åŒçš„æ‰§è¡Œè·¯å¾„\n",
    "- **æ— æ³•å®ç°å¹¶è¡Œæ‰§è¡Œ**ï¼šçº¿æ€§æµç¨‹åªèƒ½ä¸²è¡Œåœ°æ‰§è¡ŒèŠ‚ç‚¹ï¼Œæ— æ³•åŒæ—¶æ‰§è¡Œå¤šä¸ªç‹¬ç«‹çš„ä»»åŠ¡\n",
    "- **æ•ˆç‡è¾ƒä½**ï¼šå¯¹äºä¸€äº›å¯ä»¥å¹¶è¡Œå¤„ç†çš„ä»»åŠ¡ï¼Œçº¿æ€§æµç¨‹çš„ä¸²è¡Œæ‰§è¡Œæ–¹å¼ä¼šæµªè´¹è®¡ç®—èµ„æº\n",
    "\n",
    "ä¸ºäº†å…‹æœè¿™äº›å±€é™æ€§ï¼ŒLangGraph æä¾›äº†å¼ºå¤§çš„æµç¨‹æ§åˆ¶æœºåˆ¶ï¼Œå…¶ä¸­åˆ†æ”¯ (Branching) å’Œ å¹¶å‘ (Concurrency) æ˜¯æ ¸å¿ƒè¦ç´ ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w6cgoebnpwf",
   "metadata": {},
   "source": [
    "### 3.2.1 å¹¶è¡Œåˆ†æ”¯ï¼šæ‰‡å‡º (Fan-out) ä¸æ‰‡å…¥ (Fan-in)\n",
    "\n",
    "LangGraph å®ç°å¹¶è¡Œåˆ†æ”¯çš„æ ¸å¿ƒæœºåˆ¶æ˜¯æ‰‡å‡º (Fan-out) å’Œ æ‰‡å…¥ (Fan-in)ã€‚\n",
    "\n",
    "- **æ‰‡å‡º (Fan-out)**ï¼šä»ä¸€ä¸ªèŠ‚ç‚¹å‡ºå‘ï¼ŒåŒæ—¶è§¦å‘å¤šä¸ªä¸‹æ¸¸èŠ‚ç‚¹ï¼Œä½¿å¾—æµç¨‹å¹¶è¡Œåœ°å‘å¤šä¸ªæ–¹å‘åˆ†æ”¯\n",
    "- **æ‰‡å…¥ (Fan-in)**ï¼šå°†å¤šä¸ªå¹¶è¡Œåˆ†æ”¯çš„æµç¨‹æ±‡èšåˆ°ä¸€ä¸ªå…±åŒçš„ä¸‹æ¸¸èŠ‚ç‚¹ï¼Œå®ç°å¹¶è¡Œæµç¨‹çš„åŒæ­¥å’Œæ±‡åˆ\n",
    "\n",
    "è¦å®ç°æ‰‡å‡ºï¼Œæœ€ç®€å•çš„æ–¹å¼æ˜¯ä¸ºä¸€ä¸ªèŠ‚ç‚¹æ·»åŠ å¤šä¸ªå‡ºè¾¹ï¼Œå°†è¯¥èŠ‚ç‚¹åŒæ—¶è¿æ¥åˆ°å¤šä¸ªä¸‹æ¸¸èŠ‚ç‚¹ã€‚å½“ LangGraph æ‰§è¡Œåˆ°è¯¥èŠ‚ç‚¹æ—¶ï¼Œä¼šåŒæ—¶ã€å¹¶å‘åœ°è§¦å‘æ‰€æœ‰å‡ºè¾¹æŒ‡å‘çš„ä¸‹æ¸¸èŠ‚ç‚¹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3jfe3zy3a42",
   "metadata": {},
   "source": [
    "##### ç¤ºä¾‹ 3-16ï¼šæ‰‡å‡ºå’Œæ‰‡å…¥çš„å®Œæ•´å¹¶è¡Œåˆ†æ”¯æµç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x5qfd76xu6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, Any\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# å®šä¹‰çŠ¶æ€ï¼Œä½¿ç”¨ operator.add Reducer å¤„ç†å¹¶è¡Œåˆ†æ”¯çš„çŠ¶æ€æ›´æ–°\n",
    "class ParallelState(TypedDict):\n",
    "    aggregate: Annotated[list, operator.add]\n",
    "\n",
    "def node_a(state: ParallelState):\n",
    "    print(f'Adding \"A\" to {state[\"aggregate\"]}')\n",
    "    return {\"aggregate\": [\"A\"]}\n",
    "\n",
    "def node_b(state: ParallelState):\n",
    "    print(f'Adding \"B\" to {state[\"aggregate\"]}')\n",
    "    return {\"aggregate\": [\"B\"]}\n",
    "\n",
    "def node_c(state: ParallelState):\n",
    "    print(f'Adding \"C\" to {state[\"aggregate\"]}')\n",
    "    return {\"aggregate\": [\"C\"]}\n",
    "\n",
    "def node_d(state: ParallelState):\n",
    "    print(f'Adding \"D\" to {state[\"aggregate\"]}')\n",
    "    return {\"aggregate\": [\"D\"]}\n",
    "\n",
    "# æ„å»ºå¹¶è¡Œåˆ†æ”¯å›¾\n",
    "builder = StateGraph(ParallelState)\n",
    "\n",
    "# æ·»åŠ èŠ‚ç‚¹\n",
    "builder.add_node(\"node_a\", node_a)\n",
    "builder.add_node(\"node_b\", node_b)\n",
    "builder.add_node(\"node_c\", node_c)\n",
    "builder.add_node(\"node_d\", node_d)\n",
    "\n",
    "# æ·»åŠ è¾¹ï¼šå®ç°æ‰‡å‡ºå’Œæ‰‡å…¥\n",
    "builder.add_edge(START, \"node_a\")  # ä» START åˆ° node_a\n",
    "builder.add_edge(\"node_a\", \"node_b\")  # node_a æ‰‡å‡ºåˆ° node_b\n",
    "builder.add_edge(\"node_a\", \"node_c\")  # node_a æ‰‡å‡ºåˆ° node_c (å®ç°æ‰‡å‡º)\n",
    "builder.add_edge(\"node_b\", \"node_d\")  # node_b æ‰‡å…¥åˆ° node_d\n",
    "builder.add_edge(\"node_c\", \"node_d\")  # node_c æ‰‡å…¥åˆ° node_d (å®ç°æ‰‡å…¥)\n",
    "builder.add_edge(\"node_d\", END)  # node_d åˆ° END\n",
    "\n",
    "# ç¼–è¯‘å›¾\n",
    "parallel_graph = builder.compile()\n",
    "\n",
    "print(\"å¹¶è¡Œåˆ†æ”¯å›¾æ„å»ºå®Œæˆ\")\n",
    "print(\"æµç¨‹: START -> node_a -> (node_b, node_c å¹¶è¡Œ) -> node_d -> END\")\n",
    "\n",
    "# æ‰§è¡Œå¹¶è¡Œåˆ†æ”¯æµç¨‹\n",
    "print(\"\\næ‰§è¡Œå¹¶è¡Œåˆ†æ”¯æµç¨‹:\")\n",
    "result = parallel_graph.invoke({\"aggregate\": []})\n",
    "print(f\"\\næœ€ç»ˆç»“æœ: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kbidijuub19",
   "metadata": {},
   "source": [
    "**ğŸ’¡ å¹¶è¡Œåˆ†æ”¯æ ¸å¿ƒæ¦‚å¿µè§£æ**ï¼š\n",
    "\n",
    "- **æ‰‡å‡ºæ•ˆæœ**ï¼šèŠ‚ç‚¹ A æ‰§è¡Œå®Œåï¼ŒåŒæ—¶è§¦å‘èŠ‚ç‚¹ B å’Œ Cï¼Œä¸¤ä¸ªèŠ‚ç‚¹å¹¶å‘æ‰§è¡Œ\n",
    "- **æ‰‡å…¥åŒæ­¥**ï¼šèŠ‚ç‚¹ D å¿…é¡»ç­‰å¾…èŠ‚ç‚¹ B å’Œ C éƒ½æ‰§è¡Œå®Œæˆåæ‰å¼€å§‹æ‰§è¡Œï¼Œèµ·åˆ°åŒæ­¥ç‚¹ä½œç”¨\n",
    "- **çŠ¶æ€ Reducer**ï¼šä½¿ç”¨ `operator.add` Reducer ç¡®ä¿å¹¶è¡Œåˆ†æ”¯çš„çŠ¶æ€æ›´æ–°èƒ½æ­£ç¡®åˆå¹¶\n",
    "- **æ‰§è¡Œæ•ˆç‡**ï¼šå¹¶è¡Œæ‰§è¡Œæ¯”ä¸²è¡Œæ‰§è¡ŒèŠ‚çœæ—¶é—´ï¼Œå¦‚æœ B éœ€è¦ 3 ç§’ï¼ŒC éœ€è¦ 5 ç§’ï¼Œå¹¶è¡Œæ‰§è¡Œæ€»å…±åªéœ€ 5 ç§’\n",
    "\n",
    "å¯ä»¥çœ‹åˆ°æ‰§è¡Œç»“æœä¸­ï¼ŒèŠ‚ç‚¹ B å’Œ C æ˜¯å¹¶å‘æ‰§è¡Œçš„ï¼Œç„¶åèŠ‚ç‚¹ D æ±‡èšäº†ä¸¤ä¸ªåˆ†æ”¯çš„ç»“æœã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ew455k5fbao",
   "metadata": {},
   "source": [
    "### 3.2.2 å¹¶å‘ (Concurrency) è€Œéå¹¶è¡Œ (Parallelism)\n",
    "\n",
    "éœ€è¦æ˜ç¡®çš„æ˜¯ï¼ŒLangGraph å®ç°çš„å¹¶éçœŸæ­£çš„å¹¶è¡Œ (Parallelism)ï¼Œè€Œæ˜¯å¹¶å‘ (Concurrency)ã€‚ç†è§£è¿™ä¸ªåŒºåˆ«å¯¹äºæ­£ç¡®ä½¿ç”¨ LangGraph éå¸¸é‡è¦ï¼š\n",
    "\n",
    "#### å¹¶å‘ vs å¹¶è¡Œçš„æœ¬è´¨åŒºåˆ«\n",
    "\n",
    "- **å¹¶è¡Œ (Parallelism)**ï¼šåŒæ—¶æ‰§è¡Œå¤šä¸ªç‹¬ç«‹çš„ä»»åŠ¡ï¼Œéœ€è¦å¤šä¸ªç‰©ç†è®¡ç®—èµ„æºï¼ˆå¤šæ ¸ CPUã€å¤šå°æœºå™¨ï¼‰çœŸæ­£åœ°åŒæ—¶è¿è¡Œä¸åŒçš„ä»£ç \n",
    "- **å¹¶å‘ (Concurrency)**ï¼šåœ¨å•è®¡ç®—èµ„æºä¸Šï¼Œ\"çœ‹ä¼¼åŒæ—¶\"æ‰§è¡Œå¤šä¸ªä»»åŠ¡ï¼Œé€šè¿‡æ—¶é—´ç‰‡è½®è½¬æˆ–å¼‚æ­¥ IOï¼ŒCPU åœ¨å¤šä¸ªä»»åŠ¡ä¹‹é—´å¿«é€Ÿåˆ‡æ¢\n",
    "\n",
    "#### LangGraph çš„ Superstep æ‰§è¡Œæ¨¡å‹\n",
    "\n",
    "LangGraph å€Ÿé‰´äº† Apache Pregel åˆ†å¸ƒå¼å›¾è®¡ç®—æ¡†æ¶çš„ Superstep (è¶…æ­¥) æ¦‚å¿µï¼š\n",
    "\n",
    "- **Superstep**ï¼šå›¾æ‰§è¡Œçš„åŸºæœ¬è¿­ä»£å•å…ƒï¼Œä»£è¡¨ LangGraph å›¾æ‰§è¡Œè¿‡ç¨‹ä¸­çš„ä¸€ä¸ª\"æ­¥éª¤\"\n",
    "- **å¹¶å‘æ‰§è¡Œ**ï¼šåœ¨æ¯ä¸ª Superstep ä¸­ï¼ŒLangGraph ä¼šå°½å¯èƒ½åœ°å¹¶å‘æ‰§è¡Œæ‰€æœ‰\"å°±ç»ª\"çš„èŠ‚ç‚¹\n",
    "- **åŒæ­¥å±éšœ**ï¼šå½“ Superstep å†…çš„æ‰€æœ‰èŠ‚ç‚¹éƒ½æ‰§è¡Œå®Œæˆåï¼Œè¿›è¡Œå…¨å±€åŒæ­¥ï¼Œåº”ç”¨çŠ¶æ€æ›´æ–°ï¼Œè¿›å…¥ä¸‹ä¸€ä¸ª Superstep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cre0fjl04um",
   "metadata": {},
   "source": [
    "##### ç¤ºä¾‹ 3-17 & 3-18ï¼šSuperstep æ‰§è¡Œæ¨¡å¼æ¼”ç¤º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495pf8a7d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "import operator\n",
    "\n",
    "class SuperstepState(TypedDict):\n",
    "    superstep_log: Annotated[list, operator.add]\n",
    "    timestamp: str\n",
    "\n",
    "def superstep_node_a(state: SuperstepState):\n",
    "    print(f\"Superstep 1: æ‰§è¡ŒèŠ‚ç‚¹ A\")\n",
    "    time.sleep(0.1)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´\n",
    "    return {\"superstep_log\": [\"Superstep 1: Node A executed\"]}\n",
    "\n",
    "def superstep_node_b(state: SuperstepState):\n",
    "    print(f\"Superstep 2: æ‰§è¡ŒèŠ‚ç‚¹ B (å¹¶å‘)\")\n",
    "    time.sleep(0.2)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´\n",
    "    return {\"superstep_log\": [\"Superstep 2: Node B executed\"]}\n",
    "\n",
    "def superstep_node_c(state: SuperstepState):\n",
    "    print(f\"Superstep 2: æ‰§è¡ŒèŠ‚ç‚¹ C (å¹¶å‘)\")\n",
    "    time.sleep(0.15)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´\n",
    "    return {\"superstep_log\": [\"Superstep 2: Node C executed\"]}\n",
    "\n",
    "def superstep_node_d(state: SuperstepState):\n",
    "    print(f\"Superstep 3: æ‰§è¡ŒèŠ‚ç‚¹ D (åŒæ­¥å)\")\n",
    "    time.sleep(0.1)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´\n",
    "    return {\"superstep_log\": [\"Superstep 3: Node D executed\"]}\n",
    "\n",
    "# æ„å»º Superstep æ¼”ç¤ºå›¾\n",
    "builder = StateGraph(SuperstepState)\n",
    "\n",
    "# æ·»åŠ èŠ‚ç‚¹\n",
    "builder.add_node(\"superstep_a\", superstep_node_a)\n",
    "builder.add_node(\"superstep_b\", superstep_node_b) \n",
    "builder.add_node(\"superstep_c\", superstep_node_c)\n",
    "builder.add_node(\"superstep_d\", superstep_node_d)\n",
    "\n",
    "# æ·»åŠ è¾¹ï¼šæ¼”ç¤º Superstep æ‰§è¡Œ\n",
    "builder.add_edge(START, \"superstep_a\")          # Superstep 1: A æ‰§è¡Œ\n",
    "builder.add_edge(\"superstep_a\", \"superstep_b\")  # Superstep 2: A -> B (å¹¶å‘)\n",
    "builder.add_edge(\"superstep_a\", \"superstep_c\")  # Superstep 2: A -> C (å¹¶å‘) \n",
    "builder.add_edge(\"superstep_b\", \"superstep_d\")  # Superstep 3: B -> D (åŒæ­¥)\n",
    "builder.add_edge(\"superstep_c\", \"superstep_d\")  # Superstep 3: C -> D (åŒæ­¥)\n",
    "builder.add_edge(\"superstep_d\", END)\n",
    "\n",
    "superstep_graph = builder.compile()\n",
    "\n",
    "print(\"Superstep æ‰§è¡Œæ¨¡å¼æ¼”ç¤º:\")\n",
    "print(\"Superstep 1: æ‰§è¡Œ A\")\n",
    "print(\"Superstep 2: å¹¶å‘æ‰§è¡Œ B å’Œ C (ç­‰å¾…åŒæ­¥)\")  \n",
    "print(\"Superstep 3: åŒæ­¥åæ‰§è¡Œ D\")\n",
    "print()\n",
    "\n",
    "# æ‰§è¡Œå¹¶è®°å½•æ—¶é—´\n",
    "start_time = time.time()\n",
    "result = superstep_graph.invoke({\"superstep_log\": []})\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"\\\\næ‰§è¡Œå®Œæˆï¼Œæ€»æ—¶é—´: {end_time - start_time:.3f} ç§’\")\n",
    "print(\"æ‰§è¡Œæ—¥å¿—:\")\n",
    "for log in result[\"superstep_log\"]:\n",
    "    print(f\"  - {log}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7jzjr48b5r7",
   "metadata": {},
   "source": [
    "**ğŸ’¡ Superstep æ ¸å¿ƒç‰¹æ€§**ï¼š\n",
    "\n",
    "- **å¹¶å‘æ€§**ï¼šåŒä¸€ä¸ª Superstep å†…çš„å¤šä¸ªèŠ‚ç‚¹å¯ä»¥å¹¶å‘æ‰§è¡Œï¼Œæé«˜æ‰§è¡Œæ•ˆç‡\n",
    "- **åŒæ­¥æ€§**ï¼šæ¯ä¸ª Superstep ç»“æŸæ—¶æœ‰å…¨å±€åŒæ­¥ç‚¹ï¼Œä¿è¯çŠ¶æ€æ›´æ–°çš„åŸå­æ€§å’Œä¸€è‡´æ€§  \n",
    "- **è¿­ä»£æ€§**ï¼šå›¾æ‰§è¡Œæ˜¯ Superstep çš„è¿­ä»£è¿‡ç¨‹ï¼Œæ¯ä¸ª Superstep åœ¨å‰ä¸€ä¸ªåŸºç¡€ä¸Šè®¡ç®—\n",
    "\n",
    "ä»ä¸Šé¢çš„ä¾‹å­å¯ä»¥çœ‹åˆ°ï¼š\n",
    "- Superstep 1: åªæœ‰èŠ‚ç‚¹ A æ‰§è¡Œ\n",
    "- Superstep 2: èŠ‚ç‚¹ B å’Œ C å¹¶å‘æ‰§è¡Œï¼ˆç­‰å¾…è¾ƒæ…¢çš„ä»»åŠ¡å®Œæˆï¼‰\n",
    "- Superstep 3: åŒæ­¥åèŠ‚ç‚¹ D æ‰§è¡Œ\n",
    "\n",
    "æ€»æ‰§è¡Œæ—¶é—´çº¦ç­‰äºæ¯ä¸ª Superstep ä¸­æœ€æ…¢èŠ‚ç‚¹çš„æ—¶é—´ä¹‹å’Œï¼Œè€Œéæ‰€æœ‰èŠ‚ç‚¹æ—¶é—´çš„æ€»å’Œã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j51m8zyo5u",
   "metadata": {},
   "source": [
    "### 3.2.3 é€’å½’é™åˆ¶ä¸å¹¶è¡Œåˆ†æ”¯\n",
    "\n",
    "é€’å½’é™åˆ¶ï¼ˆRecursion Limitï¼‰ç”¨äºé™åˆ¶ LangGraph å›¾æ‰§è¡Œè¿‡ç¨‹ä¸­çš„æœ€å¤§ Superstep è¿­ä»£æ¬¡æ•°ï¼Œé˜²æ­¢å›¾æ— é™å¾ªç¯æ‰§è¡Œã€‚\n",
    "\n",
    "#### é‡è¦ç‰¹æ€§\n",
    "\n",
    "- **è®¡æ•°å•ä½**ï¼šé€’å½’é™åˆ¶çš„è®¡æ•°å•ä½æ˜¯ Superstepï¼Œè€Œä¸æ˜¯èŠ‚ç‚¹\n",
    "- **å¹¶è¡Œä¸å½±å“è®¡æ•°**ï¼šæ— è®ºä¸€ä¸ª Superstep å†…éƒ¨å¹¶å‘æ‰§è¡Œäº†å¤šå°‘ä¸ªèŠ‚ç‚¹ï¼Œéƒ½åªè®¡ä¸ºä¸€æ¬¡ Superstep è¿­ä»£\n",
    "- **é˜²æ­¢æ— é™å¾ªç¯**ï¼šæœ‰æ•ˆé˜²æ­¢å›¾é™·å…¥æ— é™å¾ªç¯ï¼Œä¿æŠ¤è®¡ç®—èµ„æº"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3030f305",
   "metadata": {},
   "source": [
    "##### ç¤ºä¾‹ 3-19ï¼šé€’å½’é™åˆ¶æ˜¯å¯¹å¹¶è¡Œåˆ†æ”¯æµç¨‹çš„é™åˆ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f2c6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, Any\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.errors import GraphRecursionError  # å¯¼å…¥ GraphRecursionError\n",
    "\n",
    "class State(TypedDict):\n",
    "    # operator.add æ˜¯çŠ¶æ€å½’çº¦å™¨ï¼Œç¡®ä¿çŠ¶æ€é”® aggregate ä¸º append-only åˆ—è¡¨\n",
    "    aggregate: Annotated[list, operator.add]\n",
    "\n",
    "def node_a(state):\n",
    "    print(\"ğŸ”„ æ‰§è¡ŒèŠ‚ç‚¹ A\")\n",
    "    return {\"aggregate\": [\"I'm A\"]}\n",
    "\n",
    "def node_b(state):\n",
    "    print(\"ğŸ”„ æ‰§è¡ŒèŠ‚ç‚¹ B\")\n",
    "    return {\"aggregate\": [\"I'm B\"]}\n",
    "\n",
    "def node_c(state):\n",
    "    print(\"ğŸ”„ æ‰§è¡ŒèŠ‚ç‚¹ C\")\n",
    "    return {\"aggregate\": [\"I'm C\"]}\n",
    "\n",
    "def node_d(state):\n",
    "    print(\"ğŸ”„ æ‰§è¡ŒèŠ‚ç‚¹ D\")\n",
    "    return {\"aggregate\": [\"I'm D\"]}\n",
    "\n",
    "# æ„å»ºå›¾\n",
    "print(\"ğŸ—ï¸ æ„å»ºå¹¶è¡Œåˆ†æ”¯å›¾...\")\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"a\", node_a)\n",
    "builder.add_node(\"b\", node_b)\n",
    "builder.add_node(\"c\", node_c)\n",
    "builder.add_node(\"d\", node_d)\n",
    "\n",
    "# æ·»åŠ è¾¹ - åˆ›å»ºå¹¶è¡Œåˆ†æ”¯ç»“æ„\n",
    "builder.add_edge(START, \"a\")\n",
    "builder.add_edge(\"a\", \"b\")\n",
    "builder.add_edge(\"a\", \"c\")  # ä» a æ‰‡å‡ºåˆ° b å’Œ c\n",
    "builder.add_edge(\"b\", \"d\")\n",
    "builder.add_edge(\"c\", \"d\")  # b å’Œ c æ‰‡å…¥åˆ° d\n",
    "builder.add_edge(\"d\", END)\n",
    "\n",
    "graph = builder.compile()\n",
    "print(\"âœ… å›¾æ„å»ºå®Œæˆï¼\")\n",
    "\n",
    "print(\"\\n=== é€’å½’é™åˆ¶å’Œå¹¶è¡Œåˆ†æ”¯æµç¨‹çš„é™åˆ¶ ===\\n\")\n",
    "\n",
    "# æµ‹è¯• 1: æ­£å¸¸æ‰§è¡Œï¼ˆåœ¨é€’å½’é™åˆ¶å†…ï¼‰\n",
    "print(\"ğŸ“Š æµ‹è¯• 1: æ­£å¸¸æ‰§è¡Œ (recursion_limit=10)\")\n",
    "try:\n",
    "    result = graph.invoke({\"aggregate\": []}, {\"recursion_limit\": 10})\n",
    "    print(f\"âœ… æ‰§è¡ŒæˆåŠŸï¼Œæœ€ç»ˆç»“æœ: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ æ‰§è¡Œå¤±è´¥: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# æµ‹è¯• 2: è®¾ç½®è¾ƒä½çš„é€’å½’é™åˆ¶æ¥è§¦å‘ GraphRecursionError\n",
    "print(\"ğŸ“Š æµ‹è¯• 2: é€’å½’é™åˆ¶è¿‡ä½ (recursion_limit=3)\")\n",
    "try:\n",
    "    # è®¾ç½® recursion_limit=3ï¼Œå°‘äºå®Œæ•´æµç¨‹æ‰€éœ€çš„ Superstep æ•°é‡\n",
    "    result = graph.invoke({\"aggregate\": []}, {\"recursion_limit\": 3})\n",
    "    print(f\"âœ… æ‰§è¡ŒæˆåŠŸï¼Œç»“æœ: {result}\")\n",
    "except GraphRecursionError as e:\n",
    "    print(f\"âš ï¸ æ•è· GraphRecursionError å¼‚å¸¸: {e}\")\n",
    "    print(\"ğŸ“ è¯´æ˜: é€’å½’é™åˆ¶è¿‡ä½ï¼Œæ— æ³•å®Œæˆå®Œæ•´çš„å¹¶è¡Œåˆ†æ”¯æµç¨‹\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82544806",
   "metadata": {},
   "source": [
    "åœ¨è®¾è®¡åŒ…å«å¹¶è¡Œåˆ†æ”¯çš„ LangGraph æµç¨‹æ—¶ï¼Œéœ€è¦æ ¹æ®æµç¨‹çš„å¤æ‚åº¦åˆç†åœ°è®¾ç½® `recursion_limit` å‚æ•°ã€‚\n",
    "\n",
    "- å¦‚æœè®¾ç½®å¾—è¿‡ä½ï¼Œåˆ™å¯èƒ½å¯¼è‡´æµç¨‹åœ¨å¹¶è¡Œåˆ†æ”¯æ‰§è¡Œå®Œæˆå‰å°±å› ä¸ºè¾¾åˆ°é™åˆ¶è€Œä¸­æ­¢ï¼ŒæŠ›å‡º `GraphRecursionError` å¼‚å¸¸ã€‚\n",
    "- å¦‚æœè®¾ç½®å¾—è¿‡é«˜ï¼Œåˆ™å¯èƒ½æ— æ³•æœ‰æ•ˆé˜²æ­¢æµç¨‹é™·å…¥æ— é™å¾ªç¯ï¼Œé€ æˆä¸å¿…è¦çš„è®¡ç®—èµ„æºæ¶ˆè€—ã€‚\n",
    "- å»ºè®®é€šè¿‡å®éªŒå’Œè°ƒä¼˜ï¼Œæ ¹æ®æµç¨‹çš„å®é™…è¿­ä»£æ¬¡æ•°å’Œå¤æ‚åº¦é€‰æ‹©ä¸€ä¸ªåˆé€‚çš„ `recursion_limit` å€¼ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "br5s06thhnv",
   "metadata": {},
   "source": [
    "## 3.3 MapReduce æ¨¡å¼ï¼šä»»åŠ¡åˆ†è§£ä¸å¹¶è¡Œå¤„ç†\n",
    "\n",
    "åœ¨æ„å»ºå¤æ‚æ™ºèƒ½ä½“ç³»ç»Ÿæ—¶ï¼Œç»å¸¸ä¼šé‡åˆ°éœ€è¦å¤„ç†å¤§è§„æ¨¡æ•°æ®æˆ–æ‰§è¡Œè®¡ç®—å¯†é›†å‹ä»»åŠ¡çš„åœºæ™¯ã€‚ä¾‹å¦‚ï¼š\n",
    "- æ‰¹é‡å¤„ç†æµ·é‡æ–‡æ¡£è¿›è¡Œä¿¡æ¯æå–\n",
    "- å¹¶è¡Œç”Ÿæˆå¤šä¸ªåˆ›æ„æ–‡æ¡ˆ\n",
    "- åˆ†å¸ƒå¼åˆ†æç”¨æˆ·è¡Œä¸ºæ•°æ®\n",
    "\n",
    "MapReduce (æ˜ å°„-å½’çº¦) æ¨¡å¼ä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ç§é«˜æ•ˆã€å¯æ‰©å±•åœ°å¤„ç†è¿™ç±»é—®é¢˜çš„é€šç”¨è§£å†³æ–¹æ¡ˆã€‚\n",
    "\n",
    "### 3.3.1 MapReduce æ¨¡å¼çš„æ ¸å¿ƒæ€æƒ³\n",
    "\n",
    "MapReduce æ¨¡å¼çš„æ ¸å¿ƒæ€æƒ³å¯ä»¥ç”¨ä¸¤ä¸ªè¯æ¦‚æ‹¬ï¼š\"åˆ†è€Œæ²»ä¹‹\"ã€‚å®ƒå°†ä¸€ä¸ªå¤æ‚ã€å¤§è§„æ¨¡çš„è®¡ç®—ä»»åŠ¡åˆ†è§£æˆä¸¤ä¸ªç›¸äº’åä½œçš„é˜¶æ®µï¼š\n",
    "\n",
    "#### Map é˜¶æ®µ (æ˜ å°„)\n",
    "- **\"åˆ†\"çš„è¿‡ç¨‹**ï¼šå°†åŸå§‹çš„ã€å¤§è§„æ¨¡çš„è¾“å…¥æ•°æ®åˆ†å‰²æˆå¤šä¸ªç‹¬ç«‹çš„ã€è§„æ¨¡è¾ƒå°çš„å­æ•°æ®é›†\n",
    "- **å¹¶è¡Œå¤„ç†**ï¼šæ¯ä¸ªå­æ•°æ®é›†åˆ†é…ç»™ä¸åŒçš„è®¡ç®—èŠ‚ç‚¹å¹¶è¡Œå¤„ç†\n",
    "- **ç‹¬ç«‹æ‰§è¡Œ**ï¼šæ¯ä¸ªè®¡ç®—èŠ‚ç‚¹ç‹¬ç«‹åœ°å¯¹åˆ†é…çš„å­æ•°æ®é›†æ‰§è¡Œç›¸åŒçš„\"æ˜ å°„\"æ“ä½œ\n",
    "\n",
    "#### Reduce é˜¶æ®µ (å½’çº¦) \n",
    "- **\"æ²»\"çš„è¿‡ç¨‹**ï¼šå°† Map é˜¶æ®µå¹¶è¡Œç”Ÿæˆçš„å¤šä¸ªä¸­é—´ç»“æœè¿›è¡Œ\"å½’çº¦\"æ“ä½œ\n",
    "- **èšåˆæ±‡æ€»**ï¼šå°†åˆ†æ•£çš„ã€å±€éƒ¨çš„ä¸­é—´ç»“æœåˆå¹¶æˆæœ€ç»ˆçš„ã€å…¨å±€çš„ç»“æœ\n",
    "- **æ•´åˆæç‚¼**ï¼šå°† Map é˜¶æ®µçš„\"åŠæˆå“\"ç»„è£…æˆ\"æˆå“\"\n",
    "\n",
    "#### MapReduce çš„æ ¸å¿ƒä¼˜åŠ¿\n",
    "\n",
    "- **å¹¶è¡Œå¤„ç†**ï¼šå……åˆ†åˆ©ç”¨å¹¶è¡Œè®¡ç®—èµ„æºï¼Œæ˜¾è‘—æé«˜æ•°æ®å¤„ç†æ•ˆç‡\n",
    "- **é«˜æ‰©å±•æ€§**ï¼šæ˜“äºæ‰©å±•ï¼Œå¯ä»¥é€šè¿‡å¢åŠ è®¡ç®—èŠ‚ç‚¹æ¥å¤„ç†æ›´å¤§è§„æ¨¡çš„æ•°æ®\n",
    "- **ç®€åŒ–ç¼–ç¨‹æ¨¡å‹**ï¼šéšè—äº†åº•å±‚å¹¶è¡Œè®¡ç®—çš„å¤æ‚æ€§ï¼Œå¼€å‘è€…åªéœ€å…³æ³¨ä¸šåŠ¡é€»è¾‘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6752btnaevh",
   "metadata": {},
   "source": [
    "##### ç¤ºä¾‹ 3-20 ~ 3-25ï¼šLangGraph ä¸­çš„ MapReduce å®ç°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oofs1kxehq",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, List, Any\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.constants import Send\n",
    "import operator\n",
    "import re\n",
    "\n",
    "# å®šä¹‰æ•´ä½“çŠ¶æ€ç»“æ„ä½“\n",
    "class OverallState(TypedDict):\n",
    "    # åŸå§‹å¤§è§„æ¨¡è¾“å…¥æ•°æ®\n",
    "    large_input_data: List[str]\n",
    "    # åˆ†å‰²åçš„å­æ•°æ®é›†\n",
    "    sub_datasets: List[List[str]]\n",
    "    # Map é˜¶æ®µçš„å¤„ç†ç»“æœ (ä½¿ç”¨ operator.add Reducer æ”¶é›†ç»“æœ)\n",
    "    intermediate_results: Annotated[List[dict], operator.add]\n",
    "    # Reduce é˜¶æ®µçš„æœ€ç»ˆç»“æœ\n",
    "    final_result: dict\n",
    "\n",
    "# å®šä¹‰ Map èŠ‚ç‚¹çš„ç§æœ‰çŠ¶æ€ç»“æ„ä½“\n",
    "class MapState(TypedDict):\n",
    "    sub_data: Any  # å­ä»»åŠ¡æ•°æ®ç±»å‹å¯ä»¥æ˜¯ä»»æ„ç±»å‹\n",
    "\n",
    "def split_large_data(input_data: List[str], num_sub_tasks: int = 10) -> List[List[str]]:\n",
    "    \"\"\"å°†å¤§è§„æ¨¡æ•°æ®åˆ†å‰²æˆå­æ•°æ®é›†\"\"\"\n",
    "    chunk_size = max(1, len(input_data) // num_sub_tasks)\n",
    "    chunks = []\n",
    "    for i in range(0, len(input_data), chunk_size):\n",
    "        chunks.append(input_data[i:i + chunk_size])\n",
    "    return chunks\n",
    "\n",
    "def split_input_data(state: OverallState):\n",
    "    \"\"\"åˆ†å‰²èŠ‚ç‚¹å‡½æ•°ï¼šåªè´Ÿè´£æ•°æ®åˆ†å‰²ï¼Œä¸è¿”å› Send å¯¹è±¡\"\"\"\n",
    "    input_data = state[\"large_input_data\"]  # ä»çŠ¶æ€ä¸­è·å–å¤§è§„æ¨¡è¾“å…¥æ•°æ®\n",
    "    sub_datasets = split_large_data(input_data, num_sub_tasks=4)  # å°†å¤§è§„æ¨¡æ•°æ®åˆ†å‰²æˆå­æ•°æ®é›†\n",
    "\n",
    "    print(f\"ğŸ”„ åˆ†å‰²èŠ‚ç‚¹: å°† {len(input_data)} ä¸ªæ–‡æ¡£åˆ†å‰²æˆ {len(sub_datasets)} ä¸ªå­æ•°æ®é›†\")\n",
    "    for i, sub_dataset in enumerate(sub_datasets):\n",
    "        print(f\"ğŸ“¦ å­æ•°æ®é›† {i}: {len(sub_dataset)} ä¸ªæ–‡æ¡£\")\n",
    "\n",
    "    return {\"sub_datasets\": sub_datasets}\n",
    "\n",
    "def route_to_map_nodes(state: OverallState):\n",
    "    \"\"\"è·¯ç”±å‡½æ•°ï¼šæ ¹æ®åˆ†å‰²çš„æ•°æ®åˆ›å»º Send å¯¹è±¡\"\"\"\n",
    "    sub_datasets = state[\"sub_datasets\"]\n",
    "\n",
    "    print(f\"ğŸ”€ è·¯ç”±å‡½æ•°: åˆ›å»º {len(sub_datasets)} ä¸ªå¹¶è¡Œä»»åŠ¡\")\n",
    "\n",
    "    send_list = []\n",
    "    for i, sub_dataset in enumerate(sub_datasets):  # éå†æ¯ä¸ªå­æ•°æ®é›†\n",
    "        send_list.append(\n",
    "            Send(\"map_node\", {\"sub_data\": sub_dataset})  # ä¸ºæ¯ä¸ªå­æ•°æ®é›†åˆ›å»ºä¸€ä¸ª Send å¯¹è±¡\n",
    "        )\n",
    "\n",
    "    print(f\"âœ… è·¯ç”±å®Œæˆ: åˆ›å»ºäº† {len(send_list)} ä¸ª Send å¯¹è±¡\")\n",
    "    return send_list  # è¿”å› Send å¯¹è±¡åˆ—è¡¨ï¼Œç”¨äºåŠ¨æ€è·¯ç”±åˆ°å¤šä¸ª Map èŠ‚ç‚¹å®ä¾‹\n",
    "\n",
    "def process_sub_data(sub_data: List[str]) -> dict:\n",
    "    \"\"\"å¤„ç†å­ä»»åŠ¡æ•°æ®ï¼Œç”Ÿæˆä¸­é—´ç»“æœ\"\"\"\n",
    "    word_count = {}\n",
    "    total_chars = 0\n",
    "\n",
    "    for doc in sub_data:\n",
    "        # ç»Ÿè®¡è¯é¢‘\n",
    "        words = re.findall(r'\\b\\w+\\b', doc.lower())\n",
    "        for word in words:\n",
    "            word_count[word] = word_count.get(word, 0) + 1\n",
    "        # ç»Ÿè®¡å­—ç¬¦æ•°\n",
    "        total_chars += len(doc)\n",
    "\n",
    "    return {\n",
    "        \"word_count\": word_count,\n",
    "        \"doc_count\": len(sub_data),\n",
    "        \"total_chars\": total_chars,\n",
    "        \"unique_words\": len(word_count)\n",
    "    }\n",
    "\n",
    "def map_node(state: MapState):\n",
    "    \"\"\"Map èŠ‚ç‚¹å‡½æ•°ï¼Œè¾“å…¥çŠ¶æ€ä¸º MapState\"\"\"\n",
    "    sub_data = state[\"sub_data\"]  # ä»çŠ¶æ€ä¸­è·å–å­ä»»åŠ¡æ•°æ®\n",
    "    print(f\"ğŸ”§ Map èŠ‚ç‚¹: å¼€å§‹å¤„ç† {len(sub_data)} ä¸ªæ–‡æ¡£\")\n",
    "\n",
    "    intermediate_result = process_sub_data(sub_data)  # å¤„ç†å­ä»»åŠ¡æ•°æ®ï¼Œç”Ÿæˆä¸­é—´ç»“æœ\n",
    "\n",
    "    print(f\"âœ… Map èŠ‚ç‚¹: å¤„ç†å®Œæˆï¼Œæ‰¾åˆ° {intermediate_result['unique_words']}ä¸ªä¸åŒå•è¯\")\n",
    "\n",
    "    return {\"intermediate_results\": [intermediate_result]}  # è¿”å›ä¸­é—´ç»“æœï¼Œç”¨äºåç»­ Reduce é˜¶æ®µèšåˆ\n",
    "\n",
    "def aggregate_results(intermediate_results: List[dict]) -> dict:\n",
    "    \"\"\"èšåˆä¸­é—´ç»“æœï¼Œç”Ÿæˆæœ€ç»ˆç»“æœ\"\"\"\n",
    "    global_word_count = {}\n",
    "    total_docs = 0\n",
    "    total_chars = 0\n",
    "\n",
    "    for result in intermediate_results:\n",
    "        total_docs += result[\"doc_count\"]\n",
    "        total_chars += result[\"total_chars\"]\n",
    "\n",
    "        # åˆå¹¶è¯é¢‘ç»Ÿè®¡\n",
    "        for word, count in result[\"word_count\"].items():\n",
    "            global_word_count[word] = global_word_count.get(word, 0) + count\n",
    "\n",
    "    # æ‰¾å‡ºæœ€é«˜é¢‘å’Œæœ€ä½é¢‘çš„è¯\n",
    "    if global_word_count:\n",
    "        sorted_words = sorted(global_word_count.items(), key=lambda x: x[1],\n",
    "reverse=True)\n",
    "        most_common = sorted_words[0]\n",
    "        least_common = sorted_words[-1]\n",
    "    else:\n",
    "        most_common = (\"\", 0)\n",
    "        least_common = (\"\", 0)\n",
    "\n",
    "    return {\n",
    "        \"total_documents\": total_docs,\n",
    "        \"total_characters\": total_chars,\n",
    "        \"total_unique_words\": len(global_word_count),\n",
    "        \"total_words\": sum(global_word_count.values()),\n",
    "        \"most_common_word\": most_common,\n",
    "        \"least_common_word\": least_common,\n",
    "        \"word_distribution\": dict(sorted_words[:10])  # åªä¿ç•™å‰10ä¸ªé«˜é¢‘è¯\n",
    "    }\n",
    "\n",
    "def reduce_node(state: OverallState):\n",
    "    \"\"\"Reduce èŠ‚ç‚¹å‡½æ•°ï¼Œè¾“å…¥çŠ¶æ€ä¸º OverallState\"\"\"\n",
    "    intermediate_results = state[\"intermediate_results\"]  # ä»çŠ¶æ€ä¸­è·å– Map é˜¶æ®µç”Ÿæˆçš„ä¸­é—´ç»“æœåˆ—è¡¨\n",
    "\n",
    "    print(f\"ğŸ”„ Reduce èŠ‚ç‚¹: æ±‡èš {len(intermediate_results)} ä¸ªä¸­é—´ç»“æœ\")\n",
    "\n",
    "    final_result = aggregate_results(intermediate_results)  # èšåˆä¸­é—´ç»“æœï¼Œç”Ÿæˆæœ€ç»ˆç»“æœ\n",
    "\n",
    "    print(f\"âœ… Reduce å®Œæˆ: æ±‡æ€»äº† {final_result['total_documents']} ä¸ªæ–‡æ¡£\")\n",
    "\n",
    "    return {\"final_result\": final_result}  # è¿”å›æœ€ç»ˆç»“æœ\n",
    "\n",
    "# æ„å»º MapReduce å›¾\n",
    "print(\"ğŸ—ï¸ æ„å»ºæ ‡å‡† MapReduce å›¾...\")\n",
    "builder = StateGraph(OverallState)\n",
    "\n",
    "# æ·»åŠ èŠ‚ç‚¹\n",
    "builder.add_node(\"split_node\", split_input_data)\n",
    "builder.add_node(\"map_node\", map_node)\n",
    "builder.add_node(\"reduce_node\", reduce_node)\n",
    "\n",
    "# è¿æ¥ MapReduce æµç¨‹ä¸­çš„èŠ‚ç‚¹å’Œè¾¹\n",
    "builder.add_edge(START, \"split_node\")\n",
    "\n",
    "# å…³é”®ä¿®æ­£ï¼šåˆ†ç¦»æ•°æ®åˆ†å‰²å’Œä»»åŠ¡è·¯ç”±\n",
    "# åˆ†å‰²èŠ‚ç‚¹ -> Map èŠ‚ç‚¹ (æ¡ä»¶è¾¹, ä½¿ç”¨ä¸“é—¨çš„è·¯ç”±å‡½æ•°)\n",
    "builder.add_conditional_edges(\"split_node\", route_to_map_nodes, [\"map_node\"])\n",
    "\n",
    "# Map èŠ‚ç‚¹ -> Reduce èŠ‚ç‚¹ (æ™®é€šè¾¹)\n",
    "builder.add_edge(\"map_node\", \"reduce_node\")\n",
    "\n",
    "# Reduce èŠ‚ç‚¹ -> END (æ™®é€šè¾¹)\n",
    "builder.add_edge(\"reduce_node\", END)\n",
    "\n",
    "mapreduce_graph = builder.compile()\n",
    "print(\"âœ… å›¾æ„å»ºå®Œæˆï¼\")\n",
    "\n",
    "# æµ‹è¯•æ•°æ®ï¼šæ¨¡æ‹Ÿå¤§è§„æ¨¡æ–‡æ¡£æ•°æ®\n",
    "large_documents = [\n",
    "    \"LangGraph is a powerful framework for building AI agent systems with complex workflows.\",\n",
    "    \"The framework provides comprehensive state management and advanced flow control capabilities.\",\n",
    "    \"Parallel processing in LangGraph enables efficient task execution and resource utilization.\",\n",
    "    \"MapReduce pattern helps process large datasets effectively using distributed computing principles.\",\n",
    "    \"AI agents can use various tools and manage complex workflows with sophisticated coordination.\",\n",
    "    \"State management is crucial for building reliable and scalable distributed systems.\",\n",
    "    \"LangGraph supports dynamic branching with Send API for flexible workflowdesign.\",\n",
    "    \"Concurrent execution improves overall system performance and throughput significantly.\",\n",
    "    \"The Send API enables dynamic task distribution and parallel processing capabilities.\",\n",
    "    \"Reducer functions ensure safe concurrent state updates in multi-threadedenvironments.\",\n",
    "    \"Graph-based workflows provide clear visualization and better debugging capabilities.\",\n",
    "    \"Advanced error handling and retry mechanisms ensure robust system operation.\"\n",
    "]\n",
    "\n",
    "print(\"\\n=== ğŸš€ MapReduce å¤§è§„æ¨¡æ–‡æ¡£å¤„ç†æ¼”ç¤º ===\")\n",
    "print(f\"ğŸ“„ è¾“å…¥æ–‡æ¡£æ•°é‡: {len(large_documents)}\")\n",
    "print(f\"ğŸ“Š ä½¿ç”¨ Send API å®ç°åŠ¨æ€ä»»åŠ¡åˆ†å‘\")\n",
    "print(f\"ğŸ”„ MapReduce æµç¨‹: åˆ†å‰² -> å¹¶è¡Œæ˜ å°„ -> å½’çº¦\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# æ‰§è¡Œ MapReduce æµç¨‹\n",
    "result = mapreduce_graph.invoke({\n",
    "    \"large_input_data\": large_documents,\n",
    "    \"sub_datasets\": [],\n",
    "    \"intermediate_results\": [],\n",
    "    \"final_result\": {}\n",
    "})\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"\\n=== âœ¨ MapReduce å¤„ç†ç»“æœ ===\")\n",
    "final_result = result[\"final_result\"]\n",
    "print(f\"ğŸ“Š æ€»æ–‡æ¡£æ•°: {final_result['total_documents']}\")\n",
    "print(f\"ğŸ“ æ€»å­—ç¬¦æ•°: {final_result['total_characters']}\")\n",
    "print(f\"ğŸ”¤ ä¸åŒå•è¯æ•°: {final_result['total_unique_words']}\")\n",
    "print(f\"ğŸ”¢ æ€»å•è¯æ•°: {final_result['total_words']}\")\n",
    "print(f\"ğŸ† æœ€é«˜é¢‘è¯: '{final_result['most_common_word'][0]}' ({final_result['most_common_word'][1]} æ¬¡)\")\n",
    "print(f\"ğŸ¥‰ æœ€ä½é¢‘è¯: '{final_result['least_common_word'][0]}' ({final_result['least_common_word'][1]} æ¬¡)\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ é«˜é¢‘è¯æ±‡ TOP 10:\")\n",
    "for word, count in final_result['word_distribution'].items():\n",
    "    print(f\"  ğŸ“Œ {word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lust6ioew1o",
   "metadata": {},
   "source": [
    "**ğŸ’¡ MapReduce å®ç°æ ¸å¿ƒè¦ç‚¹**ï¼š\n",
    "\n",
    "#### æµç¨‹è®¾è®¡\n",
    "1. **åˆ†å‰²é˜¶æ®µ (Split)**ï¼šå°†è¾“å…¥æ–‡æ¡£æŒ‰å—å¤§å°åˆ†å‰²ï¼Œä¸ºå¹¶è¡Œå¤„ç†åšå‡†å¤‡\n",
    "2. **æ˜ å°„é˜¶æ®µ (Map)**ï¼šå¤šä¸ª Map èŠ‚ç‚¹å¹¶å‘å¤„ç†ä¸åŒçš„æ•°æ®å—ï¼Œæ‰§è¡Œè¯é¢‘ç»Ÿè®¡\n",
    "3. **å½’çº¦é˜¶æ®µ (Reduce)**ï¼šæ±‡èšæ‰€æœ‰ Map ç»“æœï¼Œç”Ÿæˆå…¨å±€ç»Ÿè®¡\n",
    "\n",
    "#### å®é™…åº”ç”¨åœºæ™¯\n",
    "- **æ–‡æ¡£åˆ†æ**ï¼šæ‰¹é‡å¤„ç†å¤§é‡æ–‡æ¡£è¿›è¡Œå†…å®¹åˆ†æ\n",
    "- **æ•°æ®æŒ–æ˜**ï¼šä»æµ·é‡æ•°æ®ä¸­æå–ç»Ÿè®¡ä¿¡æ¯\n",
    "- **å¹¶è¡Œè®¡ç®—**ï¼šä»»ä½•å¯ä»¥åˆ†å‰²å¤„ç†çš„è®¡ç®—å¯†é›†å‹ä»»åŠ¡\n",
    "\n",
    "è¿™ä¸ªä¾‹å­å±•ç¤ºäº†å¦‚ä½•å°†ä¼ ç»Ÿçš„ MapReduce æ€æƒ³ä¸ LangGraph çš„å›¾è®¡ç®—æ¨¡å‹ç›¸ç»“åˆï¼Œæ„å»ºé«˜æ•ˆçš„å¹¶è¡Œæ•°æ®å¤„ç†æµç¨‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i1p8k8um7mi",
   "metadata": {},
   "source": [
    "## 3.4 å­å›¾æœºåˆ¶ï¼šæ¨¡å—åŒ–ã€å¤ç”¨ä¸å¤æ‚æ€§ç®¡ç†\n",
    "\n",
    "åœ¨æ„å»ºæ—¥ç›Šå¤æ‚çš„ AI æ™ºèƒ½ä½“ç³»ç»Ÿæ—¶ï¼Œæ¨¡å—åŒ– (Modularity) å’Œå¤ç”¨ (Reuse) å˜å¾—è‡³å…³é‡è¦ã€‚LangGraph æ¡†æ¶é€šè¿‡å­å›¾ (Subgraph) æœºåˆ¶ï¼Œä¸ºæˆ‘ä»¬æä¾›äº†å¼ºå¤§çš„æ¨¡å—åŒ–å’Œå¤ç”¨èƒ½åŠ›ï¼Œä½¿å¾—æˆ‘ä»¬å¯ä»¥åƒæ­ç§¯æœ¨ä¸€æ ·æ„å»ºå¤æ‚çš„æ™ºèƒ½ä½“ç³»ç»Ÿã€‚\n",
    "\n",
    "### 3.4.1 å­å›¾ (Subgraph) çš„æ¦‚å¿µä¸ä¼˜åŠ¿\n",
    "\n",
    "å­å›¾ (Subgraph) åœ¨ LangGraph ä¸­ï¼ŒæŒ‡çš„æ˜¯ä¸€ä¸ª\"åµŒå¥—\"åœ¨å¦ä¸€ä¸ª LangGraph å›¾ (çˆ¶å›¾ï¼ŒParent Graph) å†…éƒ¨çš„å›¾ç»“æ„ã€‚å­å›¾æœ¬è´¨ä¸Šä»ç„¶æ˜¯ä¸€ä¸ª LangGraph å›¾ï¼Œä½†å®ƒè¢«\"å°è£…\"åœ¨çˆ¶å›¾å†…éƒ¨ï¼Œä½œä¸ºçˆ¶å›¾çš„ä¸€ä¸ªç»„æˆéƒ¨åˆ†ã€‚\n",
    "\n",
    "#### å­å›¾çš„æ ¸å¿ƒä¼˜åŠ¿ï¼š\n",
    "\n",
    "- **æ¨¡å—åŒ–**ï¼šå°†å¤æ‚ç³»ç»Ÿåˆ†è§£æˆå¤šä¸ªç‹¬ç«‹çš„ã€èŒè´£å•ä¸€çš„å­å›¾æ¨¡å—\n",
    "- **å¤ç”¨æ€§**ï¼šå­å›¾ä½œä¸ºç‹¬ç«‹æ¨¡å—ï¼Œå¯ä»¥åœ¨ä¸åŒçš„çˆ¶å›¾ä¸­é‡å¤ä½¿ç”¨  \n",
    "- **çŠ¶æ€éš”ç¦»**ï¼šå­å›¾æ‹¥æœ‰ç‹¬ç«‹çš„çŠ¶æ€ç»“æ„ä½“å’ŒçŠ¶æ€ç©ºé—´ï¼Œä¸çˆ¶å›¾çŠ¶æ€ç›¸äº’éš”ç¦»\n",
    "- **å‘½åç©ºé—´ç®¡ç†**ï¼šé¿å…ä¸åŒå­å›¾æˆ–çˆ¶å›¾ä¸­èŠ‚ç‚¹åç§°çš„å†²çª\n",
    "- **ç®€åŒ–å¤æ‚æ€§**ï¼šé€šè¿‡å±‚å±‚åµŒå¥—å’Œæ¨¡å—åŒ–ç»„åˆï¼Œæœ‰æ•ˆæ§åˆ¶ç³»ç»Ÿå¤æ‚æ€§"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nkn4cj8ozc",
   "metadata": {},
   "source": [
    "### 3.4.2 LangGraph ä¸­å®šä¹‰å’Œä½¿ç”¨å­å›¾\n",
    "\n",
    "åœ¨ LangGraph ä¸­ï¼Œå®šä¹‰å’Œä½¿ç”¨å­å›¾ä¸»è¦æœ‰ä¸¤ç§æ–¹å¼ï¼š\n",
    "\n",
    "#### æ–¹å¼ 1ï¼šå°†å·²ç¼–è¯‘çš„å­å›¾ä½œä¸ºèŠ‚ç‚¹æ·»åŠ åˆ°çˆ¶å›¾\n",
    "\n",
    "è¿™æ˜¯æœ€ç®€å•ã€æœ€ç›´æ¥çš„å­å›¾ä½¿ç”¨æ–¹å¼ã€‚å¦‚æœçˆ¶å›¾å’Œå­å›¾ä¹‹é—´éœ€è¦å…±äº«çŠ¶æ€é”®ï¼ˆä¾‹å¦‚ï¼Œå…±äº« `messages` å¯¹è¯å†å²çŠ¶æ€é”®ï¼‰ï¼Œå¯ä»¥ç›´æ¥å°†å·²ç¼–è¯‘çš„å­å›¾ä½œä¸ºç‰¹æ®ŠèŠ‚ç‚¹æ·»åŠ åˆ°çˆ¶å›¾ä¸­ã€‚\n",
    "\n",
    "#### æ–¹å¼ 2ï¼šä½¿ç”¨èŠ‚ç‚¹å‡½æ•°è°ƒç”¨å­å›¾ï¼Œå¹¶è¿›è¡ŒçŠ¶æ€è½¬æ¢\n",
    "\n",
    "å¯¹äºæ›´å¤æ‚çš„ç³»ç»Ÿï¼Œçˆ¶å›¾å’Œå­å›¾çš„çŠ¶æ€ç»“æ„ä½“å¯èƒ½å®Œå…¨ä¸åŒã€‚è¿™æ—¶éœ€è¦åˆ›å»ºèŠ‚ç‚¹å‡½æ•°ï¼Œåœ¨å‡½æ•°å†…éƒ¨è°ƒç”¨å­å›¾ï¼Œå¹¶è¿›è¡ŒçŠ¶æ€è½¬æ¢ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bmmrjjz7yyc",
   "metadata": {},
   "source": [
    "##### ç¤ºä¾‹ 3-26ï¼šå­å›¾æ¨¡å—åŒ–è®¾è®¡ç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qfuf04x3rmc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# ===== å®šä¹‰å­å›¾ 1ï¼šæ•°æ®å¤„ç†å­å›¾ =====\n",
    "class DataProcessingState(TypedDict):\n",
    "    input_data: str\n",
    "    processed_data: str\n",
    "    processing_steps: list\n",
    "\n",
    "def validate_data(state: DataProcessingState):\n",
    "    \"\"\"æ•°æ®éªŒè¯èŠ‚ç‚¹\"\"\"\n",
    "    input_data = state[\"input_data\"]\n",
    "    is_valid = len(input_data.strip()) > 0\n",
    "    return {\n",
    "        \"processing_steps\": [f\"Validation: {'Passed' if is_valid else 'Failed'}\"],\n",
    "        \"processed_data\": input_data if is_valid else \"\"\n",
    "    }\n",
    "\n",
    "def clean_data(state: DataProcessingState):\n",
    "    \"\"\"æ•°æ®æ¸…æ´—èŠ‚ç‚¹\"\"\"\n",
    "    data = state[\"processed_data\"]\n",
    "    cleaned_data = data.strip().lower().replace(\",\", \"\")\n",
    "    return {\n",
    "        \"processing_steps\": [f\"Cleaning: Removed whitespace and special chars\"],\n",
    "        \"processed_data\": cleaned_data\n",
    "    }\n",
    "\n",
    "def transform_data(state: DataProcessingState):\n",
    "    \"\"\"æ•°æ®è½¬æ¢èŠ‚ç‚¹\"\"\"\n",
    "    data = state[\"processed_data\"]\n",
    "    transformed_data = f\"PROCESSED: {data.upper()}\"\n",
    "    return {\n",
    "        \"processing_steps\": [f\"Transform: Converted to uppercase with prefix\"],\n",
    "        \"processed_data\": transformed_data\n",
    "    }\n",
    "\n",
    "# æ„å»ºæ•°æ®å¤„ç†å­å›¾\n",
    "data_processing_builder = StateGraph(DataProcessingState)\n",
    "data_processing_builder.add_node(\"validate\", validate_data)\n",
    "data_processing_builder.add_node(\"clean\", clean_data)  \n",
    "data_processing_builder.add_node(\"transform\", transform_data)\n",
    "\n",
    "data_processing_builder.add_edge(START, \"validate\")\n",
    "data_processing_builder.add_edge(\"validate\", \"clean\")\n",
    "data_processing_builder.add_edge(\"clean\", \"transform\")\n",
    "data_processing_builder.add_edge(\"transform\", END)\n",
    "\n",
    "# ç¼–è¯‘æ•°æ®å¤„ç†å­å›¾\n",
    "data_processing_subgraph = data_processing_builder.compile()\n",
    "\n",
    "print(\"æ•°æ®å¤„ç†å­å›¾æ„å»ºå®Œæˆ\")\n",
    "print(\"æµç¨‹: START -> validate -> clean -> transform -> END\")\n",
    "\n",
    "# ===== å®šä¹‰å­å›¾ 2ï¼šæŠ¥å‘Šç”Ÿæˆå­å›¾ =====\n",
    "class ReportState(TypedDict):\n",
    "    processed_data: str\n",
    "    report_title: str\n",
    "    final_report: str\n",
    "\n",
    "def generate_title(state: ReportState):\n",
    "    \"\"\"ç”ŸæˆæŠ¥å‘Šæ ‡é¢˜\"\"\"\n",
    "    processed_data = state[\"processed_data\"]\n",
    "    title = f\"Data Analysis Report: {processed_data[:20]}...\"\n",
    "    return {\"report_title\": title}\n",
    "\n",
    "def create_report(state: ReportState):\n",
    "    \"\"\"åˆ›å»ºå®Œæ•´æŠ¥å‘Š\"\"\"\n",
    "    title = state[\"report_title\"]  \n",
    "    data = state[\"processed_data\"]\n",
    "    report = f\"\"\"{title}\n",
    "    \n",
    "Data Content: {data}\n",
    "Report Generated: Successfully processed data\n",
    "Status: Complete\n",
    "\"\"\"\n",
    "    return {\"final_report\": report}\n",
    "\n",
    "# æ„å»ºæŠ¥å‘Šç”Ÿæˆå­å›¾  \n",
    "report_builder = StateGraph(ReportState)\n",
    "report_builder.add_node(\"generate_title\", generate_title)\n",
    "report_builder.add_node(\"create_report\", create_report)\n",
    "\n",
    "report_builder.add_edge(START, \"generate_title\")\n",
    "report_builder.add_edge(\"generate_title\", \"create_report\") \n",
    "report_builder.add_edge(\"create_report\", END)\n",
    "\n",
    "# ç¼–è¯‘æŠ¥å‘Šç”Ÿæˆå­å›¾\n",
    "report_subgraph = report_builder.compile()\n",
    "\n",
    "print(\"æŠ¥å‘Šç”Ÿæˆå­å›¾æ„å»ºå®Œæˆ\")\n",
    "print(\"æµç¨‹: START -> generate_title -> create_report -> END\")\n",
    "\n",
    "# ===== å®šä¹‰çˆ¶å›¾ï¼šæ•´åˆä¸¤ä¸ªå­å›¾ =====\n",
    "class MainWorkflowState(TypedDict):\n",
    "    raw_input: str\n",
    "    workflow_status: str\n",
    "    final_output: str\n",
    "\n",
    "def call_data_processing(state: MainWorkflowState):\n",
    "    \"\"\"è°ƒç”¨æ•°æ®å¤„ç†å­å›¾çš„èŠ‚ç‚¹å‡½æ•°\"\"\"\n",
    "    # çŠ¶æ€è½¬æ¢ï¼šçˆ¶å›¾çŠ¶æ€ -> å­å›¾çŠ¶æ€\n",
    "    subgraph_input = {\n",
    "        \"input_data\": state[\"raw_input\"],\n",
    "        \"processed_data\": \"\",\n",
    "        \"processing_steps\": []\n",
    "    }\n",
    "    \n",
    "    # è°ƒç”¨æ•°æ®å¤„ç†å­å›¾\n",
    "    subgraph_output = data_processing_subgraph.invoke(subgraph_input)\n",
    "    \n",
    "    # çŠ¶æ€è½¬æ¢ï¼šå­å›¾çŠ¶æ€ -> çˆ¶å›¾çŠ¶æ€  \n",
    "    return {\n",
    "        \"workflow_status\": \"Data processing completed\",\n",
    "        \"final_output\": subgraph_output[\"processed_data\"]\n",
    "    }\n",
    "\n",
    "def call_report_generation(state: MainWorkflowState):\n",
    "    \"\"\"è°ƒç”¨æŠ¥å‘Šç”Ÿæˆå­å›¾çš„èŠ‚ç‚¹å‡½æ•°\"\"\"\n",
    "    # çŠ¶æ€è½¬æ¢ï¼šçˆ¶å›¾çŠ¶æ€ -> å­å›¾çŠ¶æ€\n",
    "    subgraph_input = {\n",
    "        \"processed_data\": state[\"final_output\"],\n",
    "        \"report_title\": \"\",\n",
    "        \"final_report\": \"\"\n",
    "    }\n",
    "    \n",
    "    # è°ƒç”¨æŠ¥å‘Šç”Ÿæˆå­å›¾\n",
    "    subgraph_output = report_subgraph.invoke(subgraph_input)\n",
    "    \n",
    "    # çŠ¶æ€è½¬æ¢ï¼šå­å›¾çŠ¶æ€ -> çˆ¶å›¾çŠ¶æ€\n",
    "    return {\n",
    "        \"workflow_status\": \"Report generation completed\", \n",
    "        \"final_output\": subgraph_output[\"final_report\"]\n",
    "    }\n",
    "\n",
    "# æ„å»ºä¸»å·¥ä½œæµçˆ¶å›¾\n",
    "main_builder = StateGraph(MainWorkflowState)\n",
    "main_builder.add_node(\"data_processing\", call_data_processing)\n",
    "main_builder.add_node(\"report_generation\", call_report_generation)\n",
    "\n",
    "main_builder.add_edge(START, \"data_processing\")\n",
    "main_builder.add_edge(\"data_processing\", \"report_generation\")  \n",
    "main_builder.add_edge(\"report_generation\", END)\n",
    "\n",
    "# ç¼–è¯‘ä¸»å·¥ä½œæµ\n",
    "main_workflow = main_builder.compile()\n",
    "\n",
    "print(\"ä¸»å·¥ä½œæµæ„å»ºå®Œæˆ\")\n",
    "print(\"æµç¨‹: START -> data_processing(å­å›¾1) -> report_generation(å­å›¾2) -> END\")\n",
    "\n",
    "# ===== æµ‹è¯•å­å›¾æ¨¡å—åŒ–ç³»ç»Ÿ =====\n",
    "print(\"\\n=== æµ‹è¯•å­å›¾æ¨¡å—åŒ–ç³»ç»Ÿ ===\")\n",
    "test_input = \"Hello, World! This is sample data for processing.\"\n",
    "\n",
    "result = main_workflow.invoke({\n",
    "    \"raw_input\": test_input,\n",
    "    \"workflow_status\": \"\",\n",
    "    \"final_output\": \"\"\n",
    "})\n",
    "\n",
    "print(f\"\\nè¾“å…¥æ•°æ®: {test_input}\")\n",
    "print(f\"å·¥ä½œæµçŠ¶æ€: {result['workflow_status']}\")\n",
    "print(\"\\næœ€ç»ˆæŠ¥å‘Š:\")\n",
    "print(result[\"final_output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80edb534",
   "metadata": {},
   "source": [
    "##### ç¤ºä¾‹ 3-27ï¼šä½¿ç”¨èŠ‚ç‚¹å‡½æ•°è°ƒç”¨å­å›¾å¹¶è¿›è¡ŒçŠ¶æ€è½¬æ¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925efec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# æ¨¡æ‹Ÿå­å›¾ child_graph çš„å®šä¹‰å’Œç¼–è¯‘ï¼ˆè¿™é‡Œç”¨ç®€åŒ–ç‰ˆæœ¬ï¼‰\n",
    "class ChildState(TypedDict):\n",
    "    my_child_key: str\n",
    "\n",
    "def child_node_1(state: ChildState):\n",
    "    print(f\"ğŸ”§ å­å›¾èŠ‚ç‚¹1: å¤„ç†è¾“å…¥ '{state['my_child_key']}'\")\n",
    "    return {\"my_child_key\": f\"å­å›¾å¤„ç†: {state['my_child_key']}\"}\n",
    "\n",
    "def child_node_2(state: ChildState):\n",
    "    print(f\"ğŸ”§ å­å›¾èŠ‚ç‚¹2: è¿›ä¸€æ­¥å¤„ç† '{state['my_child_key']}'\")\n",
    "    return {\"my_child_key\": f\"{state['my_child_key']} -> å®Œæˆå¤„ç†\"}\n",
    "\n",
    "# æ„å»ºå­å›¾\n",
    "print(\"ğŸ—ï¸ æ„å»ºå­å›¾...\")\n",
    "child_builder = StateGraph(ChildState)\n",
    "child_builder.add_node(\"child_1\", child_node_1)\n",
    "child_builder.add_node(\"child_2\", child_node_2)\n",
    "child_builder.add_edge(START, \"child_1\")\n",
    "child_builder.add_edge(\"child_1\", \"child_2\")\n",
    "child_builder.add_edge(\"child_2\", END)\n",
    "\n",
    "child_graph = child_builder.compile()\n",
    "print(\"âœ… å­å›¾ç¼–è¯‘å®Œæˆ\")\n",
    "\n",
    "# å®šä¹‰çˆ¶å›¾\n",
    "class ParentState(TypedDict):  # çˆ¶å›¾çš„çŠ¶æ€ç»“æ„ä½“ \n",
    "    my_key: str  # çˆ¶å›¾çŠ¶æ€é”®: my_key\n",
    "\n",
    "def call_child_graph(state: ParentState) -> ParentState:  # èŠ‚ç‚¹å‡½æ•°ï¼Œè¾“å…¥è¾“å‡ºçŠ¶æ€ç±»å‹éƒ½ä¸º ParentState\n",
    "    print(f\"ğŸ”„ è°ƒç”¨å­å›¾èŠ‚ç‚¹: æ¥æ”¶çˆ¶å›¾çŠ¶æ€ '{state['my_key']}'\")\n",
    "\n",
    "    # çŠ¶æ€è½¬æ¢: çˆ¶å›¾çŠ¶æ€ -> å­å›¾çŠ¶æ€ (ParentState.my_key -> ChildState.my_child_key)\n",
    "    child_graph_input = {\"my_child_key\": state[\"my_key\"]}  # å°†çˆ¶å›¾çŠ¶æ€çš„ my_key å€¼ï¼Œèµ‹å€¼ç»™å­å›¾çŠ¶æ€çš„ my_child_key é”®\n",
    "    print(f\"ğŸ”€ çŠ¶æ€è½¬æ¢: çˆ¶å›¾çŠ¶æ€ -> å­å›¾çŠ¶æ€\")\n",
    "    print(f\"   è¾“å…¥å­å›¾: {child_graph_input}\")\n",
    "\n",
    "    # è°ƒç”¨å­å›¾\n",
    "    child_graph_output = child_graph.invoke(child_graph_input)  # è°ƒç”¨å­å›¾ child_graph\n",
    "    print(f\"ğŸ“¤ å­å›¾è¾“å‡º: {child_graph_output}\")\n",
    "\n",
    "    # çŠ¶æ€è½¬æ¢: å­å›¾çŠ¶æ€ -> çˆ¶å›¾çŠ¶æ€ (ChildState.my_child_key -> ParentState.my_key)\n",
    "    result = {\"my_key\": child_graph_output[\"my_child_key\"]}  # å°†å­å›¾è¾“å‡ºçŠ¶æ€çš„ my_child_key å€¼ï¼Œèµ‹å€¼ç»™çˆ¶å›¾çŠ¶æ€çš„ my_key é”®\n",
    "    print(f\"ğŸ”€ çŠ¶æ€è½¬æ¢: å­å›¾çŠ¶æ€ -> çˆ¶å›¾çŠ¶æ€\")\n",
    "    print(f\"   è¿”å›çˆ¶å›¾: {result}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "def parent_node_1(state: ParentState):\n",
    "    print(f\"ğŸ”§ çˆ¶å›¾èŠ‚ç‚¹1: é¢„å¤„ç† '{state['my_key']}'\")\n",
    "    return {\"my_key\": f\"çˆ¶å›¾é¢„å¤„ç†: {state['my_key']}\"}\n",
    "\n",
    "def parent_node_2(state: ParentState):\n",
    "    print(f\"ğŸ”§ çˆ¶å›¾èŠ‚ç‚¹2: åå¤„ç† '{state['my_key']}'\")\n",
    "    return {\"my_key\": f\"{state['my_key']} -> çˆ¶å›¾å®Œæˆ\"}\n",
    "\n",
    "print(\"\\nğŸ—ï¸ æ„å»ºçˆ¶å›¾...\")\n",
    "builder = StateGraph(ParentState)  # åˆ›å»ºçˆ¶å›¾ StateGraphï¼ŒæŒ‡å®šçŠ¶æ€ç»“æ„ä½“ä¸º \n",
    "ParentState\n",
    "\n",
    "# çˆ¶å›¾çš„å…¶ä»–èŠ‚ç‚¹å®šä¹‰\n",
    "builder.add_node(\"parent_1\", parent_node_1)\n",
    "builder.add_node(\"parent_2\", parent_node_2)\n",
    "builder.add_node(\"child\", call_child_graph)  # å°†èŠ‚ç‚¹å‡½æ•° call_child_graph ä½œä¸ºèŠ‚ç‚¹æ·»åŠ åˆ°çˆ¶å›¾ï¼ŒèŠ‚ç‚¹åä¸º \"child\"\n",
    "\n",
    "builder.add_edge(START, \"parent_1\")  # çˆ¶å›¾èŠ‚ç‚¹ parent_1 -> èŠ‚ç‚¹å‡½æ•° call_child_graph (æ™®é€šè¾¹)\n",
    "builder.add_edge(\"parent_1\", \"child\")\n",
    "builder.add_edge(\"child\", \"parent_2\")\n",
    "builder.add_edge(\"parent_2\", END)\n",
    "\n",
    "graph = builder.compile()  # ç¼–è¯‘çˆ¶å›¾\n",
    "print(\"âœ… çˆ¶å›¾æ„å»ºå®Œæˆï¼\")\n",
    "\n",
    "# æµ‹è¯•è¿è¡Œ\n",
    "initial_state = {\"my_key\": \"ç”¨æˆ·è¾“å…¥æ•°æ®\"}\n",
    "print(f\"\\nğŸ“¥ åˆå§‹çˆ¶å›¾çŠ¶æ€: {initial_state}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "result = graph.invoke(initial_state)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nğŸ“¤ æœ€ç»ˆçˆ¶å›¾çŠ¶æ€: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22458d3e",
   "metadata": {},
   "source": [
    "ğŸ”„ æ ¸å¿ƒä»·å€¼\n",
    "\n",
    "- çµæ´»æ€§: æ”¯æŒçˆ¶å­å›¾ä½¿ç”¨å®Œå…¨ä¸åŒçš„çŠ¶æ€ç»“æ„ä½“\n",
    "- æ¨¡å—åŒ–: å®ç°å¤æ‚ç³»ç»Ÿçš„æ¨¡å—åŒ–è®¾è®¡\n",
    "- è§£è€¦åˆ: ä¸åŒç»„ä»¶å¯ä»¥ç‹¬ç«‹è®¾è®¡çŠ¶æ€ç»“æ„\n",
    "\n",
    "ğŸ¯ å…¸å‹åº”ç”¨åœºæ™¯\n",
    "\n",
    "å¤šæ™ºèƒ½ä½“RAGç³»ç»Ÿç¤ºä¾‹ï¼š\n",
    "- çˆ¶å›¾(Supervisor Agent): å…³æ³¨æœ€ç»ˆRAGæŠ¥å‘Š\n",
    "- å­å›¾(ReAct Agent): ç»´æŠ¤è¯¦ç»†å¯¹è¯å†å²å’Œå·¥å…·è°ƒç”¨è®°å½•\n",
    "\n",
    "ğŸ”€ çŠ¶æ€è½¬æ¢å››æ­¥æµç¨‹\n",
    "\n",
    "1. è¾“å…¥è½¬æ¢: çˆ¶å›¾çŠ¶æ€ â†’ å­å›¾çŠ¶æ€\n",
    "2. å­å›¾å¤„ç†: å­å›¾å†…éƒ¨æ‰§è¡Œä¸šåŠ¡é€»è¾‘\n",
    "3. è¾“å‡ºè½¬æ¢: å­å›¾çŠ¶æ€ â†’ çˆ¶å›¾çŠ¶æ€\n",
    "4. çŠ¶æ€æ›´æ–°: è¿”å›è½¬æ¢åçš„çˆ¶å›¾çŠ¶æ€\n",
    "\n",
    "âœ… é€‰æ‹©ç­–ç•¥\n",
    "\n",
    "- å…±äº«çŠ¶æ€é”® â†’ ç›´æ¥å°†å­å›¾ä½œä¸ºèŠ‚ç‚¹æ·»åŠ \n",
    "- å¼‚æ„çŠ¶æ€ç»“æ„ â†’ ä½¿ç”¨èŠ‚ç‚¹å‡½æ•°+çŠ¶æ€è½¬æ¢æ–¹å¼\n",
    "- å¤æ‚å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ â†’ å„æ™ºèƒ½ä½“ç‹¬ç«‹çŠ¶æ€ç»“æ„ä½“\n",
    "\n",
    "å…³é”®ä¼˜åŠ¿: å®ç°äº†çŠ¶æ€ç»“æ„çš„å®Œå…¨è§£è€¦ï¼Œæ”¯æŒå¤æ‚ç³»ç»Ÿçš„åˆ†å±‚è®¾è®¡å’Œæ¨¡å—åŒ–å¼€å‘ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rmkyk31y4e",
   "metadata": {},
   "source": [
    "**ğŸ’¡ å­å›¾æœºåˆ¶æ ¸å¿ƒä»·å€¼**ï¼š\n",
    "\n",
    "#### æ¨¡å—åŒ–è®¾è®¡\n",
    "- **æ•°æ®å¤„ç†å­å›¾**ï¼šå°è£…äº†æ•°æ®éªŒè¯ã€æ¸…æ´—ã€è½¬æ¢çš„å®Œæ•´æµç¨‹ï¼Œå¯ä»¥åœ¨ä¸åŒåœºæ™¯ä¸­å¤ç”¨\n",
    "- **æŠ¥å‘Šç”Ÿæˆå­å›¾**ï¼šä¸“é—¨è´Ÿè´£æŠ¥å‘Šç”Ÿæˆé€»è¾‘ï¼Œç‹¬ç«‹ç»´æŠ¤å’Œæµ‹è¯•\n",
    "- **ä¸»å·¥ä½œæµ**ï¼šé€šè¿‡ç»„åˆä¸åŒå­å›¾ï¼Œæ„å»ºå®Œæ•´çš„æ•°æ®å¤„ç†ç®¡é“\n",
    "\n",
    "#### çŠ¶æ€éš”ç¦»ä¸è½¬æ¢\n",
    "- æ¯ä¸ªå­å›¾ç»´æŠ¤è‡ªå·±çš„çŠ¶æ€ç»“æ„\n",
    "- é€šè¿‡èŠ‚ç‚¹å‡½æ•°è¿›è¡ŒçŠ¶æ€è½¬æ¢ï¼Œå®ç°çˆ¶å›¾ä¸å­å›¾ä¹‹é—´çš„æ•°æ®é€‚é…\n",
    "- çŠ¶æ€éš”ç¦»é¿å…äº†æ¨¡å—é—´çš„ç›¸äº’å¹²æ‰°\n",
    "\n",
    "#### å¤ç”¨ä¸æ‰©å±•\n",
    "- å­å›¾å¯ä»¥åœ¨ä¸åŒçš„çˆ¶å›¾ä¸­å¤ç”¨ï¼Œæé«˜å¼€å‘æ•ˆç‡\n",
    "- æ–°çš„å¤„ç†æ­¥éª¤å¯ä»¥é€šè¿‡æ·»åŠ æ–°å­å›¾æˆ–ä¿®æ”¹ç°æœ‰å­å›¾æ¥å®ç°\n",
    "- æ¨¡å—åŒ–è®¾è®¡ä½¿ç³»ç»Ÿæ˜“äºç»´æŠ¤å’Œæ‰©å±•"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oqdf6x1e7k",
   "metadata": {},
   "source": [
    "## 3.5 å·¥å…·è°ƒç”¨ï¼šæ‰©å±•æ™ºèƒ½ä½“çš„èƒ½åŠ›è¾¹ç•Œ\n",
    "\n",
    "åœ¨æ„å»ºæ™ºèƒ½ä½“ç³»ç»Ÿçš„è¿‡ç¨‹ä¸­ï¼Œå·¥å…·è°ƒç”¨ (Tool Calling) æ˜¯ä¸€é¡¹è‡³å…³é‡è¦çš„èƒ½åŠ›ã€‚æ™ºèƒ½ä½“é€šè¿‡è°ƒç”¨å„ç§å¤–éƒ¨å·¥å…·ï¼Œå¯ä»¥æ‰©å±•è‡ªèº«çš„èƒ½åŠ›è¾¹ç•Œï¼Œå®Œæˆæ›´å¤æ‚ã€æ›´å®ç”¨çš„ä»»åŠ¡ã€‚\n",
    "\n",
    "LangGraph æ¡†æ¶æä¾›äº†å¼ºå¤§çš„å·¥å…·è°ƒç”¨æ”¯æŒï¼Œå¹¶é¢„ç½®äº† `ToolNode` ç»„ä»¶ï¼Œæå¤§åœ°ç®€åŒ–äº†åœ¨ LangGraph å›¾ä¸­é›†æˆå’Œä½¿ç”¨å·¥å…·çš„æµç¨‹ã€‚\n",
    "\n",
    "### 3.5.1 ToolNodeï¼šLangGraph çš„å·¥å…·è°ƒç”¨ä¸­å¿ƒ\n",
    "\n",
    "`ToolNode` æ˜¯ LangGraph æ¡†æ¶é¢„ç½®çš„æ ¸å¿ƒç»„ä»¶ï¼Œä¸“é—¨ç”¨äºå¤„ç†å·¥å…·è°ƒç”¨æ“ä½œã€‚å¯ä»¥å°†å…¶ç†è§£ä¸º LangGraph å›¾ä¸­çš„\"å·¥å…·æ‰§è¡Œå™¨\"æˆ–\"å·¥å…·è°ƒåº¦ä¸­å¿ƒ\"ã€‚\n",
    "\n",
    "#### ToolNode çš„æ ¸å¿ƒèŒè´£ï¼š\n",
    "\n",
    "1. **æ¥æ”¶æ¥è‡ª LLM çš„å·¥å…·è°ƒç”¨è¯·æ±‚**ï¼šä¸æ”¯æŒå·¥å…·è°ƒç”¨çš„ LLM æ¨¡å‹é›†æˆï¼Œæ¥æ”¶å·¥å…·è°ƒç”¨è¯·æ±‚\n",
    "2. **æ‰§è¡Œå·¥å…·è°ƒç”¨**ï¼šåŠ¨æ€è°ƒåº¦å’Œè°ƒç”¨é¢„å…ˆæ³¨å†Œçš„å·¥å…·ï¼Œå¹¶è¿”å›æ‰§è¡Œç»“æœ  \n",
    "3. **æ›´æ–°å›¾çŠ¶æ€**ï¼šå°†å·¥å…·æ‰§è¡Œç»“æœå°è£…æˆ `ToolMessage` å¹¶æ·»åŠ åˆ°å›¾çŠ¶æ€ä¸­\n",
    "\n",
    "#### ä½¿ç”¨ ToolNode çš„ä¼˜åŠ¿ï¼š\n",
    "\n",
    "- **ç®€åŒ–å·¥å…·é›†æˆ**ï¼šå°è£…å·¥å…·è°ƒç”¨å¤æ‚æ€§ï¼Œå¼€å‘è€…åªéœ€ç®€å•æ³¨å†Œå·¥å…·åˆ—è¡¨\n",
    "- **åŸç”Ÿæ”¯æŒ LangChain å·¥å…·**ï¼šä¸ LangChain å·¥å…·ä½“ç³»æ— ç¼é›†æˆ\n",
    "- **å†…ç½®é”™è¯¯å¤„ç†**ï¼šè‡ªåŠ¨æ•è·å·¥å…·æ‰§è¡Œå¼‚å¸¸ï¼Œæé«˜ç³»ç»Ÿé²æ£’æ€§\n",
    "- **å…¼å®¹ ReAct Agent**ï¼šä¸ LangGraph é¢„ç½®çš„ ReAct ç»„ä»¶é«˜åº¦å…¼å®¹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m2y219n8kbj",
   "metadata": {},
   "source": [
    "##### ç¤ºä¾‹ 3-28 & 3-29ï¼šä½¿ç”¨ ToolNode æ„å»ºå·¥å…·è°ƒç”¨æ™ºèƒ½ä½“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12394k6achoa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import AIMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "@tool # ä½¿ç”¨ @tool è£…é¥°å™¨ï¼Œå°† Python å‡½æ•°è½¬æ¢ä¸º LangChain Tool\n",
    "def get_weather(location: str): # å®šä¹‰å·¥å…·å‡½æ•° get_weather, location å‚æ•°ç”¨äºæ¥æ”¶åŸå¸‚åç§°\n",
    "    \"\"\"Call to get the current weather.\"\"\" # å·¥å…·å‡½æ•°çš„ docstring ä¼šè¢«ä½œä¸ºå·¥å…·çš„æè¿°ä¿¡æ¯ï¼Œæä¾›ç»™ LLM\n",
    "    if location.lower() in [\"sf\", \"san francisco\"]:\n",
    "        return \"It's 60 degrees and foggy.\"\n",
    "    else:\n",
    "        return \"It's 90 degrees and sunny.\"\n",
    "\n",
    "@tool\n",
    "def get_coolest_cities():\n",
    "    \"\"\"Get a list of coolest cities\"\"\"\n",
    "    return \"nyc, sf\"\n",
    "\n",
    "\n",
    "\n",
    "# åˆ›å»º ToolNode å®ä¾‹ï¼Œæ³¨å†Œå·¥å…·åˆ—è¡¨ (åŒ…å« get_weather å’Œ get_coolest_cities ä¸¤ä¸ªå·¥å…·)\n",
    "tools = [get_weather, get_coolest_cities]\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "# æ„é€ åŒ…å«å•ä¸ªå·¥å…·è°ƒç”¨è¯·æ±‚çš„ AIMessage\n",
    "message_with_single_tool_call = AIMessage( # åˆ›å»º AIMessage\n",
    "    content=\"\", # content ä¸ºç©ºå­—ç¬¦ä¸²ï¼Œè¡¨ç¤ºè¯¥æ¶ˆæ¯ä¸»è¦ç”¨äºå·¥å…·è°ƒç”¨ï¼Œä¸åŒ…å«æ–‡æœ¬å†…å®¹\n",
    "    tool_calls=[ # tool_calls å‚æ•°ï¼ŒåŒ…å«å·¥å…·è°ƒç”¨è¯·æ±‚åˆ—è¡¨\n",
    "        {\n",
    "            \"name\": \"get_weather\", #  å·¥å…·åç§°ï¼Œå¿…é¡»ä¸æ³¨å†Œçš„å·¥å…·åç§°ä¸€è‡´\n",
    "            \"args\": {\"location\": \"sf\"}, #  å·¥å…·å‚æ•°ï¼Œå¿…é¡»ä¸å·¥å…·å‡½æ•°å®šä¹‰çš„å‚æ•°åŒ¹é…\n",
    "            \"id\": \"tool_call_id\", #  å·¥å…·è°ƒç”¨ IDï¼Œç”¨äºå”¯ä¸€æ ‡è¯†å·¥å…·è°ƒç”¨ï¼Œ  å¯ä»¥è‡ªå®šä¹‰\n",
    "            \"type\": \"tool_call\", #  æ¶ˆæ¯ç±»å‹ï¼Œå›ºå®šä¸º \"tool_call\"\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "# æ‰‹åŠ¨è°ƒç”¨ ToolNodeï¼Œè¾“å…¥ä¸ºåŒ…å« AIMessage çš„çŠ¶æ€å­—å…¸\n",
    "tool_node_output = tool_node.invoke({\"messages\": [message_with_single_tool_call]})\n",
    "\n",
    "print(tool_node_output) # æ‰“å° ToolNode çš„è¾“å‡ºç»“æœ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27e1dce",
   "metadata": {},
   "source": [
    "##### ç¤ºä¾‹ 3-30ï¼šæ‰‹åŠ¨è°ƒç”¨ ToolNodeï¼ˆå¹¶è¡Œå·¥å…·è°ƒç”¨ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7384c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ„é€ åŒ…å«å¤šä¸ªå·¥å…·è°ƒç”¨è¯·æ±‚çš„ AIMessage\n",
    "message_with_multiple_tool_calls = AIMessage( # åˆ›å»º AIMessage\n",
    "    content=\"\",\n",
    "    tool_calls=[ # tool_calls å‚æ•°ï¼ŒåŒ…å«å¤šä¸ªå·¥å…·è°ƒç”¨è¯·æ±‚\n",
    "        {\n",
    "            \"name\": \"get_coolest_cities\", # å·¥å…· 1ï¼šget_coolest_cities\n",
    "            \"args\": {},\n",
    "            \"id\": \"tool_call_id_1\",\n",
    "            \"type\": \"tool_call\",\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"get_weather\", # å·¥å…· 2ï¼šget_weather\n",
    "            \"args\": {\"location\": \"sf\"},\n",
    "            \"id\": \"tool_call_id_2\",\n",
    "            \"type\": \"tool_call\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "# æ‰‹åŠ¨è°ƒç”¨ ToolNode,  è¾“å…¥ä¸ºåŒ…å« AIMessage çš„çŠ¶æ€å­—å…¸\n",
    "tool_node_output = tool_node.invoke({\"messages\": [message_with_multiple_tool_calls]})\n",
    "\n",
    "print(tool_node_output) # æ‰“å° ToolNode çš„è¾“å‡ºç»“æœ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c18ae78",
   "metadata": {},
   "source": [
    "##### ç¤ºä¾‹ 3-31ï¼šåœ¨ LangGraph å›¾ä¸­ä½¿ç”¨ ToolNode æ„å»º ReAct æ™ºèƒ½ä½“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04a94d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, MessagesState, START, END # å¯¼å…¥ MessagesState\n",
    "from langgraph.prebuilt import ToolNode # å¯¼å…¥ ToolNode\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ... (get_weather, get_coolest_cities å·¥å…·å‡½æ•°çš„å®šä¹‰ï¼Œæ­¤å¤„çœç•¥) ...\n",
    "\n",
    "tools = [get_weather, get_coolest_cities] #  å·¥å…·åˆ—è¡¨\n",
    "\n",
    "tool_node = ToolNode(tools) # åˆ›å»º ToolNode å®ä¾‹ï¼Œæ³¨å†Œå·¥å…·åˆ—è¡¨\n",
    "\n",
    "model_with_tools = ChatOpenAI(model=\"Qwen/Qwen3-8B\", temperature=0).bind_tools(tools) # ç»‘å®šå·¥å…·åˆ—è¡¨åˆ° LLM æ¨¡å‹\n",
    "\n",
    "def should_continue(state: MessagesState): # æ¡ä»¶è·¯ç”±å‡½æ•°ï¼Œåˆ¤æ–­æ˜¯å¦ç»§ç»­å·¥å…·è°ƒç”¨\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1] # è·å–æœ€åä¸€ä¸ªæ¶ˆæ¯ (LLM æ¨¡å‹çš„è¾“å‡º)\n",
    "    if last_message.tool_calls: # åˆ¤æ–­æœ€åä¸€ä¸ªæ¶ˆæ¯æ˜¯å¦åŒ…å« tool_calls (å·¥å…·è°ƒç”¨è¯·æ±‚)\n",
    "        return \"tools\" # å¦‚æœåŒ…å« tool_calls, åˆ™è·¯ç”±åˆ° \"tools\" èŠ‚ç‚¹ (ToolNode)ï¼Œæ‰§è¡Œå·¥å…·è°ƒç”¨\n",
    "    return END # å¦‚æœä¸åŒ…å« tool_calls,  åˆ™è·¯ç”±åˆ° END èŠ‚ç‚¹ï¼Œç»“æŸæµç¨‹\n",
    "\n",
    "def call_model(state: MessagesState): #  LLM æ¨¡å‹èŠ‚ç‚¹å‡½æ•°\n",
    "    messages = state[\"messages\"]\n",
    "    response = model_with_tools.invoke(messages) # è°ƒç”¨ LLM æ¨¡å‹ï¼Œç”Ÿæˆ AI æ¶ˆæ¯ (å¯èƒ½åŒ…å« tool_calls)\n",
    "    return {\"messages\": [response]} # è¿”å›åŒ…å« AI æ¶ˆæ¯çš„çŠ¶æ€æ›´æ–°\n",
    "\n",
    "workflow = StateGraph(MessagesState) # åˆ›å»º StateGraph å®ä¾‹ï¼ŒçŠ¶æ€ Schema ä¸º MessagesState\n",
    "\n",
    "workflow.add_node(\"agent\", call_model) # æ·»åŠ  LLM æ¨¡å‹èŠ‚ç‚¹ï¼ŒèŠ‚ç‚¹åä¸º \"agent\"\n",
    "workflow.add_node(\"tools\", tool_node) # æ·»åŠ  ToolNode èŠ‚ç‚¹ï¼ŒèŠ‚ç‚¹åä¸º \"tools\"\n",
    "\n",
    "workflow.add_edge(START, \"agent\") # å®šä¹‰ä» START èŠ‚ç‚¹åˆ° \"agent\" èŠ‚ç‚¹çš„è¾¹ (æµç¨‹å…¥å£)\n",
    "workflow.add_conditional_edges(\"agent\", should_continue, [\"tools\", END]) #  å®šä¹‰æ¡ä»¶è¾¹ï¼Œæ ¹æ® \"agent\" èŠ‚ç‚¹çš„è¾“å‡ºï¼ŒåŠ¨æ€è·¯ç”±åˆ° \"tools\" èŠ‚ç‚¹æˆ– END èŠ‚ç‚¹\n",
    "workflow.add_edge(\"tools\", \"agent\") # å®šä¹‰ä» \"tools\" èŠ‚ç‚¹åˆ° \"agent\" èŠ‚ç‚¹çš„è¾¹ (ReAct å¾ªç¯)\n",
    "\n",
    "app = workflow.compile() # ç¼–è¯‘ LangGraph å›¾\n",
    "\n",
    "app.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e21f3b",
   "metadata": {},
   "source": [
    "##### ç¤ºä¾‹ 3-32ï¼šè‡ªå®šä¹‰å·¥å…·è°ƒç”¨é”™è¯¯å¤„ç†ç­–ç•¥ ï¼ˆæ¨¡å‹é™çº§ + æ¸…ç†é”™è¯¯ä¿¡æ¯ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bee9fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Literal\n",
    "\n",
    "from langchain_core.messages import AIMessage, ToolMessage\n",
    "from langchain_core.messages.modifier import RemoveMessage\n",
    "\n",
    "from langgraph.graph import MessagesState, StateGraph, END, START\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class HaikuRequest(BaseModel):\n",
    "    topic: list[str] = Field(\n",
    "        max_length=3,\n",
    "        min_length=3,\n",
    "    )\n",
    "\n",
    "@tool\n",
    "def master_haiku_generator(request: HaikuRequest):\n",
    "    \"\"\"Generates a haiku based on the provided topics.\"\"\"\n",
    "    model = ChatOpenAI(model=\"Qwen/Qwen2-1.5B-Instruct\", temperature=0)\n",
    "    chain = model | StrOutputParser()\n",
    "    topics = \", \".join(request.topic)\n",
    "    haiku = chain.invoke(f\"Write a haiku about {topics}\")\n",
    "    return haiku\n",
    "\n",
    "def call_tool(state: MessagesState):\n",
    "    # åˆ›å»ºå·¥å…·åç§°åˆ°å·¥å…·å‡½æ•°çš„æ˜ å°„å­—å…¸\n",
    "    tools_by_name = {master_haiku_generator.name: master_haiku_generator}\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]  # è·å–æœ€åä¸€æ¡æ¶ˆæ¯\n",
    "    output_messages = []\n",
    "    # éå†æœ€åä¸€æ¡æ¶ˆæ¯ä¸­çš„æ‰€æœ‰å·¥å…·è°ƒç”¨\n",
    "    for tool_call in last_message.tool_calls:\n",
    "        try:\n",
    "            # æ ¹æ®å·¥å…·åç§°æ‰¾åˆ°å¯¹åº”çš„å·¥å…·å‡½æ•°å¹¶è°ƒç”¨ï¼Œä¼ å…¥å‚æ•°\n",
    "            tool_result = tools_by_name[tool_call[\"name\"]].invoke(tool_call[\"args\"])\n",
    "            # å°†å·¥å…·è°ƒç”¨ç»“æœå°è£…ä¸ºToolMessageæ·»åŠ åˆ°è¾“å‡ºæ¶ˆæ¯åˆ—è¡¨\n",
    "            output_messages.append(\n",
    "                ToolMessage(\n",
    "                    content=json.dumps(tool_result),\n",
    "                    name=tool_call[\"name\"],\n",
    "                    tool_call_id=tool_call[\"id\"],\n",
    "                )\n",
    "            )\n",
    "        except Exception as e:\n",
    "            # å¦‚æœå·¥å…·è°ƒç”¨å¤±è´¥ï¼Œæ•è·å¼‚å¸¸å¹¶è¿”å›é”™è¯¯ä¿¡æ¯\n",
    "            # å°†é”™è¯¯ä¿¡æ¯å°è£…ä¸ºToolMessageï¼Œå¹¶åœ¨additional_kwargsä¸­æ ‡è®°é”™è¯¯\n",
    "            output_messages.append(\n",
    "                ToolMessage(\n",
    "                    content=str(e), \n",
    "                    name=tool_call[\"name\"],\n",
    "                    tool_call_id=tool_call[\"id\"],\n",
    "                    additional_kwargs={\"error\": e},  # åœ¨é¢å¤–å‚æ•°ä¸­å­˜å‚¨é”™è¯¯å¯¹è±¡ï¼Œç”¨äºåç»­é”™è¯¯å¤„ç†\n",
    "                )\n",
    "            )\n",
    "    return {\"messages\": output_messages}\n",
    "\n",
    "# åˆå§‹åŒ–åŸºç¡€æ¨¡å‹ï¼ˆè¾ƒå¼±çš„æ¨¡å‹ï¼‰\n",
    "model = ChatOpenAI(model=\"Qwen/Qwen2-1.5B-Instruct\", temperature=0)\n",
    "model_with_tools = model.bind_tools([master_haiku_generator])\n",
    "\n",
    "# åˆå§‹åŒ–æ›´å¼ºå¤§çš„æ¨¡å‹ï¼ˆç”¨äºé™çº§ç­–ç•¥ï¼‰\n",
    "better_model = ChatOpenAI(model=\"Qwen/Qwen2.5-7B-Instruct\", temperature=0)\n",
    "better_model_with_tools = better_model.bind_tools([master_haiku_generator])\n",
    "\n",
    "def should_continue(state: MessagesState):\n",
    "    # å†³å®šæ˜¯å¦ç»§ç»­å·¥å…·è°ƒç”¨å¾ªç¯æˆ–ç»“æŸæµç¨‹\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    if last_message.tool_calls:  # å¦‚æœæœ€åä¸€æ¡æ¶ˆæ¯åŒ…å«å·¥å…·è°ƒç”¨è¯·æ±‚\n",
    "        return \"tools\"  # ç»§ç»­æ‰§è¡Œå·¥å…·è°ƒç”¨\n",
    "    return END  # å¦åˆ™ç»“æŸæµç¨‹\n",
    "\n",
    "def should_fallback(\n",
    "    state: MessagesState,\n",
    ") -> Literal[\"agent\", \"remove_failed_tool_call_attempt\"]:\n",
    "    # å†³å®šæ˜¯å¦éœ€è¦é™çº§åˆ°æ›´å¼ºå¤§çš„æ¨¡å‹\n",
    "    messages = state[\"messages\"]\n",
    "    # æŸ¥æ‰¾æ˜¯å¦æœ‰å¤±è´¥çš„å·¥å…·è°ƒç”¨æ¶ˆæ¯ï¼ˆé€šè¿‡ additional_kwargs ä¸­çš„ error æ ‡è®°è¯†åˆ«ï¼‰\n",
    "    failed_tool_messages = [\n",
    "        msg\n",
    "        for msg in messages\n",
    "        if isinstance(msg, ToolMessage)\n",
    "        and msg.additional_kwargs.get(\"error\") is not None\n",
    "    ]\n",
    "    if failed_tool_messages:  # å¦‚æœå­˜åœ¨å¤±è´¥çš„å·¥å…·è°ƒç”¨\n",
    "        return \"remove_failed_tool_call_attempt\"  # è·¯ç”±åˆ°ç§»é™¤å¤±è´¥å°è¯•çš„èŠ‚ç‚¹\n",
    "    return \"agent\"  # å¦åˆ™ç»§ç»­ä½¿ç”¨å½“å‰æ¨¡å‹\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    # ä½¿ç”¨åŸºç¡€æ¨¡å‹å¤„ç†æ¶ˆæ¯\n",
    "    messages = state[\"messages\"]\n",
    "    response = model_with_tools.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def remove_failed_tool_call_attempt(state: MessagesState):\n",
    "    # ç§»é™¤å¤±è´¥çš„å·¥å…·è°ƒç”¨å°è¯•ï¼Œæ¸…ç†æ¶ˆæ¯å†å²\n",
    "    messages = state[\"messages\"]\n",
    "    # ä»åå‘å‰æŸ¥æ‰¾æœ€è¿‘çš„AIæ¶ˆæ¯ç´¢å¼•\n",
    "    last_ai_message_index = next(\n",
    "        i\n",
    "        for i, msg in reversed(list(enumerate(messages)))\n",
    "        if isinstance(msg, AIMessage)\n",
    "    )\n",
    "    # è·å–éœ€è¦ç§»é™¤çš„æ¶ˆæ¯ï¼ˆä»æœ€è¿‘çš„AIæ¶ˆæ¯å¼€å§‹çš„æ‰€æœ‰æ¶ˆæ¯ï¼‰\n",
    "    messages_to_remove = messages[last_ai_message_index:]\n",
    "    # è¿”å›ç§»é™¤æŒ‡ä»¤ï¼Œé€šè¿‡RemoveMessageæ ‡è®°éœ€è¦ç§»é™¤çš„æ¶ˆæ¯\n",
    "    return {\"messages\": [RemoveMessage(id=m.id) for m in messages_to_remove]}\n",
    "\n",
    "# é™çº§ç­–ç•¥ï¼šä½¿ç”¨æ›´å¼ºå¤§çš„æ¨¡å‹é‡è¯•\n",
    "def call_fallback_model(state: MessagesState):\n",
    "    # ä½¿ç”¨æ›´å¼ºå¤§çš„æ¨¡å‹å¤„ç†æ¶ˆæ¯\n",
    "    messages = state[\"messages\"]\n",
    "    response = better_model_with_tools.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# åˆ›å»ºçŠ¶æ€å›¾\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# æ·»åŠ èŠ‚ç‚¹\n",
    "workflow.add_node(\"agent\", call_model)  # åŸºç¡€æ¨¡å‹èŠ‚ç‚¹\n",
    "workflow.add_node(\"tools\", call_tool)  # å·¥å…·è°ƒç”¨èŠ‚ç‚¹\n",
    "workflow.add_node(\"remove_failed_tool_call_attempt\", remove_failed_tool_call_attempt)  # æ¸…ç†å¤±è´¥å°è¯•èŠ‚ç‚¹\n",
    "workflow.add_node(\"fallback_agent\", call_fallback_model)  # é™çº§æ¨¡å‹èŠ‚ç‚¹\n",
    "\n",
    "# æ·»åŠ è¾¹å’Œæ¡ä»¶è¾¹\n",
    "workflow.add_edge(START, \"agent\")  # æµç¨‹ä»agentèŠ‚ç‚¹å¼€å§‹\n",
    "workflow.add_conditional_edges(\"agent\", should_continue, [\"tools\", END])  # æ ¹æ®should_continueå‡½æ•°å†³å®šæ˜¯ç»§ç»­å·¥å…·è°ƒç”¨è¿˜æ˜¯ç»“æŸ\n",
    "# æ ¹æ®å·¥å…·è°ƒç”¨ç»“æœå†³å®šæ˜¯ç»§ç»­ä½¿ç”¨å½“å‰æ¨¡å‹è¿˜æ˜¯æ¸…ç†å¤±è´¥å°è¯•\n",
    "workflow.add_conditional_edges(\"tools\", should_fallback, path_map = {\"agent\": \"agent\", \"remove_failed_tool_call_attempt\": \"remove_failed_tool_call_attempt\"}) \n",
    "workflow.add_edge(\"remove_failed_tool_call_attempt\", \"fallback_agent\")  # æ¸…ç†å¤±è´¥å°è¯•åä½¿ç”¨é™çº§æ¨¡å‹\n",
    "workflow.add_edge(\"fallback_agent\", \"tools\")  # é™çº§æ¨¡å‹ç”Ÿæˆçš„å·¥å…·è°ƒç”¨è¯·æ±‚ç»§ç»­ç”±toolsèŠ‚ç‚¹å¤„ç†\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb149d8",
   "metadata": {},
   "source": [
    "##### ç¤ºä¾‹ 3-33ï¼šå·¥å…·å‡½æ•°è¿”å› Command å¯¹è±¡ï¼Œæ›´æ–°å›¾çŠ¶æ€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8dfc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing_extensions import Annotated, Any\n",
    "\n",
    "from langchain_core.tools import tool, InjectedToolCallId\n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "from langgraph.types import Command\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt.chat_agent_executor import AgentState\n",
    "from langgraph.prebuilt import ToolNode, InjectedState\n",
    "\n",
    "\n",
    "USER_INFO = [ # å®šä¹‰ç”¨æˆ·ä¿¡æ¯åˆ—è¡¨ (ç¤ºä¾‹æ•°æ®)\n",
    "    {\"user_id\": \"1\", \"name\": \"Bob Dylan\", \"location\": \"New York, NY\"},\n",
    "    {\"user_id\": \"2\", \"name\": \"Taylor Swift\", \"location\": \"Beverly Hills, CA\"},\n",
    "]\n",
    "\n",
    "USER_ID_TO_USER_INFO = {info[\"user_id\"]: info for info in USER_INFO} #  ç”¨æˆ· ID -> ç”¨æˆ·ä¿¡æ¯ å­—å…¸\n",
    "\n",
    "class State(AgentState): # å®šä¹‰å›¾çŠ¶æ€ç»“æ„ä½“, ç»§æ‰¿è‡ª AgentState, å¹¶æ·»åŠ  user_info çŠ¶æ€é”®\n",
    "    user_info: dict[str, Any]\n",
    "    user_id: str\n",
    "\n",
    "@tool\n",
    "def lookup_user_info( # å®šä¹‰å·¥å…·å‡½æ•° lookup_user_info\n",
    "    tool_call_id: Annotated[str, InjectedToolCallId],\n",
    "    user_id: Annotated[str, InjectedState(\"user_id\")]\n",
    "):\n",
    "    \"\"\"Use this to look up user information to better assist them with their questions.\"\"\" # å·¥å…·æè¿°ä¿¡æ¯\n",
    "    if user_id is None:\n",
    "        raise ValueError(\"Please provide user ID\")\n",
    "    if user_id not in USER_ID_TO_USER_INFO:\n",
    "        raise ValueError(f\"User '{user_id}' not found\")\n",
    "\n",
    "    user_info = USER_ID_TO_USER_INFO[user_id] # æ ¹æ® user_id æŸ¥è¯¢ç”¨æˆ·ä¿¡æ¯\n",
    "\n",
    "    return Command( # å·¥å…·å‡½æ•°è¿”å› Command å¯¹è±¡\n",
    "        update={ # Command å¯¹è±¡åŒ…å«çŠ¶æ€æ›´æ–°æŒ‡ä»¤\n",
    "            \"user_info\": user_info, # æ›´æ–° user_info çŠ¶æ€é”®ï¼Œå€¼ä¸ºæŸ¥è¯¢åˆ°çš„ç”¨æˆ·ä¿¡æ¯\n",
    "            \"messages\": [ # æ›´æ–° messages çŠ¶æ€é”®ï¼Œæ·»åŠ  ToolMessage\n",
    "                ToolMessage(\n",
    "                    \"Successfully looked up user information\", tool_call_id=tool_call_id\n",
    "                )\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "\n",
    "# åˆå§‹åŒ–çŠ¶æ€å›¾\n",
    "graph = StateGraph(State)\n",
    "\n",
    "# å®šä¹‰èŠ‚ç‚¹\n",
    "def agent_node(state: State):\n",
    "    \"\"\"æ™ºèƒ½ä½“èŠ‚ç‚¹ï¼Œå¤„ç†ç”¨æˆ·è¯·æ±‚\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    user_info = state.get(\"user_info\", {})\n",
    "    \n",
    "    # å¦‚æœæœ‰ç”¨æˆ·ä¿¡æ¯ï¼Œå°†å…¶æ·»åŠ åˆ°ç³»ç»Ÿæ¶ˆæ¯ä¸­\n",
    "    if user_info:\n",
    "        system_message = f\"You are assisting {user_info['name']} who lives in {user_info['location']}.\"\n",
    "    else:\n",
    "        system_message = \"You are a helpful assistant.\"\n",
    "    \n",
    "    # è°ƒç”¨æ¨¡å‹å¤„ç†è¯·æ±‚\n",
    "    model = ChatOpenAI(model=\"Qwen/Qwen3-8B\", temperature=0)\n",
    "    model_with_tools = model.bind_tools([lookup_user_info])\n",
    "    response = model_with_tools.invoke([{\"role\": \"system\", \"content\": system_message}] + messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def should_use_tools(state: State):\n",
    "    \"\"\"å†³å®šæ˜¯å¦ä½¿ç”¨å·¥å…·\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # æ£€æŸ¥æœ€åä¸€æ¡æ¶ˆæ¯æ˜¯å¦åŒ…å«å·¥å…·è°ƒç”¨\n",
    "    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    return \"end\"\n",
    "\n",
    "# ä½¿ç”¨ ToolNode ç®€åŒ–å·¥å…·è°ƒç”¨é€»è¾‘\n",
    "tools_node = ToolNode([lookup_user_info])\n",
    "\n",
    "# æ·»åŠ èŠ‚ç‚¹åˆ°å›¾\n",
    "graph.add_node(\"agent\", agent_node)\n",
    "graph.add_node(\"tools\", tools_node)\n",
    "\n",
    "# æ·»åŠ è¾¹å’Œæ¡ä»¶è¾¹\n",
    "graph.add_edge(START, \"agent\")\n",
    "graph.add_edge(\"tools\", \"agent\")\n",
    "graph.add_conditional_edges(\"agent\", should_use_tools, {\"tools\": \"tools\", \"end\": END})\n",
    "\n",
    "# ç¼–è¯‘å›¾\n",
    "agent = graph.compile()\n",
    "\n",
    "# è°ƒç”¨ ReAct æ™ºèƒ½ä½“ï¼Œé€šè¿‡ config å‚æ•°ä¼ é€’è¿è¡Œæ—¶å‚æ•° user_id\n",
    "for chunk in agent.stream(\n",
    "    # é€šè¿‡ user_id çŠ¶æ€é”®ä¼ é€’è¿è¡Œæ—¶å‚æ•°\n",
    "    {\"messages\": [(\"human\", \"who am i and where do i live?\")], \"user_id\": \"1\"},\n",
    "):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5590b732",
   "metadata": {},
   "source": [
    "##### ç¤ºä¾‹ 3-34ï¼šä½¿ç”¨ Annotated å’Œ InjectedState æ³¨è§£ä¼ é€’è¿è¡Œæ—¶å‚æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d024150a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing_extensions import Annotated, Any\n",
    "\n",
    "from langchain_core.tools import tool, InjectedToolCallId\n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "from langgraph.types import Command\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt.chat_agent_executor import AgentState\n",
    "from langgraph.prebuilt import ToolNode, InjectedState\n",
    "\n",
    "\n",
    "USER_INFO = [ #  ç”¨æˆ·ä¿¡æ¯åˆ—è¡¨ (ç¤ºä¾‹æ•°æ®)\n",
    "    {\"user_id\": \"1\", \"name\": \"Bob Dylan\", \"location\": \"New York, NY\"},\n",
    "    {\"user_id\": \"2\", \"name\": \"Taylor Swift\", \"location\": \"Beverly Hills, CA\"},\n",
    "]\n",
    "\n",
    "USER_ID_TO_USER_INFO = {info[\"user_id\"]: info for info in USER_INFO} #  ç”¨æˆ· ID -> ç”¨æˆ·ä¿¡æ¯ å­—å…¸\n",
    "\n",
    "class State(AgentState): # å®šä¹‰å›¾çŠ¶æ€ç»“æ„ä½“, ç»§æ‰¿è‡ª AgentState, å¹¶æ·»åŠ  user_info çŠ¶æ€é”®\n",
    "    user_info: dict[str, Any]\n",
    "    user_id: str\n",
    "\n",
    "@tool\n",
    "def lookup_user_info( # å®šä¹‰å·¥å…·å‡½æ•° lookup_user_info\n",
    "    tool_call_id: Annotated[str, InjectedToolCallId],\n",
    "    user_id: Annotated[str, InjectedState(\"user_id\")]\n",
    "):\n",
    "    \"\"\"Use this to look up user information to better assist them with their questions.\"\"\" # å·¥å…·æè¿°ä¿¡æ¯\n",
    "    if user_id is None:\n",
    "        raise ValueError(\"Please provide user ID\")\n",
    "    if user_id not in USER_ID_TO_USER_INFO:\n",
    "        raise ValueError(f\"User '{user_id}' not found\")\n",
    "\n",
    "    user_info = USER_ID_TO_USER_INFO[user_id] # æ ¹æ® user_id æŸ¥è¯¢ç”¨æˆ·ä¿¡æ¯\n",
    "\n",
    "    return Command( # å·¥å…·å‡½æ•°è¿”å› Command å¯¹è±¡\n",
    "        update={ # Command å¯¹è±¡åŒ…å«çŠ¶æ€æ›´æ–°æŒ‡ä»¤\n",
    "            \"user_info\": user_info, # æ›´æ–° user_info çŠ¶æ€é”®ï¼Œå€¼ä¸ºæŸ¥è¯¢åˆ°çš„ç”¨æˆ·ä¿¡æ¯\n",
    "            \"messages\": [ # æ›´æ–° messages çŠ¶æ€é”®ï¼Œæ·»åŠ  ToolMessage\n",
    "                ToolMessage(\n",
    "                    \"Successfully looked up user information\", tool_call_id=tool_call_id\n",
    "                )\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "\n",
    "# åˆå§‹åŒ–çŠ¶æ€å›¾\n",
    "graph = StateGraph(State)\n",
    "\n",
    "# å®šä¹‰èŠ‚ç‚¹\n",
    "def agent_node(state: State):\n",
    "    \"\"\"æ™ºèƒ½ä½“èŠ‚ç‚¹ï¼Œå¤„ç†ç”¨æˆ·è¯·æ±‚\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    user_info = state.get(\"user_info\", {})\n",
    "    \n",
    "    # å¦‚æœæœ‰ç”¨æˆ·ä¿¡æ¯ï¼Œå°†å…¶æ·»åŠ åˆ°ç³»ç»Ÿæ¶ˆæ¯ä¸­\n",
    "    if user_info:\n",
    "        system_message = f\"You are assisting {user_info['name']} who lives in {user_info['location']}.\"\n",
    "    else:\n",
    "        system_message = \"You are a helpful assistant.\"\n",
    "    \n",
    "    # è°ƒç”¨æ¨¡å‹å¤„ç†è¯·æ±‚\n",
    "    model = ChatOpenAI(model=\"Qwen/Qwen3-8B\", temperature=0)\n",
    "    model_with_tools = model.bind_tools([lookup_user_info])\n",
    "    response = model_with_tools.invoke([{\"role\": \"system\", \"content\": system_message}] + messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def should_use_tools(state: State):\n",
    "    \"\"\"å†³å®šæ˜¯å¦ä½¿ç”¨å·¥å…·\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # æ£€æŸ¥æœ€åä¸€æ¡æ¶ˆæ¯æ˜¯å¦åŒ…å«å·¥å…·è°ƒç”¨\n",
    "    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    return \"end\"\n",
    "\n",
    "# ä½¿ç”¨ ToolNode ç®€åŒ–å·¥å…·è°ƒç”¨é€»è¾‘\n",
    "tools_node = ToolNode([lookup_user_info])\n",
    "\n",
    "# æ·»åŠ èŠ‚ç‚¹åˆ°å›¾\n",
    "graph.add_node(\"agent\", agent_node)\n",
    "graph.add_node(\"tools\", tools_node)\n",
    "\n",
    "# æ·»åŠ è¾¹å’Œæ¡ä»¶è¾¹\n",
    "graph.add_edge(START, \"agent\")\n",
    "graph.add_edge(\"tools\", \"agent\")\n",
    "graph.add_conditional_edges(\"agent\", should_use_tools, {\"tools\": \"tools\", \"end\": END})\n",
    "\n",
    "# ç¼–è¯‘å›¾\n",
    "agent = graph.compile()\n",
    "\n",
    "# è°ƒç”¨ ReAct æ™ºèƒ½ä½“ï¼Œé€šè¿‡ config å‚æ•°ä¼ é€’è¿è¡Œæ—¶å‚æ•° user_id\n",
    "for chunk in agent.stream(\n",
    "    # é€šè¿‡ user_id çŠ¶æ€é”®ä¼ é€’è¿è¡Œæ—¶å‚æ•°\n",
    "    {\"messages\": [(\"human\", \"who am i and where do i live?\")], \"user_id\": \"1\"},\n",
    "):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8g4owxr8xk",
   "metadata": {},
   "source": [
    "**ğŸ’¡ å·¥å…·è°ƒç”¨æ ¸å¿ƒè¦ç‚¹**ï¼š\n",
    "\n",
    "#### å·¥å…·å®šä¹‰ä¸æ³¨å†Œ\n",
    "- **@tool è£…é¥°å™¨**ï¼šå°† Python å‡½æ•°è½¬æ¢ä¸º LangChain å·¥å…·ï¼Œè‡ªåŠ¨ç”Ÿæˆå·¥å…·æè¿°\n",
    "- **æ¸…æ™°çš„ docstring**ï¼šå·¥å…·æè¿°ä¿¡æ¯å¸®åŠ© LLM ç†è§£ä½•æ—¶å’Œå¦‚ä½•ä½¿ç”¨å·¥å…·\n",
    "- **ç±»å‹æç¤º**ï¼šä¸ºå·¥å…·å‚æ•°æ·»åŠ ç±»å‹æç¤ºï¼Œæé«˜å·¥å…·è°ƒç”¨çš„å‡†ç¡®æ€§\n",
    "\n",
    "#### ToolNode å·¥ä½œæœºåˆ¶\n",
    "- **å·¥å…·è°ƒåº¦**ï¼šæ ¹æ® LLM çš„å·¥å…·è°ƒç”¨è¯·æ±‚ï¼ŒåŠ¨æ€è°ƒåº¦å¯¹åº”çš„å·¥å…·å‡½æ•°\n",
    "- **é”™è¯¯å¤„ç†**ï¼šè‡ªåŠ¨æ•è·å·¥å…·æ‰§è¡Œå¼‚å¸¸ï¼Œå°†é”™è¯¯ä¿¡æ¯å°è£…ä¸º ToolMessage\n",
    "- **çŠ¶æ€æ›´æ–°**ï¼šå°†å·¥å…·æ‰§è¡Œç»“æœæ·»åŠ åˆ°å›¾çŠ¶æ€çš„æ¶ˆæ¯å†å²ä¸­\n",
    "\n",
    "#### ReAct å¾ªç¯å®ç°\n",
    "- **Agent èŠ‚ç‚¹**ï¼šè´Ÿè´£æ¥æ”¶è¾“å…¥ï¼Œå†³å®šæ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·\n",
    "- **Tools èŠ‚ç‚¹**ï¼šæ‰§è¡Œå…·ä½“çš„å·¥å…·è°ƒç”¨æ“ä½œ\n",
    "- **æ¡ä»¶è·¯ç”±**ï¼šæ ¹æ®æ˜¯å¦åŒ…å« tool_calls å†³å®šæµç¨‹èµ°å‘\n",
    "- **å¾ªç¯åé¦ˆ**ï¼šå·¥å…·ç»“æœè¿”å›ç»™ Agentï¼Œå½¢æˆâ€œæ¨ç†-è¡ŒåŠ¨-è§‚å¯Ÿâ€å¾ªç¯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfh581kwcus",
   "metadata": {},
   "source": [
    "## 3.6 å›¾çš„å¯è§†åŒ–\n",
    "\n",
    "éšç€ LangGraph æµç¨‹å˜å¾—è¶Šæ¥è¶Šå¤æ‚ï¼Œç‰¹åˆ«æ˜¯å½“ç³»ç»Ÿä¸­å¼•å…¥äº†å­å›¾æœºåˆ¶åï¼Œä»…ä»…ä¾é ä»£ç æ¥ç†è§£æ•´ä¸ªæµç¨‹çš„ç»“æ„å’Œæ‰§è¡Œé€»è¾‘å˜å¾—è¶Šæ¥è¶Šå›°éš¾ã€‚å›¾çš„å¯è§†åŒ–æˆä¸ºäº†ä¸€ç§è‡³å…³é‡è¦çš„è¾…åŠ©å·¥å…·ã€‚\n",
    "\n",
    "é€šè¿‡å°† LangGraph å›¾ç»“æ„å¯è§†åŒ–åœ°æ¸²æŸ“æˆæ˜“äºç†è§£çš„å›¾è¡¨ï¼Œæˆ‘ä»¬å¯ä»¥ç›´è§‚åœ°æŠŠæ¡æµç¨‹çš„æ•´ä½“ç»“æ„ã€èŠ‚ç‚¹ä¹‹é—´çš„è¿æ¥å…³ç³»ä»¥åŠæ•°æ®æµåŠ¨çš„æ–¹å‘ã€‚\n",
    "\n",
    "### 3.6.1 Mermaid è¯­æ³•\n",
    "\n",
    "Mermaid æ˜¯ä¸€ç§æµè¡Œçš„æ–‡æœ¬æè¿°è¯­è¨€ï¼Œç”¨äºå¿«é€Ÿåˆ›å»ºå„ç§ç±»å‹çš„å›¾è¡¨ã€‚LangGraph å†…ç½®äº†å°† Graph å¯¹è±¡è½¬æ¢ä¸º Mermaid è¯­æ³•çš„åŠŸèƒ½ã€‚\n",
    "\n",
    "#### Mermaid çš„ä¼˜åŠ¿ï¼š\n",
    "- **è½»é‡çº§**ï¼šæ— éœ€å®‰è£…é¢å¤–çš„ä¾èµ–åº“ï¼Œè·¨å¹³å°ä½¿ç”¨  \n",
    "- **æ˜“äºç¼–è¾‘**ï¼šå¯ä»¥ç›´æ¥åœ¨æµè§ˆå™¨æˆ– Markdown ç¼–è¾‘å™¨ä¸­æŸ¥çœ‹å’Œç¼–è¾‘\n",
    "- **æ˜“äºåˆ†äº«**ï¼šæ–‡æœ¬æ ¼å¼ä¾¿äºç‰ˆæœ¬æ§åˆ¶å’Œå›¢é˜Ÿåä½œ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h86f4i16z4",
   "metadata": {},
   "source": [
    "##### ç¤ºä¾‹ 3-35 & 3-36ï¼šå›¾å¯è§†åŒ–æ¼”ç¤º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ohfvi2nrw",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, Any\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    aggregate: Annotated[list, operator.add]\n",
    "\n",
    "def a(state: State):\n",
    "    print(f'Adding \"A\" to {state[\"aggregate\"]}')\n",
    "    return {\"aggregate\": [\"A\"]}\n",
    "\n",
    "def b(state: State):\n",
    "    print(f'Adding \"B\" to {state[\"aggregate\"]}')\n",
    "    return {\"aggregate\": [\"B\"]}\n",
    "\n",
    "def c(state: State):\n",
    "    print(f'Adding \"C\" to {state[\"aggregate\"]}')\n",
    "    return {\"aggregate\": [\"C\"]}\n",
    "\n",
    "def d(state: State):\n",
    "    print(f'Adding \"D\" to {state[\"aggregate\"]}')\n",
    "    return {\"aggregate\": [\"D\"]}\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(a)\n",
    "builder.add_node(b)\n",
    "builder.add_node(c)\n",
    "builder.add_node(d)\n",
    "builder.add_edge(START, \"a\")\n",
    "builder.add_edge(\"a\", \"b\")\n",
    "builder.add_edge(\"a\", \"c\")\n",
    "builder.add_edge(\"b\", \"d\")\n",
    "builder.add_edge(\"c\", \"d\")\n",
    "builder.add_edge(\"d\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "print(graph.get_graph().draw_mermaid())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7372cc0f",
   "metadata": {},
   "source": [
    "##### ç¤ºä¾‹ 3-37ï¼šä½¿ç”¨ Mermaid.ink API æ¸²æŸ“ PNG å›¾ç‰‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb71a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "\n",
    "png_bytes = graph.get_graph().draw_mermaid_png(draw_method=MermaidDrawMethod.API) #  ä½¿ç”¨ MermaidDrawMethod.API æŒ‡å®šä½¿ç”¨ Mermaid.ink API æ¸²æŸ“\n",
    "display(Image(png_bytes)) #  åœ¨ Notebook ä¸­æ˜¾ç¤º PNG å›¾ç‰‡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63afd74b",
   "metadata": {},
   "source": [
    "##### ç¤ºä¾‹ 3-38ï¼šä½¿ç”¨ Mermaid å’Œ Pyppeteer åº“æ¸²æŸ“ PNG å›¾ç‰‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2cf151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Annotated, Literal\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "class MyNode:\n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(self, state: State):\n",
    "        return {\"messages\": [(\"assistant\", f\"Called node {self.name}\")]}\n",
    "\n",
    "\n",
    "def route(state) -> Literal[\"entry_node\", \"__end__\"]:\n",
    "    if len(state[\"messages\"]) > 10:\n",
    "        return \"__end__\"\n",
    "    return \"entry_node\"\n",
    "\n",
    "\n",
    "def add_fractal_nodes(builder, current_node, level, max_level):\n",
    "    if level > max_level:\n",
    "        return\n",
    "\n",
    "    # Number of nodes to create at this level\n",
    "    num_nodes = random.randint(1, 3)  # Adjust randomness as needed\n",
    "    for i in range(num_nodes):\n",
    "        nm = [\"A\", \"B\", \"C\"][i]\n",
    "        node_name = f\"node_{current_node}_{nm}\"\n",
    "        builder.add_node(node_name, MyNode(node_name))\n",
    "        builder.add_edge(current_node, node_name)\n",
    "\n",
    "        # Recursively add more nodes\n",
    "        r = random.random()\n",
    "        if r > 0.2 and level + 1 < max_level:\n",
    "            add_fractal_nodes(builder, node_name, level + 1, max_level)\n",
    "        elif r > 0.05:\n",
    "            builder.add_conditional_edges(node_name, route, node_name)\n",
    "        else:\n",
    "            # End\n",
    "            builder.add_edge(node_name, \"__end__\")\n",
    "\n",
    "\n",
    "def build_fractal_graph(max_level: int):\n",
    "    builder = StateGraph(State)\n",
    "    entry_point = \"entry_node\"\n",
    "    builder.add_node(entry_point, MyNode(entry_point))\n",
    "    builder.add_edge(START, entry_point)\n",
    "\n",
    "    add_fractal_nodes(builder, entry_point, 1, max_level)\n",
    "\n",
    "    # Optional: set a finish point if required\n",
    "    builder.add_edge(entry_point, END)  # or any specific node\n",
    "\n",
    "    return builder.compile()\n",
    "\n",
    "\n",
    "app = build_fractal_graph(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d422f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "import nest_asyncio\n",
    "\n",
    "# ä¿®å¤ asyncio è¿è¡Œæ—¶é”™è¯¯\n",
    "nest_asyncio.apply()\n",
    "\n",
    "png_bytes = app.get_graph().draw_mermaid_png( #  ä½¿ç”¨ draw_mermaid_png() æ–¹æ³•æ¸²æŸ“ PNG å›¾ç‰‡\n",
    "    draw_method=MermaidDrawMethod.PYPPETEER, #  æŒ‡å®šä½¿ç”¨ MermaidDrawMethod.PYPPETEER,  ä½¿ç”¨ Mermaid + Pyppeteer æ¸²æŸ“\n",
    "    curve_style=CurveStyle.LINEAR, #  è®¾ç½®æ›²çº¿é£æ ¼ä¸ºçº¿æ€§\n",
    "    node_colors=NodeStyles(first=\"#ffdfba\", last=\"#baffc9\", default=\"#fad7de\"), #  è‡ªå®šä¹‰èŠ‚ç‚¹é¢œè‰²\n",
    "    wrap_label_n_words=9, #  è®¾ç½®èŠ‚ç‚¹æ ‡ç­¾è‡ªåŠ¨æ¢è¡Œï¼Œ  æ¯è¡Œæœ€å¤š 9 ä¸ªå•è¯\n",
    "    output_file_path=None, #  ä¸è¾“å‡ºåˆ°æ–‡ä»¶\n",
    "    background_color=\"white\", #  è®¾ç½®èƒŒæ™¯è‰²ä¸ºç™½è‰²\n",
    "    padding=10, #  è®¾ç½®è¾¹è·ä¸º 10 åƒç´ \n",
    ")\n",
    "display(Image(png_bytes)) #  åœ¨ Notebook ä¸­æ˜¾ç¤º PNG å›¾ç‰‡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "je31bve7tv",
   "metadata": {},
   "source": [
    "**ğŸ’¡ å›¾å¯è§†åŒ–æ ¸å¿ƒä»·å€¼**ï¼š\n",
    "\n",
    "#### å¯è§†åŒ–æ–¹å¼å¯¹æ¯”\n",
    "\n",
    "| æ–¹å¼ | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |\n",
    "|------|------|------|----------|\n",
    "| **Mermaid è¯­æ³•** | è½»é‡çº§ï¼Œæ— éœ€ä¾èµ–ï¼Œè·¨å¹³å° | å®šåˆ¶åŒ–é€‰é¡¹æœ‰é™ | å¿«é€ŸåŸå‹ï¼Œæ–‡æ¡£åµŒå…¥ |\n",
    "| **PNG å›¾ç‰‡** | ç›´è§‚æ¸…æ™°ï¼Œæ˜“äºåˆ†äº« | æ— æ³•åŠ¨æ€ç¼–è¾‘ | æŠ¥å‘Šå±•ç¤ºï¼Œé™æ€æ–‡æ¡£ |\n",
    "| **äº¤äº’å¼å›¾è¡¨** | å¯äº¤äº’ï¼ŒåŠŸèƒ½ä¸°å¯Œ | éœ€è¦é¢å¤–å·¥å…· | å¤æ‚ç³»ç»Ÿåˆ†æ |\n",
    "\n",
    "#### å®é™…åº”ç”¨åœºæ™¯\n",
    "\n",
    "- **å¼€å‘é˜¶æ®µ**ï¼šä½¿ç”¨ Mermaid å¿«é€ŸéªŒè¯æµç¨‹é€»è¾‘\n",
    "- **è°ƒè¯•é˜¶æ®µ**ï¼šé€šè¿‡å›¾å¯è§†åŒ–å‘ç°æµç¨‹é—®é¢˜\n",
    "- **æ–‡æ¡£é˜¶æ®µ**ï¼šç”Ÿæˆå›¾è¡¨ç”¨äºæŠ€æœ¯æ–‡æ¡£å’Œå›¢é˜Ÿåˆ†äº«\n",
    "- **ç»´æŠ¤é˜¶æ®µ**ï¼šé€šè¿‡å›¾ç»“æ„åˆ†æä¼˜åŒ–ç³»ç»Ÿæ¶æ„\n",
    "\n",
    "å›¾å¯è§†åŒ–ä¸ä»…æ˜¯ä¸€ä¸ªå¼€å‘è¾…åŠ©å·¥å…·ï¼Œæ›´æ˜¯ç†è§£å’Œä¼˜åŒ– LangGraph ç³»ç»Ÿçš„é‡è¦æ‰‹æ®µï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå’Œå­å›¾åµŒå¥—åœºæ™¯æ—¶å‘æŒ¥é‡è¦ä½œç”¨ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occ4jxydkyr",
   "metadata": {},
   "source": [
    "## ğŸ“š æœ¬ç« æ€»ç»“\n",
    "\n",
    "é€šè¿‡æœ¬ç« çš„å­¦ä¹ ï¼Œæˆ‘ä»¬æ·±å…¥æ¢è®¨äº† LangGraph çš„å›¾é©±åŠ¨ AI æ™ºèƒ½ä½“ç³»ç»Ÿï¼ŒæŒæ¡äº†çŠ¶æ€ã€èŠ‚ç‚¹ã€è¾¹ã€å‘½ä»¤å››å¤§æ ¸å¿ƒåŸè¯­çš„åº”ç”¨ï¼Œå­¦ä¼šäº†å¹¶è¡Œå¤„ç†ã€MapReduce æ¨¡å¼ã€å­å›¾æœºåˆ¶ç­‰é«˜çº§æŠ€æœ¯ï¼Œä»¥åŠå·¥å…·é›†æˆå’Œå›¾å¯è§†åŒ–ç­‰å®ç”¨å¼€å‘æŠ€èƒ½ã€‚LangGraph çš„å›¾è®¡ç®—æ¨¡å‹ä¸ºæˆ‘ä»¬æä¾›äº†æ„å»ºå¤æ‚ã€åŠ¨æ€ã€å¯æ‰©å±•æ™ºèƒ½ä½“ç³»ç»Ÿçš„å¼ºå¤§èƒ½åŠ›ï¼Œè®©æˆ‘ä»¬èƒ½å¤Ÿè®¾è®¡å‡ºçœŸæ­£é€‚åº”ç°å®ä¸–ç•Œå¤æ‚åœºæ™¯çš„ AI åº”ç”¨ã€‚åœ¨ä¸‹ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨ AI æ™ºèƒ½ä½“çš„äº¤äº’ä½“éªŒè®¾è®¡ï¼Œå­¦ä¹ å¦‚ä½•æ„å»ºæ›´åŠ äººæ€§åŒ–ã€ç›´è§‚æ˜“ç”¨çš„æ™ºèƒ½ä½“ç•Œé¢å’Œäº¤äº’æ¨¡å¼ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
