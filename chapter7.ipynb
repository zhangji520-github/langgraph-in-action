{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "chapter-intro",
   "metadata": {},
   "source": [
    "# 第 7 章：AI 智能体系统的架构与范式\n",
    "\n",
    "> 本笔记文件需要与《LangGraph实战》的第 7 章的内容配套使用。\n",
    "\n",
    "在本章中，我们将深入探索构建复杂、高效且可扩展的 AI 智能体系统的核心要素：架构设计与模式应用。随着我们不断提升 AI 智能体的能力，使其能够处理日益复杂的任务，仅仅依赖单一、线性的智能体模型已显得力不从心。如同城市规划需要蓝图、软件开发需要架构设计，构建强大的 AI 智能体系统同样需要精心设计的架构作为支撑。\n",
    "\n",
    "我们将从基础但至关重要的智能体工作流模式开始，例如提示链、路由、并行化、协调器-工作者和评估器-优化器，这些模式构成了构建复杂智能体行为的基石。随后，我们将逐步深入多智能体架构的世界，重点介绍主管架构和分层架构，揭示如何通过组织和协调多个专业智能体来提升系统的整体性能和可管理性。最后，我们将前瞻性地审视情境感知智能体架构，这种架构代表了 AI 智能体发展的新方向。\n",
    "\n",
    "通过本章的学习，您将不仅了解各种智能体系统的架构蓝图，更将掌握在 LangGraph 中实践这些架构模式的关键技术和方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environment-setup",
   "metadata": {},
   "source": [
    "### 🚀 环境准备\n",
    "\n",
    "首先加载必要的环境变量配置："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7-1",
   "metadata": {},
   "source": [
    "## 7.1 常见的智能体系统工作流\n",
    "\n",
    "在人工智能体开发领域，尤其是在 AI 智能体生态系统中，理解不同类型的智能体系统之间的细微差别至关重要。正如 Anthropic 在其对智能体构建模式的富有洞察力的分析中所强调的那样，我们可以更好地驾驭 AI 系统的复杂性。\n",
    "\n",
    "Anthropic 的研究突出了工作流（Workflow）和智能体（Agent）之间的一个关键区别：\n",
    "\n",
    "- **工作流**：LLM 和相关工具通过显式预定义的代码路径进行编排的系统\n",
    "- **智能体**：大语言模型（LLM）动态指导自身流程的系统，实时决策工具的使用以及实现目标所需的步骤\n",
    "\n",
    "在 LangGraph 的背景下，工作流使用其状态图架构优雅地实现，允许开发人员以可视化和编程方式定义系统中不同组件之间的信息和控制流。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7-1-1",
   "metadata": {},
   "source": [
    "### 7.1.1 工作流的基础构建模块：增强型 LLM\n",
    "\n",
    "现代 LLM 不仅仅是独立的模型，它们通过一系列功能得到增强，这些功能使它们能够与世界互动并执行超出简单文本生成的复杂任务。核心增强功能通常包括：\n",
    "\n",
    "- **检索（Retrieval）**：允许 LLM 访问和整合来自外部来源的信息\n",
    "- **工具（Tools）**：使 LLM 能够与外部系统交互并在现实世界中执行操作\n",
    "- **记忆（Memory）**：允许 LLM 保留和利用来自过去交互或工作流步骤的信息\n",
    "\n",
    "这些增强功能与核心 LLM 协同工作，构成了构建复杂工作流和智能体的基础。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7-1-2",
   "metadata": {},
   "source": [
    "### 7.1.2 提示链（Prompt Chaining）\n",
    "\n",
    "提示链是一种基本的工作流模式，专注于将复杂任务分解为一系列更简单、相互关联的步骤。在这种工作流中，一个 LLM 调用的输出成为后续调用的输入，从而创建一系列处理阶段。\n",
    "\n",
    "提示链的主要优势在于它能够通过简化每个 LLM 调用来提高准确性。通过将复杂任务分解为更小、更易于管理的子任务，每个 LLM 调用都会获得更集中且更明确的提示。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example-7-1",
   "metadata": {},
   "source": [
    "##### 示例 7-1：基于 Graph API 的提示链工作流实现\n",
    "\n",
    "首先导入必要的 LangGraph 和 LangChain 组件，并实现一个笑话生成和改进的提示链："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始笑话：\n",
      "为什么猫咪不喜欢上网？\n",
      "\n",
      "因为它们怕“鼠”标！\n",
      "\n",
      "改进后的笑话：\n",
      "为什么猫咪不喜欢上网？\n",
      "\n",
      "因为它们怕“鼠”标，但它们更喜欢“点”心，尤其是“喵”点心！\n",
      "\n",
      "最终笑话：\n",
      "为什么猫咪不喜欢上网？\n",
      "\n",
      "因为它们怕“鼠”标，但它们更喜欢“点”心，尤其是“喵”点心！然而，有一天，一只猫咪意外打开了一个关于“养生”的网站，结果它发现只要虚拟点击就能获得无限的“鱼”干和“奶”瓶，顿时就成了网上购物的狂欢者，彻底抛弃了对“鼠”标的恐惧！\n"
     ]
    }
   ],
   "source": [
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from llm_utils import llm\n",
    "\n",
    "# 使用 TypedDict 定义图状态，用于类型提示和状态管理\n",
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    joke: str\n",
    "    improved_joke: str\n",
    "    final_joke: str\n",
    "\n",
    "# 图中的节点，每个节点代表提示链中的一个步骤\n",
    "def generate_joke(state: State):\n",
    "    \"\"\"第一个 LLM 调用，根据主题生成初始笑话\"\"\"\n",
    "    msg = llm.invoke(f\"写一个关于 {state['topic']} 的简短笑话\") # 使用状态中的主题调用 LLM\n",
    "    return {\"joke\": msg.content} # 返回生成的笑话，更新状态中的 'joke' 键\n",
    "\n",
    "def check_punchline(state: State):\n",
    "    \"\"\"门控函数，检查笑话是否有妙语\"\"\"\n",
    "    # 简单检查 - 笑话是否包含 \"?\" 或 \"!\" 作为妙语存在的代理\n",
    "    if \"?\" in state[\"joke\"] or \"!\" in state[\"joke\"]:\n",
    "        return \"Fail\" # 笑话未能通过妙语检查\n",
    "    return \"Pass\" # 笑话通过妙语检查\n",
    "\n",
    "def improve_joke(state: State):\n",
    "    \"\"\"第二个 LLM 调用，通过添加文字游戏来改进笑话\"\"\"\n",
    "    msg = llm.invoke(f\"通过添加文字游戏使这个笑话更有趣：{state['joke']}\") # 调用 LLM 来改进笑话\n",
    "    return {\"improved_joke\": msg.content} # 返回改进后的笑话，更新状态中的 'improved_joke'\n",
    "\n",
    "def polish_joke(state: State):\n",
    "    \"\"\"第三个 LLM 调用，用于最终润色，添加令人惊讶的转折\"\"\"\n",
    "    msg = llm.invoke(f\"为这个笑话添加一个令人惊讶的转折：{state['improved_joke']}\") # 调用 LLM 来润色笑话\n",
    "    return {\"final_joke\": msg.content} # 返回润色后的笑话，更新状态中的 'final_joke'\n",
    "\n",
    "# 使用 StateGraph 构建工作流，使用定义的状态进行初始化\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# 将节点添加到工作流图中，将它们与定义的函数关联起来\n",
    "workflow.add_node(\"generate_joke\", generate_joke)\n",
    "workflow.add_node(\"improve_joke\", improve_joke)\n",
    "workflow.add_node(\"polish_joke\", polish_joke)\n",
    "\n",
    "# 定义边缘以连接节点并建立工作流序列\n",
    "workflow.add_edge(START, \"generate_joke\") # 开始节点连接到 'generate_joke' 节点\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate_joke\", check_punchline, {\"Pass\": \"improve_joke\", \"Fail\": END} # 'generate_joke' 之后的条件边缘，基于 'check_punchline' 输出\n",
    ")\n",
    "workflow.add_edge(\"improve_joke\", \"polish_joke\") # 'improve_joke' 节点连接到 'polish_joke' 节点\n",
    "workflow.add_edge(\"polish_joke\", END) # 'polish_joke' 节点连接到结束节点\n",
    "\n",
    "# 将工作流图编译为可执行链\n",
    "chain = workflow.compile()\n",
    "\n",
    "\n",
    "# 使用初始状态（主题：\"cats\"）调用编译链\n",
    "state = chain.invoke({\"topic\": \"cats\"})\n",
    "print(\"初始笑话：\")\n",
    "print(state[\"joke\"])\n",
    "\n",
    "if \"improved_joke\" in state: # 检查 'improved_joke' 是否存在于状态中，指示妙语检查失败\n",
    "    print(\"\\n改进后的笑话：\")\n",
    "    print(state[\"improved_joke\"])\n",
    "    print(\"\\n最终笑话：\")\n",
    "    print(state[\"final_joke\"])\n",
    "\n",
    "# 保存图像\n",
    "from draw import save_graph_as_png\n",
    "save_graph_as_png(chain, \"./graphs/c7/augumented_chain.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explanation-7-1",
   "metadata": {},
   "source": [
    "**💡 核心概念解析**：\n",
    "\n",
    "在这段代码中，我们展示了提示链的关键特性：\n",
    "\n",
    "- **顺序处理**：每个步骤的输出成为下一个步骤的输入\n",
    "- **门控机制**：`check_punchline` 函数作为质量控制检查点\n",
    "- **条件分支**：根据门控结果决定是直接结束还是继续改进\n",
    "- **状态管理**：使用 TypedDict 清晰定义数据流"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example-7-2",
   "metadata": {},
   "source": [
    "##### 示例 7-2：基于 Functional API 的提示链工作流实现\n",
    "\n",
    "接下来让我们看看如何使用 LangGraph 的 Functional API 实现相同的提示链逻辑："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.func import entrypoint, task\n",
    "\n",
    "llm = ChatOpenAI(model=\"Qwen/Qwen3-8B\")\n",
    "\n",
    "# 使用 @task 装饰器定义的任务，代表工作流中的步骤\n",
    "@task\n",
    "def generate_joke(topic: str):\n",
    "    \"\"\"第一个 LLM 调用，生成初始笑话\"\"\"\n",
    "    msg = llm.invoke(f\"写一个关于 {topic} 的简短笑话\") # 调用 LLM 以根据主题生成笑话\n",
    "    return msg.content # 返回生成的笑话\n",
    "\n",
    "def check_punchline(joke: str):\n",
    "    \"\"\"门控函数，检查笑话是否有妙语\"\"\"\n",
    "    # 简单检查 - 笑话是否包含 \"?\" 或 \"!\"\n",
    "    if \"?\" in joke or \"!\" in joke:\n",
    "        return \"Fail\" # 笑话未能通过妙语检查\n",
    "    return \"Pass\" # 笑话通过妙语检查\n",
    "\n",
    "@task\n",
    "def improve_joke(joke: str):\n",
    "    \"\"\"第二个 LLM 调用，改进笑话\"\"\"\n",
    "    msg = llm.invoke(f\"通过添加文字游戏使这个笑话更有趣：{joke}\") # 调用 LLM 以改进笑话\n",
    "    return msg.content # 返回改进后的笑话\n",
    "\n",
    "@task\n",
    "def polish_joke(joke: str):\n",
    "    \"\"\"第三个 LLM 调用，用于最终润色\"\"\"\n",
    "    msg = llm.invoke(f\"为这个笑话添加一个令人惊讶的转折：{joke}\") # 调用 LLM 以润色笑话\n",
    "    return msg.content # 返回润色后的笑话\n",
    "\n",
    "# 入口点装饰函数使用 Functional API 定义工作流\n",
    "@entrypoint()\n",
    "def workflow(topic: str):\n",
    "    original_joke = generate_joke(topic).result() # 执行 'generate_joke' 任务\n",
    "    if check_punchline(original_joke) == \"Pass\": # 基于 'check_punchline' 输出的条件检查\n",
    "        return original_joke # 如果妙语检查通过，则返回原始笑话\n",
    "\n",
    "    improved_joke = improve_joke(original_joke).result() # 如果妙语检查失败，则执行 'improve_joke' 任务\n",
    "    return polish_joke(improved_joke).result() # 执行 'polish_joke' 任务并返回最终结果\n",
    "\n",
    "# 调用工作流\n",
    "state = workflow.invoke(\"cats\")\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explanation-7-2",
   "metadata": {},
   "source": [
    "**💡 Functional API 的优势**：\n",
    "\n",
    "- **更简洁的代码**：使用 `@task` 装饰器和函数式编程风格\n",
    "- **类型安全**：函数参数提供清晰的类型提示\n",
    "- **易于测试**：每个任务都是独立的函数，便于单元测试\n",
    "- **灵活的控制流**：使用 Python 的原生控制结构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7-1-3",
   "metadata": {},
   "source": [
    "### 7.1.3 路由 (Routing)\n",
    "\n",
    "路由工作流旨在通过对输入进行分类并将其定向到专门的下游任务来处理各种输入。当处理需要处理各种输入类型的复杂应用程序时，此模式尤其有价值，每种输入类型都需要不同的处理方法。\n",
    "\n",
    "路由背后的核心思想是实施决策步骤，该步骤分析输入并确定最合适的后续处理路径。这允许关注点分离，从而可以为每个输入类别开发更集中和优化的提示和流程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example-7-3",
   "metadata": {},
   "source": [
    "##### 示例 7-3：基于 Graph API 的路由工作流实现\n",
    "\n",
    "让我们实现一个智能内容路由器，能够根据用户输入生成故事、笑话或诗歌："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2048d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "为什么猫总是喜欢坐在电脑键盘上？\n",
      "\n",
      "因为它们想要掌控“鼠标”!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from llm_utils import llm\n",
    "\n",
    "# 路由工作流的状态定义\n",
    "class State(TypedDict):\n",
    "    input: str\n",
    "    decision: str\n",
    "    output: str\n",
    "\n",
    "# 图中的节点，每个节点处理特定的路由 (story, joke, poem)\n",
    "def llm_call_1(state: State):\n",
    "    \"\"\"写一个故事\"\"\"\n",
    "    result = llm.invoke(state[\"input\"]) # 调用 LLM 根据输入写一个故事\n",
    "    return {\"output\": result.content} # 返回故事，更新状态中的 'output'\n",
    "\n",
    "def llm_call_2(state: State):\n",
    "    \"\"\"写一个笑话\"\"\"\n",
    "    result = llm.invoke(state[\"input\"]) # 调用 LLM 根据输入写一个笑话\n",
    "    return {\"output\": result.content} # 返回笑话，更新状态中的 'output'\n",
    "\n",
    "def llm_call_3(state: State):\n",
    "    \"\"\"写一首诗\"\"\"\n",
    "    result = llm.invoke(state[\"input\"]) # 调用 LLM 根据输入写一首诗\n",
    "    return {\"output\": result.content} # 返回诗歌，更新状态中的 'output'\n",
    "\n",
    "def llm_call_router(state: State):\n",
    "    \"\"\"使用结构化输出将输入路由到适当的节点\"\"\"\n",
    "    # 使用结构化输出调用增强型 LLM，以充当路由逻辑\n",
    "    \n",
    "    # 创建链式调用\n",
    "    chain = llm | StrOutputParser()\n",
    "    \n",
    "    response = chain.invoke([\n",
    "        SystemMessage(\n",
    "            content=\"You are a router that directs user input to the appropriate handler. Return a JSON object with a 'step' key and one of these values: 'story', 'joke', or 'poem'. For example: {'step': 'joke'}\"\n",
    "        ),\n",
    "        HumanMessage(content=state[\"input\"]),\n",
    "    ])\n",
    "    \n",
    "    # 解析 JSON 字符串\n",
    "    try:\n",
    "        decision = json.loads(response) # 这行代码的作用是将 JSON 格式的字符串解析为 Python 对象（通常是字典或列表）\n",
    "        return {\"decision\": decision[\"step\"]}\n",
    "    except (json.JSONDecodeError, KeyError):\n",
    "        # 如果解析失败，默认返回 joke 路由\n",
    "        return {\"decision\": \"joke\"}\n",
    "\n",
    "# 条件边缘函数，根据决策路由到适当的节点\n",
    "def route_decision(state: State):\n",
    "    # 根据状态中的 'decision' 返回您想要访问的下一个节点名称\n",
    "    if state[\"decision\"] == \"story\":\n",
    "        return \"llm_call_1\"\n",
    "    elif state[\"decision\"] == \"joke\":\n",
    "        return \"llm_call_2\"\n",
    "    elif state[\"decision\"] == \"poem\":\n",
    "        return \"llm_call_3\"\n",
    "\n",
    "# 使用 StateGraph 构建路由工作流\n",
    "router_builder = StateGraph(State)\n",
    "\n",
    "# 将节点添加到图中\n",
    "router_builder.add_node(\"llm_call_1\", llm_call_1)\n",
    "router_builder.add_node(\"llm_call_2\", llm_call_2)\n",
    "router_builder.add_node(\"llm_call_3\", llm_call_3)\n",
    "router_builder.add_node(\"llm_call_router\", llm_call_router)\n",
    "\n",
    "# 定义边缘以连接节点并建立路由逻辑\n",
    "router_builder.add_edge(START, \"llm_call_router\") # 开始节点连接到路由器节点\n",
    "router_builder.add_conditional_edges(\n",
    "    \"llm_call_router\",\n",
    "    route_decision,\n",
    "    {  # 由 route_decision 返回的名称：要访问的下一个节点的名称\n",
    "        \"llm_call_1\": \"llm_call_1\",\n",
    "        \"llm_call_2\": \"llm_call_2\",\n",
    "        \"llm_call_3\": \"llm_call_3\",\n",
    "    },\n",
    ") # 从路由器到专用节点的条件边缘，基于路由决策\n",
    "router_builder.add_edge(\"llm_call_1\", END) # 专用节点连接到结束节点\n",
    "router_builder.add_edge(\"llm_call_2\", END)\n",
    "router_builder.add_edge(\"llm_call_3\", END)\n",
    "\n",
    "# 编译路由工作流图\n",
    "router_workflow = router_builder.compile()\n",
    "\n",
    "# 使用示例输入调用路由工作流\n",
    "state = router_workflow.invoke({\"input\": \"给我写一个关于猫的笑话\"})\n",
    "print(state[\"output\"]) \n",
    "# 保存图像\n",
    "# 保存 Mermaid 生成的 PNG 图像到文件\n",
    "from draw import save_graph_as_png\n",
    "save_graph_as_png(router_workflow, \"./graphs/c7/7-3router_workflow.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explanation-7-3",
   "metadata": {},
   "source": [
    "**💡 路由机制关键特性**：\n",
    "\n",
    "- **智能分类**：路由器节点使用 LLM 的理解能力来分析输入意图\n",
    "- **结构化输出**：使用 JSON 格式确保路由决策的可靠性\n",
    "- **专门处理**：每个路由目标都有专门的处理逻辑\n",
    "- **可扩展性**：易于添加新的路由目标和处理器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example-7-4",
   "metadata": {},
   "source": [
    "##### 示例 7-4：基于 Functional API 的路由工作流实现\n",
    "\n",
    "让我们用 Functional API 实现相同的路由逻辑："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langgraph.func import entrypoint, task\n",
    "\n",
    "llm = ChatOpenAI(model=\"Qwen/Qwen2.5-7B-Instruct\")\n",
    "\n",
    "@task\n",
    "def llm_call_1(input: str):\n",
    "    \"\"\"写一个故事\"\"\"\n",
    "    result = llm.invoke(input) # 调用 LLM 根据输入写一个故事\n",
    "    return result.content # 返回故事\n",
    "\n",
    "@task\n",
    "def llm_call_2(input: str):\n",
    "    \"\"\"写一个笑话\"\"\"\n",
    "    result = llm.invoke(input) # 调用 LLM 根据输入写一个笑话\n",
    "    return result.content # 返回笑话\n",
    "\n",
    "@task\n",
    "def llm_call_3(input: str):\n",
    "    \"\"\"写一首诗\"\"\"\n",
    "    result = llm.invoke(input) # 调用 LLM 根据输入写一首诗\n",
    "    return result.content # 返回诗歌\n",
    "\n",
    "def llm_call_router(input: str):\n",
    "    \"\"\"使用结构化输出将输入路由到适当的节点\"\"\"\n",
    "    # 使用结构化输出调用增强型 LLM，以充当路由逻辑\n",
    "    model = ChatOpenAI(model=\"Qwen/Qwen2.5-7B-Instruct\", model_kwargs={ \"response_format\": { \"type\": \"json_object\" } })\n",
    "    ai_msg = model.invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=\"You are a router that directs user input to the appropriate handler. Return a JSON object with a 'step' key and one of these values: 'story', 'joke', or 'poem'. For example: {'step': 'joke'}\" # 路由 LLM 的系统消息\n",
    "            ),\n",
    "            HumanMessage(content=input), # 用户输入消息\n",
    "        ]\n",
    "    )\n",
    "    decision = json.loads(ai_msg.content)\n",
    "    return {\"decision\": decision[\"step\"]} # 返回路由决策\n",
    "\n",
    "# 入口点装饰函数定义路由工作流\n",
    "@entrypoint()\n",
    "def router_workflow(input: str):\n",
    "    next_step = llm_call_router(input)[\"decision\"] # 获取路由决策的 'decision' 值\n",
    "    llm_call = None # 初始化 llm_call 变量\n",
    "    \n",
    "    if next_step == \"story\": # 基于路由器决策的条件路由\n",
    "        llm_call = llm_call_1 # 如果路由是 'story'，则分配 'llm_call_1' 任务\n",
    "    elif next_step == \"joke\":\n",
    "        llm_call = llm_call_2 # 如果路由是 'joke'，则分配 'llm_call_2' 任务\n",
    "    elif next_step == \"poem\":\n",
    "        llm_call = llm_call_3 # 如果路由是 'poem'，则分配 'llm_call_3' 任务\n",
    "        \n",
    "    if llm_call is None:\n",
    "        raise ValueError(f\"Invalid routing decision: {next_step}\")\n",
    "        \n",
    "    return llm_call(input) # 执行选定的 LLM 调用任务并返回结果\n",
    "\n",
    "# 调用路由工作流\n",
    "for step in router_workflow.stream(\"给我写一个关于猫的笑话\", stream_mode=\"updates\"):\n",
    "    print(step)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7-1-4",
   "metadata": {},
   "source": [
    "### 7.1.4 并行化 (Parallelization)\n",
    "\n",
    "并行化是另一种工作流模式，它利用增强型 LLM 同时处理任务不同方面的能力。并行化不是按顺序处理任务，而是允许同时进行 LLM 调用，其输出稍后以编程方式聚合。\n",
    "\n",
    "这种方法可以体现在两个主要变体中：\n",
    "- **分段 (Sectioning)**：将任务分解为可以并行执行的独立子任务\n",
    "- **投票 (Voting)**：多次运行相同的任务以获得更稳健和可靠的结果\n",
    "\n",
    "并行化的主要好处是提高了效率，尤其是在子任务真正独立并且可以并发处理的情况下。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example-7-5",
   "metadata": {},
   "source": [
    "##### 示例 7-5：基于 Graph API 的并行化工作流实现\n",
    "\n",
    "让我们实现一个并行内容生成工作流，同时生成故事、笑话和诗歌："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是一个关于 cats 的故事、笑话和诗歌！\n",
      "\n",
      "故事：\n",
      "在一个宁静的小镇上，住着一只名叫小白的猫。小白是一只可爱的白色长毛猫，眼睛像两颗绿宝石，她的毛发在阳光下闪闪发光。小白每天都在小镇的街道上漫步，探索每一个角落，跟镇上的人们打招呼。\n",
      "\n",
      "小白最喜欢的地方是镇中心的一家小咖啡馆，咖啡馆的老板是一位和蔼的老奶奶，她每次看到小白都会给她一小碗牛奶。小白不仅喜欢牛奶，更喜欢坐在咖啡馆的窗边，观察街上来来往往的人和车，听人们的欢声笑语。\n",
      "\n",
      "有一天，小白在咖啡馆的窗边，突然注意到一个小女孩坐在街角，神情沮丧。小白心里一动，决定去看看发生了什么事。她轻巧地跳下窗台，走到小女孩的身边。小女孩抬起头，看到小白，脸上露出了微微的笑容。\n",
      "\n",
      "“你好，小猫咪。”小女孩轻声说道，“你知道吗？我丢了我的风筝。”她指着天空，几朵白云正悠悠荡荡，风筝早已飞得不见踪影。\n",
      "\n",
      "小白听了，心里想：“要是我能帮助她找到风筝该多好！”于是，她开始在周围四处探索，试图找到风筝的踪迹。小白穿过花坛、爬上了矮墙，把目光投向更高的地方，希望能找到那只风筝。\n",
      "\n",
      "就在这时，小白在一棵大树的树枝上发现了那个五彩斑斓的风筝！她兴奋地“喵喵”叫了起来，吸引了路过的行人。人们看到小白在树下仰望，纷纷停下脚步，跟随她的目光。\n",
      "\n",
      "小女孩听到小白的叫声，抬头一看，立刻惊喜地跳了起来：“我的风筝！谢谢你，小猫咪！”她连忙跑到树下，和周围的人一起想办法把风筝拿下来。最后，几位好心的邻居用长杆将风筝轻松地取了下来。\n",
      "\n",
      "小女孩高兴地抱住了风筝，脸上洋溢着灿烂的笑容。她蹲下身，轻轻抚摸着小白的头：“谢谢你，小白，你真是一只了不起的猫咪！”小白也开心地用头蹭了蹭小女孩的手，心里暖暖的。\n",
      "\n",
      "从那天起，小白和小女孩成了好朋友。每天，小女孩都会带着小白一起到咖啡馆，和她分享美味的点心和牛奶。小白也变得更加开朗，陪伴着小女孩度过了许多个快乐的日子。\n",
      "\n",
      "小镇上的人们都知道了小白和小女孩的故事，他们都说：“这只猫咪不仅是我们的朋友，还是小女孩的幸运星！”小白在小镇上留下了一段美好的回忆，也明白了友谊的珍贵。\n",
      "\n",
      "笑话：\n",
      "为什么猫咪总是坐在电脑键盘上？\n",
      "\n",
      "因为它们喜欢“按猫”键！\n",
      "\n",
      "诗歌：\n",
      "在静谧的夜晚，月光柔和，  \n",
      "小猫悄然漫步，似乎在歌唱。  \n",
      "它们的眼睛明亮，璀璨如星，  \n",
      "在黑幕中闪烁，若隐若现的灵。\n",
      "\n",
      "毛发如丝绸般，温暖又轻柔，  \n",
      "每个轻巧的步伐，都是一场舞蹈。  \n",
      "它们在窗边守候，梦境的守护者，  \n",
      "或在阳光下打盹，沉醉于温暖的阳光。\n",
      "\n",
      "咪咪的呼唤，如微风细语，  \n",
      "充满了温情，分享我的心语。  \n",
      "愉悦的小爪子，轻轻地抚摸，  \n",
      "在生活的每个角落，增添一份欢愉。\n",
      "\n",
      "它们是家中的精灵，探索的冒险者，  \n",
      "无声的伴侣，给予我无尽的柔和。  \n",
      "猫咪的世界，神秘而奇妙，  \n",
      "每一个瞬间，都是爱的召唤。  \n",
      "\n",
      "当夜幕降临，灯光渐暗，  \n",
      "我静静守候，与你共度时光。  \n",
      "在这温暖的瞬间，我轻轻低语，  \n",
      "我心中的猫咪，你是我灵魂的一部分。  \n"
     ]
    }
   ],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "from llm_utils import llm\n",
    "# 并行工作流的图状态定义\n",
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    joke: str\n",
    "    story: str\n",
    "    poem: str\n",
    "    combined_output: str\n",
    "\n",
    "\n",
    "\n",
    "# 图中的节点，每个节点并行生成不同类型的内容\n",
    "def call_llm_1(state: State):\n",
    "    \"\"\"第一个 LLM 调用，生成初始笑话\"\"\"\n",
    "    msg = llm.invoke(f\"写一个关于 {state['topic']} 的笑话\") # 调用 LLM 以根据主题写一个笑话\n",
    "    return {\"joke\": msg.content} # 返回笑话，更新状态中的 'joke'\n",
    "\n",
    "def call_llm_2(state: State):\n",
    "    \"\"\"第二个 LLM 调用，生成故事\"\"\"\n",
    "    msg = llm.invoke(f\"写一个关于 {state['topic']} 的故事\") # 调用 LLM 以根据主题写一个故事\n",
    "    return {\"story\": msg.content} # 返回故事，更新状态中的 'story'\n",
    "\n",
    "def call_llm_3(state: State):\n",
    "    \"\"\"第三个 LLM 调用，生成诗歌\"\"\"\n",
    "    msg = llm.invoke(f\"写一首关于 {state['topic']} 的诗歌\") # 调用 LLM 以根据主题写一首诗\n",
    "    return {\"poem\": msg.content} # 返回诗歌，更新状态中的 'poem'\n",
    "\n",
    "def aggregator(state: State):\n",
    "    \"\"\"将笑话、故事和诗歌组合成单个输出\"\"\"\n",
    "    combined = f\"这是一个关于 {state['topic']} 的故事、笑话和诗歌！\\n\\n\" # 开始组合输出\n",
    "    combined += f\"故事：\\n{state['story']}\\n\\n\" # 将故事添加到组合输出\n",
    "    combined += f\"笑话：\\n{state['joke']}\\n\\n\" # 将笑话添加到组合输出\n",
    "    combined += f\"诗歌：\\n{state['poem']}\" # 将诗歌添加到组合输出\n",
    "    return {\"combined_output\": combined} # 返回组合输出，更新状态中的 'combined_output'\n",
    "\n",
    "# 使用 StateGraph 构建并行工作流\n",
    "parallel_builder = StateGraph(State)\n",
    "\n",
    "# 将节点添加到图中\n",
    "parallel_builder.add_node(\"call_llm_1\", call_llm_1)\n",
    "parallel_builder.add_node(\"call_llm_2\", call_llm_2)\n",
    "parallel_builder.add_node(\"call_llm_3\", call_llm_3)\n",
    "parallel_builder.add_node(\"aggregator\", aggregator)\n",
    "\n",
    "# 定义边缘以连接节点并建立并行执行\n",
    "parallel_builder.add_edge(START, \"call_llm_1\") # 开始节点连接到 'call_llm_1' 以进行并行执行\n",
    "parallel_builder.add_edge(START, \"call_llm_2\") # 开始节点连接到 'call_llm_2' 以进行并行执行\n",
    "parallel_builder.add_edge(START, \"call_llm_3\") # 开始节点连接到 'call_llm_3' 以进行并行执行\n",
    "parallel_builder.add_edge(\"call_llm_1\", \"aggregator\") # 'call_llm_1' 节点在完成后连接到聚合器\n",
    "parallel_builder.add_edge(\"call_llm_2\", \"aggregator\") # 'call_llm_2' 节点在完成后连接到聚合器\n",
    "parallel_builder.add_edge(\"call_llm_3\", \"aggregator\") # 'call_llm_3' 节点在完成后连接到聚合器\n",
    "parallel_builder.add_edge(\"aggregator\", END) # 聚合器节点连接到结束节点\n",
    "\n",
    "# 编译并行工作流图\n",
    "parallel_workflow = parallel_builder.compile()\n",
    "\n",
    "# 使用示例输入调用并行工作流\n",
    "state = parallel_workflow.invoke({\"topic\": \"cats\"})\n",
    "print(state[\"combined_output\"]) \n",
    "\n",
    "# 保存图像\n",
    "from draw import save_graph_as_png\n",
    "save_graph_as_png(parallel_workflow, \"./graphs/c7/Parallelization_workflow.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explanation-7-5",
   "metadata": {},
   "source": [
    "**💡 并行化的关键优势**：\n",
    "\n",
    "- **效率提升**：多个 LLM 调用同时执行，减少总体等待时间\n",
    "- **独立处理**：每个任务专注于特定方面，避免上下文混乱\n",
    "- **结果聚合**：通过聚合器节点组合所有结果\n",
    "- **可扩展性**：易于添加更多并行处理分支"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example-7-6",
   "metadata": {},
   "source": [
    "##### 示例 7-6：基于 Functional API 的并行化工作流实现\n",
    "\n",
    "让我们用 Functional API 实现相同的并行处理逻辑："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.func import entrypoint, task\n",
    "\n",
    "llm = ChatOpenAI(model=\"Qwen/Qwen3-8B\")\n",
    "\n",
    "@task\n",
    "def call_llm_1(topic: str):\n",
    "    \"\"\"第一个 LLM 调用，生成初始笑话\"\"\"\n",
    "    msg = llm.invoke(f\"写一个关于 {topic} 的笑话\") # 调用 LLM 以根据主题写一个笑话\n",
    "    return msg.content # 返回笑话\n",
    "\n",
    "@task\n",
    "def call_llm_2(topic: str):\n",
    "    \"\"\"第二个 LLM 调用，生成故事\"\"\"\n",
    "    msg = llm.invoke(f\"写一个关于 {topic} 的故事\") # 调用 LLM 以根据主题写一个故事\n",
    "    return msg.content # 返回故事\n",
    "\n",
    "@task\n",
    "def call_llm_3(topic):\n",
    "    \"\"\"第三个 LLM 调用，生成诗歌\"\"\"\n",
    "    msg = llm.invoke(f\"写一首关于 {topic} 的诗歌\") # 调用 LLM 以根据主题写一首诗\n",
    "    return msg.content # 返回诗歌\n",
    "\n",
    "@task\n",
    "def aggregator(topic, joke, story, poem):\n",
    "    \"\"\"将笑话和故事组合成单个输出\"\"\"\n",
    "    combined = f\"这是一个关于 {topic} 的故事、笑话和诗歌！\\n\\n\" # 开始组合输出\n",
    "    combined += f\"故事：\\n{story}\\n\\n\" # 将故事添加到组合输出\n",
    "    combined += f\"笑话：\\n{joke}\\n\\n\" # 将笑话添加到组合输出\n",
    "    combined += f\"诗歌：\\n{poem}\" # 将诗歌添加到组合输出\n",
    "    return combined # 返回组合输出\n",
    "\n",
    "# 入口点装饰函数定义并行工作流\n",
    "@entrypoint()\n",
    "def parallel_workflow(topic: str):\n",
    "    joke_fut = call_llm_1(topic) # 执行 'call_llm_1' 任务并获取期货以进行并行执行\n",
    "    story_fut = call_llm_2(topic) # 执行 'call_llm_2' 任务并获取期货以进行并行执行\n",
    "    poem_fut = call_llm_3(topic) # 执行 'call_llm_3' 任务并获取期货以进行并行执行\n",
    "    return aggregator(\n",
    "        topic, joke_fut.result(), story_fut.result(), poem_fut.result() # 在所有并行任务完成后执行 'aggregator' 任务\n",
    "    ).result() # 从聚合器获取最终结果\n",
    "\n",
    "# 调用并行工作流\n",
    "for step in parallel_workflow.stream(\"cats\", stream_mode=\"updates\"):\n",
    "    print(step)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "todo-update-1",
   "metadata": {},
   "source": [
    "**💡 Functional API 中的并行处理**：\n",
    "\n",
    "- **Future 对象**：通过 `.result()` 方法等待异步任务完成\n",
    "- **简洁语法**：并行执行通过同时启动多个任务实现\n",
    "- **自动同步**：聚合器会等待所有前置任务完成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dj67jjxa5xq",
   "metadata": {},
   "source": [
    "### 7.1.5 协调器-工作者 (Orchestrator-Worker)\n",
    "\n",
    "协调器-工作者工作流模式专为子任务需求事先未知且需要在执行期间动态确定的复杂任务而设计。在此模式中，中央增强型 LLM 充当\"协调器 (Orchestrator)\"，负责将初始任务分解为更小、更易于管理的子任务，并将这些子任务委派给\"工作者 (Worker)\"增强型 LLM。\n",
    "\n",
    "此工作流特别适用于难以或不可能预先预测必要子任务的复杂场景，例如编码任务、复杂搜索任务等。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yhnp7oyv2k8",
   "metadata": {},
   "source": [
    "##### 示例 7-7：基于 Graph API 的\"协调器-工作者\"工作流实现\n",
    "\n",
    "让我们实现一个报告生成的协调器-工作者系统："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ppbtmr227s",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, List, TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import Send\n",
    "\n",
    "\n",
    "# 用于结构化输出的模式，用于规划报告章节\n",
    "class Section(BaseModel):\n",
    "    name: str = Field(\n",
    "        description=\"报告章节的名称\", # 报告章节的名称\n",
    "    )\n",
    "    description: str = Field(\n",
    "        description=\"本章节中要涵盖的主要主题和概念的简要概述\", # 章节内容的描述\n",
    "    )\n",
    "\n",
    "class Sections(BaseModel):\n",
    "    sections: List[Section] = Field(\n",
    "        description=\"报告的章节\", # 报告章节列表\n",
    "    )\n",
    "\n",
    "# 用于规划报告章节的增强型 LLM，使用结构化输出\n",
    "llm = ChatOpenAI(model=\"Qwen/Qwen2.5-7B-Instruct\")\n",
    "planner = llm.with_structured_output(Sections, method=\"function_calling\")\n",
    "\n",
    "# 协调器-工作者工作流的图状态定义\n",
    "class State(TypedDict):\n",
    "    topic: str  # 报告主题\n",
    "    sections: list[Section]  # 由协调器规划的报告章节列表\n",
    "    completed_sections: Annotated[\n",
    "        list, operator.add\n",
    "    ]  # 所有工作者并行写入此键，使用 operator.add 进行列表连接\n",
    "    final_report: str  # 最终合成报告\n",
    "\n",
    "\n",
    "# 工作者状态定义，特定于工作者节点\n",
    "class WorkerState(TypedDict):\n",
    "    section: Section\n",
    "    completed_sections: Annotated[list, operator.add] # 工作者也写入共享的 'completed_sections' 键\n",
    "\n",
    "# 图中的节点\n",
    "def orchestrator(state: State):\n",
    "    \"\"\"协调器，使用结构化输出生成报告计划\"\"\"\n",
    "    # 使用 planner LLM 和结构化输出生成报告章节计划\n",
    "    report_sections = planner.invoke(\n",
    "        [\n",
    "            SystemMessage(content=\"生成报告计划。\"), # planner LLM 的系统消息\n",
    "            HumanMessage(content=f\"这是报告主题：{state['topic']}\"), # 包含报告主题的用户输入消息\n",
    "        ]\n",
    "    )\n",
    "    return {\"sections\": report_sections.sections} # 返回计划的章节，更新状态中的 'sections'\n",
    "\n",
    "def llm_call(state: WorkerState):\n",
    "    \"\"\"工作者根据分配的章节详细信息编写报告章节\"\"\"\n",
    "    # 使用 LLM 根据章节名称和描述生成报告章节内容\n",
    "    section = llm.invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=\"按照提供的名称和描述编写报告章节。每节不包含序言。使用 markdown 格式。\" # 工作者 LLM 的系统消息\n",
    "            ),\n",
    "            HumanMessage(\n",
    "                content=f\"这是章节名称：{state['section'].name} 和描述：{state['section'].description}\" # 包含章节详细信息的用户消息\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    # 将生成的章节内容写入共享的 'completed_sections' 键\n",
    "    return {\"completed_sections\": [section.content]}\n",
    "\n",
    "def synthesizer(state: State):\n",
    "    \"\"\"从各个章节输出合成完整报告\"\"\"\n",
    "    # 从共享状态检索已完成章节的列表\n",
    "    completed_sections = state[\"completed_sections\"]\n",
    "\n",
    "    # 将已完成章节格式化为单个字符串以用于最终报告\n",
    "    completed_report_sections = \"\\n\\n---\\n\\n\".join(completed_sections)\n",
    "\n",
    "    return {\"final_report\": completed_report_sections} # 返回最终报告，更新状态中的 'final_report'\n",
    "\n",
    "# 条件边缘函数，用于将工作者动态分配给计划中的每个章节\n",
    "def assign_workers(state: State):\n",
    "    \"\"\"使用 Send API 将工作者分配给计划中的每个章节，以实现动态工作者创建\"\"\"\n",
    "    # 使用 Send API 为每个章节动态创建和发送 'llm_call' 工作者节点\n",
    "    return [Send(\"llm_call\", {\"section\": s}) for s in state[\"sections\"]]\n",
    "\n",
    "# 使用 StateGraph 构建协调器-工作者工作流\n",
    "orchestrator_worker_builder = StateGraph(State)\n",
    "\n",
    "# 将节点添加到图中\n",
    "orchestrator_worker_builder.add_node(\"orchestrator\", orchestrator)\n",
    "orchestrator_worker_builder.add_node(\"llm_call\", llm_call) # 工作者节点\n",
    "orchestrator_worker_builder.add_node(\"synthesizer\", synthesizer)\n",
    "\n",
    "# 定义边缘以连接节点并建立协调器-工作者流程\n",
    "orchestrator_worker_builder.add_edge(START, \"orchestrator\") # 开始节点连接到协调器\n",
    "orchestrator_worker_builder.add_conditional_edges(\n",
    "    \"orchestrator\", assign_workers, [\"llm_call\"] # 从协调器到使用 Send API 动态创建的工作者节点的条件边缘\n",
    ")\n",
    "orchestrator_worker_builder.add_edge(\"llm_call\", \"synthesizer\") # 工作者节点在完成后连接到合成器\n",
    "orchestrator_worker_builder.add_edge(\"synthesizer\", END) # 合成器节点连接到结束节点\n",
    "\n",
    "# 编译协调器-工作者工作流图\n",
    "orchestrator_worker = orchestrator_worker_builder.compile()\n",
    "\n",
    "# 使用示例报告主题调用协调器-工作者工作流\n",
    "state = orchestrator_worker.invoke({\"topic\": \"创建关于 LLM 缩放定律的报告\"})\n",
    "\n",
    "from IPython.display import Markdown\n",
    "Markdown(state[\"final_report\"]) # 以 Markdown 格式显示最终报告"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ywhte0757f",
   "metadata": {},
   "source": [
    "**💡 协调器-工作者的关键特性**：\n",
    "\n",
    "- **动态任务分解**：协调器根据输入动态生成子任务\n",
    "- **Send API**：允许运行时动态创建工作者节点\n",
    "- **共享状态**：使用 `operator.add` 聚合多个工作者的输出\n",
    "- **结构化输出**：使用 Pydantic 模型确保任务规划的一致性\n",
    "- **灵活扩展**：可以根据任务复杂性创建任意数量的工作者"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfouwqfokhl",
   "metadata": {},
   "source": [
    "##### 示例 7-8：基于 Functional API 的\"协调器-工作者\"工作流实现\n",
    "\n",
    "让我们用 Functional API 实现相同的协调器-工作者逻辑："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lbvlglvna5n",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langgraph.func import entrypoint, task\n",
    "\n",
    "\n",
    "# 用于结构化输出的模式，用于规划报告章节\n",
    "class Section(BaseModel):\n",
    "    name: str = Field(\n",
    "        description=\"报告章节的名称\", # 报告章节的名称\n",
    "    )\n",
    "    description: str = Field(\n",
    "        description=\"本章节中要涵盖的主要主题和概念的简要概述\", # 章节内容的描述\n",
    "    )\n",
    "\n",
    "class Sections(BaseModel):\n",
    "    sections: List[Section] = Field(\n",
    "        description=\"报告的章节\", # 报告章节列表\n",
    "    )\n",
    "\n",
    "# 用于规划报告章节的增强型 LLM，使用结构化输出\n",
    "llm = ChatOpenAI(model=\"Qwen/Qwen2.5-7B-Instruct\")\n",
    "planner = llm.with_structured_output(Sections, method=\"function_calling\")\n",
    "\n",
    "@task\n",
    "def orchestrator(topic: str):\n",
    "    \"\"\"协调器，使用结构化输出生成报告计划\"\"\"\n",
    "    # 使用 planner LLM 和结构化输出生成报告章节计划\n",
    "    report_sections = planner.invoke(\n",
    "        [\n",
    "            SystemMessage(content=\"生成报告计划。\"), # planner LLM 的系统消息\n",
    "            HumanMessage(content=f\"这是报告主题：{topic}\"), # 包含报告主题的用户消息\n",
    "        ]\n",
    "    )\n",
    "    return report_sections.sections # 返回计划的章节\n",
    "\n",
    "@task\n",
    "def llm_call(section: Section):\n",
    "    \"\"\"工作者根据分配的章节详细信息编写报告章节\"\"\"\n",
    "    # 使用 LLM 根据章节名称和描述生成报告章节内容\n",
    "    result = llm.invoke(\n",
    "        [\n",
    "            SystemMessage(content=\"编写报告章节。\"), # 工作者 LLM 的系统消息\n",
    "            HumanMessage(\n",
    "                content=f\"这是章节名称：{section.name} 和描述：{section.description}\" # 包含章节详细信息的用户消息\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    return result.content # 返回生成的章节内容\n",
    "\n",
    "@task\n",
    "def synthesizer(completed_sections: list[str]):\n",
    "    \"\"\"从各个章节输出合成完整报告\"\"\"\n",
    "    # 将已完成章节格式化为单个字符串以用于最终报告\n",
    "    final_report = \"\\n\\n---\\n\\n\".join(completed_sections)\n",
    "    return final_report # 返回最终报告\n",
    "\n",
    "# 入口点装饰函数定义协调器-工作者工作流\n",
    "@entrypoint()\n",
    "def orchestrator_worker(topic: str):\n",
    "    sections = orchestrator(topic).result() # 执行协调器任务以获取报告章节计划\n",
    "    section_futures = [llm_call(section) for section in sections] # 并行动态创建和执行每个章节的工作者任务\n",
    "    final_report = synthesizer(\n",
    "        [section_fut.result() for section_fut in section_futures] # 在所有工作者任务完成后执行合成器任务\n",
    "    ).result() # 获取最终合成报告\n",
    "\n",
    "    return final_report # 返回最终报告\n",
    "\n",
    "# 使用示例报告主题调用协调器-工作者工作流\n",
    "report = orchestrator_worker.invoke(\"创建关于 LLM 缩放定律的报告\")\n",
    "\n",
    "from IPython.display import Markdown\n",
    "Markdown(report) # 以 Markdown 格式显示最终报告"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20lpfn9iizf",
   "metadata": {},
   "source": [
    "### 7.1.6 评估器-优化器 (Evaluator-Optimizer)\n",
    "\n",
    "\"评估器-优化器\"工作流体现了一种迭代改进过程，模仿了人类通常通过反馈和修订来改进其工作的方式。在此模式中，一个增强型 LLM 调用负责生成初始响应，而另一个增强型 LLM 调用（\"评估器 (Evaluator)\"）的任务是提供对此响应的反馈。\n",
    "\n",
    "可以重复进行生成、评估和反馈的循环多次，直到获得令人满意的结果或达到预定义的迭代次数。当存在可以明确表达和评估的明确评估标准，并且迭代改进能够显著增加输出价值时，评估器-优化器工作流尤其有效。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9djskjxb3nv",
   "metadata": {},
   "source": [
    "##### 示例 7-9：基于 Graph API 的\"评估器-优化器\"工作流实现\n",
    "\n",
    "让我们实现一个笑话生成和改进的评估器-优化器系统："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bqncuzcyxkw",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "好的，以下是一个关于性的话题笑话，结合了比喻和双关语：\n",
      "\n",
      "为什么健身房的情侣总是很少？\n",
      "\n",
      "因为他们一直在“锻炼感情”，但每次都把“重心”放在了举重上，没时间去“抬头”看看彼此的心意！\n"
     ]
    }
   ],
   "source": [
    "from typing_extensions import TypedDict, Literal\n",
    "from pydantic import BaseModel, Field\n",
    "from llm_utils import llm\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "\n",
    "# 用于结构化输出的模式，用于评估，定义反馈结构\n",
    "class Feedback(BaseModel):\n",
    "    grade: Literal[\"funny\", \"not funny\"] = Field(\n",
    "        description=\"判断笑话是否有趣。\", # 评估等级：有趣或不好笑\n",
    "    )\n",
    "    feedback: str = Field(\n",
    "        description=\"如果笑话不好笑，请提供有关如何改进它的反馈。\", # 如果笑话不好笑，则提供关于如何改进它的反馈\n",
    "    )\n",
    "\n",
    "# 用于评估的增强型 LLM，配置为输出 Feedback 模式\n",
    "evaluator = llm.with_structured_output(Feedback)\n",
    "\n",
    "# 评估器-优化器工作流的图状态定义\n",
    "class State(TypedDict):\n",
    "    joke: str\n",
    "    topic: str\n",
    "    feedback: str\n",
    "    funny_or_not: str\n",
    "\n",
    "# 图中的节点\n",
    "def llm_call_generator(state: State):\n",
    "    \"\"\"LLM 生成笑话，可能会结合之前评估的反馈\"\"\"\n",
    "    if state.get(\"feedback\"): # 检查状态中是否存在反馈，指示之前的评估\n",
    "        msg = llm.invoke(\n",
    "            f\"写一个关于 {state['topic']} 的笑话，但要考虑反馈：{state['feedback']}\" # 调用 LLM 生成笑话，结合反馈\n",
    "        )\n",
    "    else:\n",
    "        msg = llm.invoke(f\"写一个关于 {state['topic']} 的笑话\") # 调用 LLM 生成初始笑话，不带反馈\n",
    "    return {\"joke\": msg.content} # 返回生成的笑话，更新状态中的 'joke'\n",
    "\n",
    "def llm_call_evaluator(state: State):\n",
    "    \"\"\"LLM 使用结构化输出评估生成的笑话\"\"\"\n",
    "    grade = evaluator.invoke(f\"评价笑话 {state['joke']}\") # 调用评估器 LLM 来评价笑话\n",
    "    return {\"funny_or_not\": grade.grade, \"feedback\": grade.feedback} # 返回评估等级和反馈，更新状态中的 'funny_or_not' 和 'feedback'\n",
    "\n",
    "# 条件边缘函数，用于根据评估结果进行路由，创建反馈循环\n",
    "def route_joke(state: State):\n",
    "    \"\"\"根据评估器的反馈，路由回笑话生成器或结束\"\"\"\n",
    "    if state[\"funny_or_not\"] == \"funny\": # 检查笑话是否被评估为有趣\n",
    "        return \"Accepted\" # 如果笑话被接受，则路由到结束\n",
    "    elif state[\"funny_or_not\"] == \"not funny\": # 检查笑话是否被评估为不好笑\n",
    "        return \"Rejected + Feedback\" # 如果笑话被拒绝，则路由回生成器以结合反馈\n",
    "\n",
    "# 构建评估器-优化器工作流，使用 StateGraph\n",
    "optimizer_builder = StateGraph(State)\n",
    "\n",
    "# 将节点添加到图中\n",
    "optimizer_builder.add_node(\"llm_call_generator\", llm_call_generator)\n",
    "optimizer_builder.add_node(\"llm_call_evaluator\", llm_call_evaluator)\n",
    "\n",
    "# 定义边缘以连接节点并建立反馈循环\n",
    "optimizer_builder.add_edge(START, \"llm_call_generator\") # 开始节点连接到笑话生成器\n",
    "optimizer_builder.add_edge(\"llm_call_generator\", \"llm_call_evaluator\") # 生成器节点连接到评估器节点\n",
    "optimizer_builder.add_conditional_edges(\n",
    "    \"llm_call_evaluator\",\n",
    "    route_joke,\n",
    "    {  # 由 route_joke 返回的名称：要访问的下一个节点的名称\n",
    "        \"Accepted\": END, # 如果笑话被接受，则路由到结束\n",
    "        \"Rejected + Feedback\": \"llm_call_generator\", # 如果笑话被拒绝，则路由回生成器，创建反馈循环\n",
    "    },\n",
    ")\n",
    "\n",
    "# 编译评估器-优化器工作流图\n",
    "optimizer_workflow = optimizer_builder.compile()\n",
    "\n",
    "# 使用示例主题调用评估器-优化器工作流\n",
    "state = optimizer_workflow.invoke({\"topic\": \"sex\"})\n",
    "print(state[\"joke\"]) \n",
    "\n",
    "# 保存 Mermaid 生成的 PNG 图像到文件\n",
    "# from draw import save_graph_as_png\n",
    "# save_graph_as_png(optimizer_workflow, \"./graphs/c7/evaluator_optimizer_workflow.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98i0hb88t8p",
   "metadata": {},
   "source": [
    "**💡 评估器-优化器的关键机制**：\n",
    "\n",
    "- **迭代改进**：通过反馈循环不断优化输出质量\n",
    "- **结构化评估**：使用 Pydantic 模型确保评估的一致性\n",
    "- **条件路由**：根据评估结果决定是否继续优化\n",
    "- **状态记忆**：保持反馈信息以指导后续改进\n",
    "- **质量门控**：只有达到标准的输出才会被接受"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xz8hbmaru4",
   "metadata": {},
   "source": [
    "##### 示例 7-10：基于 Functional API 的\"评估器-优化器\"工作流实现\n",
    "\n",
    "让我们用 Functional API 实现相同的评估器-优化器逻辑："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ta9y4a0uff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Literal\n",
    "from pydantic import BaseModel, Field\n",
    "from langgraph.func import entrypoint, task\n",
    "\n",
    "\n",
    "# 用于结构化输出的模式，用于评估，定义反馈结构\n",
    "class Feedback(BaseModel):\n",
    "    grade: Literal[\"funny\", \"not funny\"] = Field(\n",
    "        description=\"判断笑话是否有趣。\", # 评估等级：有趣或不好笑\n",
    "    )\n",
    "    feedback: str = Field(\n",
    "        description=\"如果笑话不好笑，请提供有关如何改进它的反馈。\", # 如果笑话不好笑，则提供关于如何改进它的反馈\n",
    "    )\n",
    "\n",
    "# 用于评估的增强型 LLM，使用结构化输出\n",
    "evaluator = llm.with_structured_output(Feedback, method=\"function_calling\")\n",
    "\n",
    "# 工作流中的节点，定义为任务\n",
    "@task\n",
    "def llm_call_generator(topic: str, feedback: str = None):\n",
    "    \"\"\"LLM 生成笑话，可能会结合反馈\"\"\"\n",
    "    if feedback: # 检查是否提供了反馈\n",
    "        msg = llm.invoke(\n",
    "            f\"写一个关于 {topic} 的笑话，但要考虑反馈：{feedback}\" # 调用 LLM 生成笑话，结合反馈\n",
    "        )\n",
    "    else:\n",
    "        msg = llm.invoke(f\"写一个关于 {topic} 的笑话\") # 调用 LLM 生成初始笑话，不带反馈\n",
    "    return msg.content # 返回生成的笑话\n",
    "\n",
    "@task\n",
    "def llm_call_evaluator(joke: str):\n",
    "    \"\"\"LLM 使用结构化输出评估生成的笑话\"\"\"\n",
    "    feedback = evaluator.invoke(f\"评价笑话 {joke}\") # 调用评估器 LLM 来评价笑话\n",
    "    return feedback # 返回评估反馈\n",
    "\n",
    "# 入口点装饰函数定义评估器-优化器工作流\n",
    "@entrypoint()\n",
    "def optimizer_workflow(topic: str):\n",
    "    feedback = None # 将反馈初始化为 None，用于第一次迭代\n",
    "    max_iterations = 3 # 设置最大迭代次数以避免无限循环\n",
    "    iteration = 0\n",
    "    \n",
    "    while iteration < max_iterations: # 迭代反馈循环的开始\n",
    "        joke = llm_call_generator(topic, feedback).result() # 执行生成器任务以创建笑话\n",
    "        evaluation = llm_call_evaluator(joke).result() # 执行评估器任务以评价笑话并获取反馈\n",
    "        \n",
    "        if evaluation.grade == \"funny\": # 检查笑话是否被评估为有趣\n",
    "            return joke # 如果笑话有趣，则返回笑话\n",
    "        \n",
    "        feedback = evaluation.feedback # 更新反馈用于下一次迭代\n",
    "        iteration += 1\n",
    "        \n",
    "    return joke # 如果达到最大迭代次数，返回最后的笑话\n",
    "\n",
    "# 调用评估器-优化器工作流\n",
    "result = optimizer_workflow.invoke(\"Cats\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jlnhvab1dfl",
   "metadata": {},
   "source": [
    "## 7.2 LangGraph 中的多智能体架构\n",
    "\n",
    "随着我们在构建 AI 智能体方面不断进步，我们旨在解决的任务的复杂性通常需要超越单一的、单片式的智能体设计。多智能体系统的概念因此变得非常宝贵。我们不再依赖单个智能体来处理所有事情，而是将我们的 AI 应用程序分解为一组更小、更专注的智能体，每个智能体都具有特定的职责和专业知识。\n",
    "\n",
    "多智能体系统的核心优势在于：\n",
    "\n",
    "- **模块化**：将复杂的智能体分解为更小的、独立的智能体简化了开发、测试和维护\n",
    "- **专业化**：使我们能够创建针对特定领域或任务量身定制的专家智能体\n",
    "- **受控通信**：允许开发人员显式定义和管理智能体如何通信、交换信息以及协调其行动\n",
    "\n",
    "LangGraph 为构建和编排多智能体系统提供了强大的框架。在本节中，我们将探索几种关键多智能体架构。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9mbea871jaq",
   "metadata": {},
   "source": [
    "### 7.2.1 主管架构\n",
    "\n",
    "主管（Supervisor）架构是多智能体系统中的核心模式，当需要明确的编排点来管理和指导任务流时，它尤其有效。在这种架构中，指定的主管智能体充当中央协调员的角色，监督和指导多个专业智能体的活动。\n",
    "\n",
    "LangChain 团队新推出的 LangGraph Supervisor 类库提供了一种简化的方式来创建基于主管的多智能体系统。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bs3i020xjc",
   "metadata": {},
   "source": [
    "##### 示例 7-11：使用 LangGraph Supervisor 类库搭建具有数学和研究智能体的主管架构\n",
    "\n",
    "首先安装必要的库，然后构建一个包含数学专家和研究专家的主管系统："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wo6v33fpkkj",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先安装 langgraph-supervisor 库（如果尚未安装）\n",
    "# !pip install langgraph-supervisor\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph_supervisor import create_supervisor\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from llm_utils import llm\n",
    "\n",
    "# 1. 为专业智能体定义工具：\n",
    "# 定义表示每个专业智能体工具的函数。\n",
    "# 'add' 和 'multiply' 用于数学智能体，'web_search' 用于研究智能体。\n",
    "def add(a: float, b: float) -> float:\n",
    "    \"\"\"添加两个数字。\"\"\"\n",
    "    return a + b\n",
    "\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"乘两个数字。\"\"\"\n",
    "    return a * b\n",
    "\n",
    "def web_search(query: str) -> str:\n",
    "    \"\"\"在网络上搜索信息。\"\"\"\n",
    "    return (\n",
    "        \"以下是 FAANG 公司 2024 年的员工人数：\\n\"\n",
    "        \"1. **Facebook (Meta)**: 67,317 名员工。\\n\"\n",
    "        \"2. **Apple**: 164,000 名员工。\\n\"\n",
    "        \"3. **Amazon**: 1,551,000 名员工。\\n\"\n",
    "        \"4. **Netflix**: 14,000 名员工。\\n\"\n",
    "        \"5. **Google (Alphabet)**: 181,269 名员工。\"\n",
    "    )\n",
    "\n",
    "# 2. 使用 create_react_agent 创建专业智能体：\n",
    "# 使用 LangGraph 的预构建 create_react_agent 创建每个专业智能体。\n",
    "# 每个智能体都配置了特定的模型、工具、名称和提示。\n",
    "math_agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=[add, multiply], # 数学智能体的工具是 'add' 和 'multiply'\n",
    "    name=\"math_expert\", # 标识数学智能体的名称\n",
    "    prompt=\"你是一名数学专家。始终一次使用一个工具。\" # 指导数学智能体行为的提示\n",
    ")\n",
    "\n",
    "research_agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=[web_search], # 研究智能体的工具是 'web_search'\n",
    "    name=\"research_expert\", # 标识研究智能体的名称\n",
    "    prompt=\"你是一名世界一流的研究员，可以访问网络搜索。不要做任何数学运算。\" # 指导研究智能体行为的提示\n",
    ")\n",
    "\n",
    "# 3. 使用 create_supervisor 创建主管工作流：\n",
    "# 使用 LangGraph Supervisor 的 create_supervisor 创建主管工作流。\n",
    "# 传递专业智能体列表、主管模型以及主管的提示。\n",
    "workflow = create_supervisor(\n",
    "    [research_agent, math_agent], # 由主管管理的专业智能体列表\n",
    "    model=llm, # 主管智能体的模型\n",
    "    prompt=\"你是一名团队主管，管理着一名研究专家和一名数学专家。研究专家能够利用网络搜索工具进行查询。\", # 指导主管行为的提示\n",
    ")\n",
    "\n",
    "# 4. 编译并运行工作流：\n",
    "# 将工作流编译为 LangGraph 应用程序，并使用用户消息调用它。\n",
    "app = workflow.compile() # 将工作流编译为可执行的 LangGraph 应用程序\n",
    "result = app.invoke({ # 使用用户消息调用已编译的应用程序\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"2024 年 FAANG 公司的总员工人数是多少？\" # 用户关于 FAANG 公司员工人数的查询\n",
    "        }\n",
    "    ]\n",
    "})\n",
    "print(result[\"messages\"][-1].content)\n",
    "# 保存图\n",
    "from draw import save_graph_as_png\n",
    "save_graph_as_png(app, \"./graphs/c7/supervisor_workflow.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6hq5q5mc0ad",
   "metadata": {},
   "source": [
    "### 7.2.2 层次化架构 (Hierarchical Architecture)\n",
    "\n",
    "层次化架构通过多层次的智能体组织来处理复杂任务。每层都有特定的职责，上层智能体负责高级决策，下层智能体执行具体任务。\n",
    "\n",
    "**核心特点：**\n",
    "- 多层次组织结构\n",
    "- 清晰的职责分工\n",
    "- 层次间的通信机制\n",
    "- 递归的任务分解"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wcc56l188xg",
   "metadata": {},
   "source": [
    "##### 示例 7-12：具有研究和写作团队的分层主管系统"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3gqhtd6h1es",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph_supervisor import create_supervisor\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "from llm_utils import llm as model\n",
    "\n",
    "# --- 1. 定义研究团队智能体和主管 ---\n",
    "# 定义研究团队（math_expert 和 research_expert）的工具和智能体\n",
    "def add(a: float, b: float) -> float:\n",
    "    \"\"\"添加两个数字。\"\"\"\n",
    "    return a + b\n",
    "\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"乘两个数字。\"\"\"\n",
    "    return a * b\n",
    "\n",
    "def web_search(query: str) -> str:\n",
    "    \"\"\"在网络上搜索信息。\"\"\"\n",
    "    return (\n",
    "        \"以下是 FAANG 公司 2024 年的员工人数：\\n\"\n",
    "        \"1. **Facebook (Meta)**: 67,317 名员工。\\n\"\n",
    "        \"2. **Apple**: 164,000 名员工。\\n\"\n",
    "        \"3. **Amazon**: 1,551,000 名员工。\\n\"\n",
    "        \"4. **Netflix**: 14,000 名员工。\\n\"\n",
    "        \"5. **Google (Alphabet)**: 181,269 名员工。\"\n",
    "    )\n",
    "\n",
    "math_agent = create_react_agent(\n",
    "    model=model,\n",
    "    tools=[add, multiply],\n",
    "    name=\"math_expert\",\n",
    "    prompt=\"你是一名数学专家。\"\n",
    ")\n",
    "\n",
    "research_agent = create_react_agent(\n",
    "    model=model,\n",
    "    tools=[web_search],\n",
    "    name=\"research_expert\",\n",
    "    prompt=\"你是一名世界一流的研究员，可以访问网络搜索。不要做任何数学运算。\"\n",
    ")\n",
    "\n",
    "research_team_supervisor = create_supervisor(\n",
    "    [research_agent, math_agent], # 研究团队内的智能体\n",
    "    model=model,\n",
    "    prompt=\"你正在管理一个研究团队，该团队由研究和数学专家组成。研究专家能够利用网络搜索工具进行查询。\", # 研究团队主管的提示\n",
    ")\n",
    "research_team = research_team_supervisor.compile(name=\"research_team\") # 编译研究团队工作流并命名\n",
    "\n",
    "# --- 2. 定义写作团队智能体和主管 ---\n",
    "# 定义写作团队（writing_expert 和 publishing_expert）的工具和智能体\n",
    "def write_report(topic: str) -> str:\n",
    "    \"\"\"撰写关于给定主题的报告。\"\"\"\n",
    "    return f\"关于 {topic} 的报告：... （报告的详细内容）\"\n",
    "\n",
    "def publish_report(report: str) -> str:\n",
    "    \"\"\"发布报告。\"\"\"\n",
    "    return f\"报告已发布：{report}\"\n",
    "\n",
    "writing_agent = create_react_agent(\n",
    "    model=model,\n",
    "    tools=[write_report],\n",
    "    name=\"writing_expert\",\n",
    "    prompt=\"你是一名写作专家。\"\n",
    ")\n",
    "\n",
    "publishing_agent = create_react_agent(\n",
    "    model=model,\n",
    "    tools=[publish_report],\n",
    "    name=\"publishing_expert\",\n",
    "    prompt=\"你是一名出版专家。\"\n",
    ")\n",
    "\n",
    "writing_team_supervisor = create_supervisor(\n",
    "    [writing_agent, publishing_agent], # 写作团队内的智能体\n",
    "    model=model,\n",
    "    prompt=\"你正在管理一个写作团队，该团队由写作和出版专家组成。\", # 写作团队主管的提示\n",
    ")\n",
    "writing_team = writing_team_supervisor.compile(name=\"writing_team\") # 编译写作团队工作流并命名\n",
    "\n",
    "# --- 3. 定义顶层主管 ---\n",
    "# 定义顶层主管以管理研究团队和写作团队\n",
    "top_level_supervisor_agent = create_supervisor(\n",
    "    [research_team, writing_team], # 传递已编译的研究和写作团队工作流作为智能体\n",
    "    model=model,\n",
    "    prompt=\"你是一名顶层主管，管理着研究团队和写作团队。\", # 顶层主管的提示\n",
    ")\n",
    "top_level_supervisor = top_level_supervisor_agent.compile(name=\"top_level_supervisor\") # 编译顶层主管工作流并命名\n",
    "\n",
    "# --- 4. 调用顶层主管 ---\n",
    "# 使用用户查询调用顶层主管\n",
    "result = top_level_supervisor.invoke({\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"2024 年 FAANG 公司的总员工人数是多少？\" # 用户查询\n",
    "        }\n",
    "    ]\n",
    "})\n",
    "print(result[\"messages\"][-1].content)\n",
    "# 保存图\n",
    "from draw import save_graph_as_png\n",
    "save_graph_as_png(top_level_supervisor, \"./graphs/c7/multi_agent_hierarchy_workflow.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hi459ml977",
   "metadata": {},
   "source": [
    "**💡 层次化架构关键特性**：\n",
    "\n",
    "- **清晰的层次结构**：高级管理层、中级管理层、执行层各司其职\n",
    "- **自上而下的任务分解**：从高级计划到详细任务再到具体执行\n",
    "- **动态工作者分配**：根据规划结果动态创建工作者节点\n",
    "- **层次间通信**：每层都有明确的输入输出接口\n",
    "- **递归管理结构**：可以进一步扩展为更深层次的管理结构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tqxdlfk4yxg",
   "metadata": {},
   "source": [
    "### 7.2.3 网络架构 (Network Architecture)\n",
    "\n",
    "网络架构（也称为群体架构 Swarm Architecture）允许智能体之间进行更灵活、自组织的交互。在这种架构中，智能体可以直接相互通信，形成动态的协作网络。\n",
    "\n",
    "**核心特点：**\n",
    "- 去中心化的协作模式\n",
    "- 智能体间的直接通信\n",
    "- 动态的角色分配\n",
    "- 自适应的工作流程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cqn061bww9a",
   "metadata": {},
   "source": [
    "##### 示例 7-13：Alice 和 Bob 的简单交互网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mha9bt3xd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先安装 langgraph-swarm 库\n",
    "# !pip install langgraph-swarm\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph_swarm import create_handoff_tool, create_swarm\n",
    "\n",
    "# 这里推荐使用 SiliconCloud 平台上参赛量较大且支持工具调用的付费模型\n",
    "model = ChatOpenAI(model=\"Qwen/Qwen3-8B\")\n",
    "\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"添加两个数字\"\"\"\n",
    "    return a + b\n",
    "\n",
    "# 1. 创建专业智能体（Alice 和 Bob）：\n",
    "# 使用 create_react_agent 定义两个专业智能体 Alice 和 Bob。\n",
    "# Alice 是一位数学专家，拥有 'add' 工具和一个移交给 Bob 的移交工具。\n",
    "alice = create_react_agent(\n",
    "    model,\n",
    "    [add, create_handoff_tool(agent_name=\"Bob\")], # Alice 拥有 'add' 工具和移交给 Bob 的移交工具\n",
    "    prompt=\"你是 Alice，一位加法专家，使用工具完成所有加法。\",\n",
    "    name=\"Alice\",\n",
    ")\n",
    "\n",
    "# Bob 说话像个海盗，并拥有一个移交给 Alice 以寻求数学帮助的移交工具。\n",
    "bob = create_react_agent(\n",
    "    model,\n",
    "    [create_handoff_tool(agent_name=\"Alice\", description=\"务必将所有数学问题请转移给 Alice，她可以帮助解决数学问题\")], # Bob 拥有一个移交给 Alice 的移交工具\n",
    "    prompt=\"你是 Bob，你说话像个海盗。\",\n",
    "    name=\"Bob\",\n",
    ")\n",
    "\n",
    "# 2. 创建用于对话记忆的内存检查点：\n",
    "# InMemorySaver 用于短期记忆，这对于保持对话的连续性至关重要。\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "# 3. 使用 create_swarm 创建 Swarm 工作流：\n",
    "# create_swarm 函数使用 Alice 和 Bob 设置 swarm 架构。\n",
    "# default_active_agent=\"Alice\" 将 Alice 设置为新对话的起始智能体。\n",
    "workflow = create_swarm(\n",
    "    [alice, bob], # swarm 中的智能体列表\n",
    "    default_active_agent=\"Alice\" # 用于启动新对话的默认智能体\n",
    ")\n",
    "\n",
    "# 4. 使用检查点编译工作流：\n",
    "# 编译 swarm 工作流，传递检查点以进行内存管理。\n",
    "app = workflow.compile(checkpointer=checkpointer)\n",
    "\n",
    "# 5. 在多轮对话中调用 Swarm：\n",
    "# 多次调用 swarm，模拟对话线程。\n",
    "# config={\"configurable\": {\"thread_id\": \"1\"}} 确保消息在同一对话线程中被跟踪。\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "turn_1 = app.invoke( # 第一轮 - 用户想和 Bob 说话\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"我想和 Bob 说话\"}]},\n",
    "    config,\n",
    ")\n",
    "print(turn_1[\"messages\"][-1].content) # 第一轮的输出\n",
    "turn_2 = app.invoke( # 第二轮 - 用户提出一个数学问题\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"5 + 7 等于多少？\"}]},\n",
    "    config,\n",
    ")\n",
    "print(turn_2[\"messages\"][-1].content) # 第二轮的输出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kjpeq6s8dlo",
   "metadata": {},
   "source": [
    "## 7.3 情境感知智能体：后台主动式 AI 架构与模式\n",
    "\n",
    "传统的人工智能应用程序，尤其是那些利用大型语言模型 (LLM) 的应用程序，主要采用基于聊天的用户体验。虽然聊天模式的用户友好且易于实施，但它本质上将发起和管理交互的责任完全放在人类用户身上。\n",
    "\n",
    "一种变革性的替代方案在于情境感知智能体（Ambient Agent）架构。这些架构设想 AI 智能体在后台主动且持续地运行，勤勉地监控相关信息流并自主地对重要事件做出反应。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eu16upd6mrf",
   "metadata": {},
   "source": [
    "##### 示例 7-14：LangGraph 中可用的人机环路交互结构体\n",
    "\n",
    "让我们定义用于实现人机环路交互的数据结构："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70w65gnuvw",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Literal, Optional, Union\n",
    "\n",
    "# HumanInterruptConfig：定义人工中断操作的配置选项。\n",
    "class HumanInterruptConfig(TypedDict):\n",
    "    allow_ignore: bool  # allow_ignore：布尔值，是否允许用户忽略中断。\n",
    "    allow_respond: bool # allow_respond：布尔值，是否允许用户发送自由格式的响应。\n",
    "    allow_edit: bool    # allow_edit：布尔值，是否允许用户编辑操作参数。\n",
    "    allow_accept: bool  # allow_accept：布尔值，是否允许用户按原样接受操作。\n",
    "\n",
    "\n",
    "# ActionRequest：定义向人类请求的操作。\n",
    "class ActionRequest(TypedDict):\n",
    "    action: str      # action：字符串，操作的描述性名称或标题。\n",
    "    args: dict       # args：字典，与操作关联的参数（例如，工具调用参数）。\n",
    "\n",
    "\n",
    "# HumanInterrupt：表示人工中断请求的主要模式。\n",
    "class HumanInterrupt(TypedDict):\n",
    "    action_request: ActionRequest         # action_request：ActionRequest，有关请求操作的详细信息。\n",
    "    config: HumanInterruptConfig         # config：HumanInterruptConfig，人工响应的配置选项。\n",
    "    description: Optional[str]          # description：可选字符串，中断的详细描述，可以是 Markdown 格式。\n",
    "\n",
    "\n",
    "# HumanResponse：从 Agent Inbox UI 接收的人工响应模式。\n",
    "class HumanResponse(TypedDict):\n",
    "    type: Literal['accept', 'ignore', 'response', 'edit'] # type：Literal String，来自用户的响应类型（accept、ignore、response、edit）。\n",
    "    args: Union[None, str, ActionRequest]                # args：Union[None, String, ActionRequest]，与响应关联的参数，根据\"type\"而变化。\n",
    "                                                        #     - 对于\"accept\"和\"edit\"：ActionRequest，其中包含可能已修改的参数。\n",
    "                                                        #     - 对于\"response\"：包含用户文本响应的字符串。\n",
    "                                                        #     - 对于\"ignore\"：None。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ie9v4jfm3",
   "metadata": {},
   "source": [
    "##### 示例 7-15：在 LangGraph 图函数中使用 HumanInterrupt 和 HumanResponse\n",
    "\n",
    "以下示例演示了如何在 LangGraph 工作流中实现人机环路交互："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hhw7p6t8hc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Literal, Optional, Union\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import interrupt\n",
    "from langchain_core.messages import ToolMessage, HumanMessage, AIMessage\n",
    "\n",
    "# 假设这些工具在其他地方定义\n",
    "tools_by_name = {\n",
    "    \"hypothetical_tool\": lambda **kwargs: f\"工具执行结果: {kwargs}\"\n",
    "}\n",
    "\n",
    "# 定义图状态，继承消息状态以用于对话历史记录\n",
    "class AgentState(TypedDict):\n",
    "    messages: list\n",
    "    tool_calls: list\n",
    "\n",
    "# 定义可能触发人工中断的图函数\n",
    "def agent_node(state: AgentState):\n",
    "    \"\"\"智能体节点，决定是否调用工具或请求人工输入。\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1] if messages else None\n",
    "\n",
    "    # 假设智能体决定调用工具并具有工具调用详细信息\n",
    "    tool_call_name = \"hypothetical_tool\"\n",
    "    tool_call_args = {\"input_arg\": \"example_value\"}\n",
    "\n",
    "    # 构建 HumanInterrupt 对象\n",
    "    request: HumanInterrupt = {\n",
    "        \"action_request\": {\n",
    "            \"action\": tool_call_name, # 操作名称是工具名称\n",
    "            \"args\": tool_call_args  # 操作参数是工具参数\n",
    "        },\n",
    "        \"config\": {\n",
    "            \"allow_ignore\": True, # 允许用户忽略工具调用\n",
    "            \"allow_respond\": True, # 允许用户提供自由格式的响应\n",
    "            \"allow_edit\": True,  # 允许用户编辑工具参数\n",
    "            \"allow_accept\": True # 允许用户按原样接受工具调用\n",
    "        },\n",
    "        \"description\": f\"智能体建议使用参数：`{tool_call_args}` 调用工具：`{tool_call_name}`。 你批准吗？\", # Agent Inbox UI 的描述\n",
    "    }\n",
    "\n",
    "    # 调用 interrupt 函数，在列表中传递 HumanInterrupt 请求\n",
    "    response_list = interrupt([request])\n",
    "    response = response_list[0] if response_list else None # 从列表中提取第一个响应\n",
    "\n",
    "    if response:\n",
    "        if response['type'] == \"accept\":\n",
    "            # 用户接受了工具调用，继续执行工具\n",
    "            tool_result = tools_by_name[tool_call_name](**response['args']['args']) # 使用（可能已修改的）参数执行工具\n",
    "            output_message = ToolMessage(content=str(tool_result), tool_call_id=\"example_id\") # 使用结果创建 ToolMessage\n",
    "        elif response['type'] == \"edit\":\n",
    "            # 用户编辑了工具调用参数，使用编辑后的参数执行工具\n",
    "            edited_args = response['args']['args'] # 从 ActionRequest 中提取编辑后的参数\n",
    "            tool_result = tools_by_name[tool_call_name](**edited_args) # 使用编辑后的参数执行工具\n",
    "            output_message = ToolMessage(content=str(tool_result), tool_call_id=\"example_id\") # 使用结果创建 ToolMessage\n",
    "        elif response['type'] == \"response\":\n",
    "            # 用户提供了文本响应，据此处理\n",
    "            user_response_text = response['args'] # 提取用户的文本响应\n",
    "            output_message = AIMessage(content=f\"用户响应：{user_response_text}。 根据响应继续进行。\") # 创建 AIMessage 以确认响应\n",
    "        elif response['type'] == \"ignore\":\n",
    "            # 用户忽略了中断，据此处理\n",
    "            output_message = AIMessage(content=\"人工中断被忽略。 继续进行，不进行工具调用。\") # 创建 AIMessage，指示中断被忽略\n",
    "        else:\n",
    "            output_message = AIMessage(content=\"未知的人工响应类型。\") # 处理意外的响应类型\n",
    "    else:\n",
    "        # 未收到响应（例如，中断处理超时或错误）\n",
    "        output_message = AIMessage(content=\"未收到人工响应，继续进行，不进行干预。\") # 处理未收到响应的情况\n",
    "\n",
    "    return {\"messages\": [output_message]} # 返回更新的消息状态\n",
    "\n",
    "\n",
    "# 构建 LangGraph 工作流\n",
    "builder = StateGraph(AgentState)\n",
    "builder.add_node(\"agent_step\", agent_node)\n",
    "builder.add_edge(START, \"agent_step\")\n",
    "builder.add_edge(\"agent_step\", END)\n",
    "\n",
    "# 编译工作流\n",
    "workflow = builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wb13gdsv69o",
   "metadata": {},
   "source": [
    "## 📚 本章总结\n",
    "\n",
    "通过本章的学习，我们深入探索了 AI 智能体系统的架构设计与范式应用，从基础的智能体工作流模式（提示链、路由、并行化、协调器-工作者、评估器-优化器）到复杂的多智能体系统架构（主管、层次化、网络架构），再到前瞻性的情境感知智能体架构。我们掌握了 LangGraph 中的关键技术实现，包括 Graph API 与 Functional API 的灵活应用、Send API 的动态工作者创建、结构化输出与状态管理、以及人机环路交互机制，为构建复杂、高效且可扩展的智能体应用系统奠定了坚实的技术基础。在下一章中，我们将进一步探讨智能体的高级应用场景和优化策略。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
